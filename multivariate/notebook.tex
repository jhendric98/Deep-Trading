
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{multivariate}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{o}{*}
        
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pylab} \PY{k}{as} \PY{n+nn}{plt}
        
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{core} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Activation}\PY{p}{,} \PY{n}{Flatten}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{recurrent} \PY{k}{import} \PY{n}{LSTM}\PY{p}{,} \PY{n}{GRU}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Convolution1D}\PY{p}{,} \PY{n}{MaxPooling1D}\PY{p}{,} \PY{n}{AtrousConvolution1D}\PY{p}{,} \PY{n}{RepeatVector}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}\PY{p}{,} \PY{n}{ReduceLROnPlateau}\PY{p}{,} \PY{n}{CSVLogger}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{wrappers} \PY{k}{import} \PY{n}{Bidirectional}
        \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{regularizers}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{normalization} \PY{k}{import} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{advanced\PYZus{}activations} \PY{k}{import} \PY{o}{*}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{RMSprop}\PY{p}{,} \PY{n}{Adam}\PY{p}{,} \PY{n}{SGD}\PY{p}{,} \PY{n}{Nadam}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{initializers} \PY{k}{import} \PY{o}{*}
        
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{sns}\PY{o}{.}\PY{n}{despine}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/J/anaconda/envs/dlnd/lib/python3.6/site-packages/h5py/\_\_init\_\_.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from .\_conv import register\_converters as \_register\_converters
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{WINDOW} \PY{o}{=} \PY{l+m+mi}{30}
        \PY{n}{EMB\PYZus{}SIZE} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{n}{STEP} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{FORECAST} \PY{o}{=} \PY{l+m+mi}{30}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{data\PYZus{}original} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/AAPL1216.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        
        \PY{n}{openp} \PY{o}{=} \PY{n}{data\PYZus{}original}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Open}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
        \PY{n}{highp} \PY{o}{=} \PY{n}{data\PYZus{}original}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{High}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
        \PY{n}{lowp} \PY{o}{=} \PY{n}{data\PYZus{}original}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
        \PY{n}{closep} \PY{o}{=} \PY{n}{data\PYZus{}original}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
        \PY{n}{volumep} \PY{o}{=} \PY{n}{data\PYZus{}original}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Volume}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{X}\PY{p}{,} \PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}original}\PY{p}{)}\PY{p}{,} \PY{n}{STEP}\PY{p}{)}\PY{p}{:} 
            \PY{k}{try}\PY{p}{:}
                \PY{n}{o} \PY{o}{=} \PY{n}{openp}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
                \PY{n}{h} \PY{o}{=} \PY{n}{highp}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
                \PY{n}{l} \PY{o}{=} \PY{n}{lowp}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
                \PY{n}{c} \PY{o}{=} \PY{n}{closep}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
                \PY{n}{v} \PY{o}{=} \PY{n}{volumep}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
        
                \PY{n}{o} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{o}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{o}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{o}\PY{p}{)}
                \PY{n}{h} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{h}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{h}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{h}\PY{p}{)}
                \PY{n}{l} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{l}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{l}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{l}\PY{p}{)}
                \PY{n}{c} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{c}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{c}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{c}\PY{p}{)}
                \PY{n}{v} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{v}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{v}\PY{p}{)}
        
                \PY{n}{x\PYZus{}i} \PY{o}{=} \PY{n}{closep}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{p}{]}
                \PY{n}{y\PYZus{}i} \PY{o}{=} \PY{n}{closep}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{n}{WINDOW}\PY{o}{+}\PY{n}{FORECAST}\PY{p}{]}  
        
                \PY{n}{last\PYZus{}close} \PY{o}{=} \PY{n}{x\PYZus{}i}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                \PY{n}{next\PYZus{}close} \PY{o}{=} \PY{n}{y\PYZus{}i}
        
                \PY{k}{if} \PY{n}{last\PYZus{}close} \PY{o}{\PYZlt{}} \PY{n}{next\PYZus{}close}\PY{p}{:}
                    \PY{n}{y\PYZus{}i} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{y\PYZus{}i} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} 
        
                \PY{n}{x\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{(}\PY{n}{o}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{l}\PY{p}{,} \PY{n}{c}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                \PY{k}{break}
        
            \PY{n}{X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{)}
            \PY{n}{Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y\PYZus{}i}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{X}\PY{p}{,} \PY{n}{Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{Y}\PY{p}{)}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{create\PYZus{}Xt\PYZus{}Yt}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{EMB\PYZus{}SIZE}\PY{p}{)}\PY{p}{)}
        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{EMB\PYZus{}SIZE}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} (946, 30, 5)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Layer 1}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Convolution1D}\PY{p}{(}\PY{n}{input\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{WINDOW}\PY{p}{,} \PY{n}{EMB\PYZus{}SIZE}\PY{p}{)}\PY{p}{,}
                                \PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,}
                                \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}
                                \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{)}
        
        
        \PY{c+c1}{\PYZsh{} Layer 2}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Convolution1D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,}
                                \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}
                                \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Layer 3}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Output Layer}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{opt} \PY{o}{=} \PY{n}{Nadam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.002}\PY{p}{)}
        
        \PY{n}{reduce\PYZus{}lr} \PY{o}{=} \PY{n}{ReduceLROnPlateau}\PY{p}{(}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{min\PYZus{}lr}\PY{o}{=}\PY{l+m+mf}{0.000001}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lolkek.hdf5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
WARNING:tensorflow:From /Users/J/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn\_ops) with data\_format=NHWC is deprecated and will be removed in a future version.
Instructions for updating:
`NHWC` for data\_format is deprecated, use `NWC` instead
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_1 (Conv1D)            (None, 30, 16)            336       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_1 (Batch (None, 30, 16)            64        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
leaky\_re\_lu\_1 (LeakyReLU)    (None, 30, 16)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)          (None, 30, 16)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_2 (Conv1D)            (None, 30, 8)             520       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_2 (Batch (None, 30, 8)             32        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
leaky\_re\_lu\_2 (LeakyReLU)    (None, 30, 8)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_2 (Dropout)          (None, 30, 8)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_1 (Flatten)          (None, 240)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 64)                15424     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_3 (Batch (None, 64)                256       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
leaky\_re\_lu\_3 (LeakyReLU)    (None, 64)                0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 2)                 130       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 16,762
Trainable params: 16,586
Non-trainable params: 176
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
None

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,} 
                      \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} 
                  \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{1500}\PY{p}{,} 
                  \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{128}\PY{p}{,} 
                  \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} 
                  \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                  \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{reduce\PYZus{}lr}\PY{p}{,} \PY{n}{checkpointer}\PY{p}{]}\PY{p}{,}
                  \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 680 samples, validate on 171 samples
Epoch 1/1500
680/680 [==============================] - 1s 2ms/step - loss: 0.8906 - acc: 0.4721 - val\_loss: 0.7031 - val\_acc: 0.5205

Epoch 00001: val\_loss improved from inf to 0.70307, saving model to lolkek.hdf5
Epoch 2/1500
680/680 [==============================] - 0s 58us/step - loss: 0.7726 - acc: 0.5176 - val\_loss: 0.6983 - val\_acc: 0.5205

Epoch 00002: val\_loss improved from 0.70307 to 0.69830, saving model to lolkek.hdf5
Epoch 3/1500
680/680 [==============================] - 0s 57us/step - loss: 0.7624 - acc: 0.4971 - val\_loss: 0.7008 - val\_acc: 0.5322

Epoch 00003: val\_loss did not improve
Epoch 4/1500
680/680 [==============================] - 0s 56us/step - loss: 0.7191 - acc: 0.5500 - val\_loss: 0.7039 - val\_acc: 0.5263

Epoch 00004: val\_loss did not improve
Epoch 5/1500
680/680 [==============================] - 0s 63us/step - loss: 0.7250 - acc: 0.5368 - val\_loss: 0.7044 - val\_acc: 0.5205

Epoch 00005: val\_loss did not improve
Epoch 6/1500
680/680 [==============================] - 0s 58us/step - loss: 0.7020 - acc: 0.5706 - val\_loss: 0.6987 - val\_acc: 0.5322

Epoch 00006: val\_loss did not improve
Epoch 7/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6793 - acc: 0.5853 - val\_loss: 0.6957 - val\_acc: 0.5439

Epoch 00007: val\_loss improved from 0.69830 to 0.69569, saving model to lolkek.hdf5
Epoch 8/1500
680/680 [==============================] - 0s 61us/step - loss: 0.6998 - acc: 0.5618 - val\_loss: 0.6930 - val\_acc: 0.5497

Epoch 00008: val\_loss improved from 0.69569 to 0.69305, saving model to lolkek.hdf5
Epoch 9/1500
680/680 [==============================] - 0s 61us/step - loss: 0.6995 - acc: 0.5632 - val\_loss: 0.6923 - val\_acc: 0.5497

Epoch 00009: val\_loss improved from 0.69305 to 0.69231, saving model to lolkek.hdf5
Epoch 10/1500
680/680 [==============================] - 0s 63us/step - loss: 0.6882 - acc: 0.5809 - val\_loss: 0.6908 - val\_acc: 0.5614

Epoch 00010: val\_loss improved from 0.69231 to 0.69079, saving model to lolkek.hdf5
Epoch 11/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6876 - acc: 0.5794 - val\_loss: 0.6847 - val\_acc: 0.5731

Epoch 00011: val\_loss improved from 0.69079 to 0.68470, saving model to lolkek.hdf5
Epoch 12/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6821 - acc: 0.5809 - val\_loss: 0.6829 - val\_acc: 0.5673

Epoch 00012: val\_loss improved from 0.68470 to 0.68287, saving model to lolkek.hdf5
Epoch 13/1500
680/680 [==============================] - 0s 63us/step - loss: 0.6731 - acc: 0.6088 - val\_loss: 0.6805 - val\_acc: 0.5965

Epoch 00013: val\_loss improved from 0.68287 to 0.68053, saving model to lolkek.hdf5
Epoch 14/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6893 - acc: 0.5853 - val\_loss: 0.6876 - val\_acc: 0.5848

Epoch 00014: val\_loss did not improve
Epoch 15/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6872 - acc: 0.5882 - val\_loss: 0.6867 - val\_acc: 0.5439

Epoch 00015: val\_loss did not improve
Epoch 16/1500
680/680 [==============================] - 0s 59us/step - loss: 0.6890 - acc: 0.5809 - val\_loss: 0.6840 - val\_acc: 0.5439

Epoch 00016: val\_loss did not improve
Epoch 17/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6818 - acc: 0.5647 - val\_loss: 0.6863 - val\_acc: 0.5380

Epoch 00017: val\_loss did not improve
Epoch 18/1500
680/680 [==============================] - 0s 57us/step - loss: 0.6813 - acc: 0.5912 - val\_loss: 0.6806 - val\_acc: 0.5789

Epoch 00018: val\_loss did not improve
Epoch 19/1500
680/680 [==============================] - 0s 66us/step - loss: 0.6769 - acc: 0.5691 - val\_loss: 0.6823 - val\_acc: 0.5614

Epoch 00019: val\_loss did not improve
Epoch 20/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6673 - acc: 0.6074 - val\_loss: 0.6710 - val\_acc: 0.5731

Epoch 00020: val\_loss improved from 0.68053 to 0.67101, saving model to lolkek.hdf5
Epoch 21/1500
680/680 [==============================] - 0s 63us/step - loss: 0.6628 - acc: 0.6088 - val\_loss: 0.6708 - val\_acc: 0.5848

Epoch 00021: val\_loss improved from 0.67101 to 0.67085, saving model to lolkek.hdf5
Epoch 22/1500
680/680 [==============================] - 0s 68us/step - loss: 0.6627 - acc: 0.6103 - val\_loss: 0.6681 - val\_acc: 0.5789

Epoch 00022: val\_loss improved from 0.67085 to 0.66811, saving model to lolkek.hdf5
Epoch 23/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6446 - acc: 0.6221 - val\_loss: 0.6635 - val\_acc: 0.5673

Epoch 00023: val\_loss improved from 0.66811 to 0.66355, saving model to lolkek.hdf5
Epoch 24/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6570 - acc: 0.5956 - val\_loss: 0.6645 - val\_acc: 0.5848

Epoch 00024: val\_loss did not improve
Epoch 25/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6627 - acc: 0.6044 - val\_loss: 0.6586 - val\_acc: 0.5848

Epoch 00025: val\_loss improved from 0.66355 to 0.65856, saving model to lolkek.hdf5
Epoch 26/1500
680/680 [==============================] - 0s 61us/step - loss: 0.6563 - acc: 0.6015 - val\_loss: 0.6559 - val\_acc: 0.6199

Epoch 00026: val\_loss improved from 0.65856 to 0.65587, saving model to lolkek.hdf5
Epoch 27/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6502 - acc: 0.5956 - val\_loss: 0.6527 - val\_acc: 0.6374

Epoch 00027: val\_loss improved from 0.65587 to 0.65272, saving model to lolkek.hdf5
Epoch 28/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6605 - acc: 0.5824 - val\_loss: 0.6511 - val\_acc: 0.6433

Epoch 00028: val\_loss improved from 0.65272 to 0.65115, saving model to lolkek.hdf5
Epoch 29/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6481 - acc: 0.6221 - val\_loss: 0.6508 - val\_acc: 0.6316

Epoch 00029: val\_loss improved from 0.65115 to 0.65085, saving model to lolkek.hdf5
Epoch 30/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6424 - acc: 0.6235 - val\_loss: 0.6497 - val\_acc: 0.6082

Epoch 00030: val\_loss improved from 0.65085 to 0.64969, saving model to lolkek.hdf5
Epoch 31/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6512 - acc: 0.6191 - val\_loss: 0.6425 - val\_acc: 0.6433

Epoch 00031: val\_loss improved from 0.64969 to 0.64247, saving model to lolkek.hdf5
Epoch 32/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6379 - acc: 0.6235 - val\_loss: 0.6392 - val\_acc: 0.6433

Epoch 00032: val\_loss improved from 0.64247 to 0.63922, saving model to lolkek.hdf5
Epoch 33/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6504 - acc: 0.6059 - val\_loss: 0.6378 - val\_acc: 0.6316

Epoch 00033: val\_loss improved from 0.63922 to 0.63778, saving model to lolkek.hdf5
Epoch 34/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6396 - acc: 0.6279 - val\_loss: 0.6349 - val\_acc: 0.6316

Epoch 00034: val\_loss improved from 0.63778 to 0.63486, saving model to lolkek.hdf5
Epoch 35/1500
680/680 [==============================] - 0s 63us/step - loss: 0.6292 - acc: 0.6309 - val\_loss: 0.6334 - val\_acc: 0.6491

Epoch 00035: val\_loss improved from 0.63486 to 0.63339, saving model to lolkek.hdf5
Epoch 36/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6275 - acc: 0.6206 - val\_loss: 0.6313 - val\_acc: 0.6374

Epoch 00036: val\_loss improved from 0.63339 to 0.63132, saving model to lolkek.hdf5
Epoch 37/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6312 - acc: 0.6221 - val\_loss: 0.6329 - val\_acc: 0.6491

Epoch 00037: val\_loss did not improve
Epoch 38/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6140 - acc: 0.6824 - val\_loss: 0.6317 - val\_acc: 0.6257

Epoch 00038: val\_loss did not improve
Epoch 39/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6489 - acc: 0.6118 - val\_loss: 0.6294 - val\_acc: 0.6374

Epoch 00039: val\_loss improved from 0.63132 to 0.62940, saving model to lolkek.hdf5
Epoch 40/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6092 - acc: 0.6618 - val\_loss: 0.6277 - val\_acc: 0.6374

Epoch 00040: val\_loss improved from 0.62940 to 0.62775, saving model to lolkek.hdf5
Epoch 41/1500
680/680 [==============================] - 0s 61us/step - loss: 0.6151 - acc: 0.6500 - val\_loss: 0.6216 - val\_acc: 0.6491

Epoch 00041: val\_loss improved from 0.62775 to 0.62159, saving model to lolkek.hdf5
Epoch 42/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6203 - acc: 0.6397 - val\_loss: 0.6261 - val\_acc: 0.6199

Epoch 00042: val\_loss did not improve
Epoch 43/1500
680/680 [==============================] - 0s 56us/step - loss: 0.6278 - acc: 0.6279 - val\_loss: 0.6236 - val\_acc: 0.6374

Epoch 00043: val\_loss did not improve
Epoch 44/1500
680/680 [==============================] - 0s 65us/step - loss: 0.6194 - acc: 0.6324 - val\_loss: 0.6255 - val\_acc: 0.6257

Epoch 00044: val\_loss did not improve
Epoch 45/1500
680/680 [==============================] - 0s 58us/step - loss: 0.6178 - acc: 0.6471 - val\_loss: 0.6231 - val\_acc: 0.6199

Epoch 00045: val\_loss did not improve
Epoch 46/1500
680/680 [==============================] - 0s 58us/step - loss: 0.6359 - acc: 0.6324 - val\_loss: 0.6262 - val\_acc: 0.6257

Epoch 00046: val\_loss did not improve
Epoch 47/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6301 - acc: 0.6176 - val\_loss: 0.6242 - val\_acc: 0.6491

Epoch 00047: val\_loss did not improve
Epoch 48/1500
680/680 [==============================] - 0s 56us/step - loss: 0.6323 - acc: 0.6221 - val\_loss: 0.6239 - val\_acc: 0.6491

Epoch 00048: val\_loss did not improve
Epoch 49/1500
680/680 [==============================] - 0s 59us/step - loss: 0.6047 - acc: 0.6412 - val\_loss: 0.6221 - val\_acc: 0.6374

Epoch 00049: val\_loss did not improve
Epoch 50/1500
680/680 [==============================] - 0s 61us/step - loss: 0.6213 - acc: 0.6441 - val\_loss: 0.6214 - val\_acc: 0.6550

Epoch 00050: val\_loss improved from 0.62159 to 0.62141, saving model to lolkek.hdf5
Epoch 51/1500
680/680 [==============================] - 0s 66us/step - loss: 0.6151 - acc: 0.6294 - val\_loss: 0.6154 - val\_acc: 0.6374

Epoch 00051: val\_loss improved from 0.62141 to 0.61537, saving model to lolkek.hdf5
Epoch 52/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6049 - acc: 0.6500 - val\_loss: 0.6141 - val\_acc: 0.6491

Epoch 00052: val\_loss improved from 0.61537 to 0.61413, saving model to lolkek.hdf5
Epoch 53/1500
680/680 [==============================] - 0s 59us/step - loss: 0.6067 - acc: 0.6500 - val\_loss: 0.6105 - val\_acc: 0.6491

Epoch 00053: val\_loss improved from 0.61413 to 0.61052, saving model to lolkek.hdf5
Epoch 54/1500
680/680 [==============================] - 0s 57us/step - loss: 0.6412 - acc: 0.6265 - val\_loss: 0.6116 - val\_acc: 0.6433

Epoch 00054: val\_loss did not improve
Epoch 55/1500
680/680 [==============================] - 0s 56us/step - loss: 0.6189 - acc: 0.6279 - val\_loss: 0.6171 - val\_acc: 0.6550

Epoch 00055: val\_loss did not improve
Epoch 56/1500
680/680 [==============================] - 0s 56us/step - loss: 0.6029 - acc: 0.6544 - val\_loss: 0.6144 - val\_acc: 0.6374

Epoch 00056: val\_loss did not improve
Epoch 57/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6241 - acc: 0.6412 - val\_loss: 0.6205 - val\_acc: 0.6316

Epoch 00057: val\_loss did not improve
Epoch 58/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5894 - acc: 0.6809 - val\_loss: 0.6204 - val\_acc: 0.6082

Epoch 00058: val\_loss did not improve
Epoch 59/1500
680/680 [==============================] - 0s 63us/step - loss: 0.6136 - acc: 0.6765 - val\_loss: 0.6219 - val\_acc: 0.6433

Epoch 00059: val\_loss did not improve
Epoch 60/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5921 - acc: 0.6809 - val\_loss: 0.6105 - val\_acc: 0.6608

Epoch 00060: val\_loss improved from 0.61052 to 0.61051, saving model to lolkek.hdf5
Epoch 61/1500
680/680 [==============================] - 0s 62us/step - loss: 0.6121 - acc: 0.6368 - val\_loss: 0.6116 - val\_acc: 0.6491

Epoch 00061: val\_loss did not improve
Epoch 62/1500
680/680 [==============================] - 0s 59us/step - loss: 0.6099 - acc: 0.6647 - val\_loss: 0.6109 - val\_acc: 0.6374

Epoch 00062: val\_loss did not improve
Epoch 63/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5935 - acc: 0.6368 - val\_loss: 0.6068 - val\_acc: 0.6257

Epoch 00063: val\_loss improved from 0.61051 to 0.60683, saving model to lolkek.hdf5
Epoch 64/1500
680/680 [==============================] - 0s 60us/step - loss: 0.6051 - acc: 0.6368 - val\_loss: 0.6131 - val\_acc: 0.6433

Epoch 00064: val\_loss did not improve
Epoch 65/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5955 - acc: 0.6750 - val\_loss: 0.6101 - val\_acc: 0.6433

Epoch 00065: val\_loss did not improve
Epoch 66/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5854 - acc: 0.6691 - val\_loss: 0.6083 - val\_acc: 0.6316

Epoch 00066: val\_loss did not improve
Epoch 67/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5907 - acc: 0.6647 - val\_loss: 0.6086 - val\_acc: 0.6374

Epoch 00067: val\_loss did not improve
Epoch 68/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5845 - acc: 0.6750 - val\_loss: 0.6072 - val\_acc: 0.6433

Epoch 00068: val\_loss did not improve
Epoch 69/1500
680/680 [==============================] - 0s 65us/step - loss: 0.5901 - acc: 0.6824 - val\_loss: 0.6064 - val\_acc: 0.6316

Epoch 00069: val\_loss improved from 0.60683 to 0.60644, saving model to lolkek.hdf5
Epoch 70/1500
680/680 [==============================] - 0s 66us/step - loss: 0.5883 - acc: 0.6838 - val\_loss: 0.6044 - val\_acc: 0.6316

Epoch 00070: val\_loss improved from 0.60644 to 0.60438, saving model to lolkek.hdf5
Epoch 71/1500
680/680 [==============================] - 0s 66us/step - loss: 0.5874 - acc: 0.6618 - val\_loss: 0.6080 - val\_acc: 0.6374

Epoch 00071: val\_loss did not improve
Epoch 72/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5725 - acc: 0.6721 - val\_loss: 0.6058 - val\_acc: 0.6550

Epoch 00072: val\_loss did not improve
Epoch 73/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5835 - acc: 0.7000 - val\_loss: 0.5997 - val\_acc: 0.6491

Epoch 00073: val\_loss improved from 0.60438 to 0.59966, saving model to lolkek.hdf5
Epoch 74/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5870 - acc: 0.6676 - val\_loss: 0.5987 - val\_acc: 0.6550

Epoch 00074: val\_loss improved from 0.59966 to 0.59874, saving model to lolkek.hdf5
Epoch 75/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5703 - acc: 0.6912 - val\_loss: 0.5958 - val\_acc: 0.6550

Epoch 00075: val\_loss improved from 0.59874 to 0.59580, saving model to lolkek.hdf5
Epoch 76/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5757 - acc: 0.6750 - val\_loss: 0.5968 - val\_acc: 0.6725

Epoch 00076: val\_loss did not improve
Epoch 77/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5924 - acc: 0.6706 - val\_loss: 0.5948 - val\_acc: 0.6491

Epoch 00077: val\_loss improved from 0.59580 to 0.59485, saving model to lolkek.hdf5
Epoch 78/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5947 - acc: 0.6588 - val\_loss: 0.5987 - val\_acc: 0.6784

Epoch 00078: val\_loss did not improve
Epoch 79/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5965 - acc: 0.6559 - val\_loss: 0.5891 - val\_acc: 0.6550

Epoch 00079: val\_loss improved from 0.59485 to 0.58908, saving model to lolkek.hdf5
Epoch 80/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5730 - acc: 0.6897 - val\_loss: 0.5873 - val\_acc: 0.6608

Epoch 00080: val\_loss improved from 0.58908 to 0.58726, saving model to lolkek.hdf5
Epoch 81/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5748 - acc: 0.6912 - val\_loss: 0.5848 - val\_acc: 0.6550

Epoch 00081: val\_loss improved from 0.58726 to 0.58478, saving model to lolkek.hdf5
Epoch 82/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5708 - acc: 0.6956 - val\_loss: 0.5871 - val\_acc: 0.6374

Epoch 00082: val\_loss did not improve
Epoch 83/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5902 - acc: 0.6515 - val\_loss: 0.5881 - val\_acc: 0.6491

Epoch 00083: val\_loss did not improve
Epoch 84/1500
680/680 [==============================] - 0s 64us/step - loss: 0.6009 - acc: 0.6603 - val\_loss: 0.5938 - val\_acc: 0.6433

Epoch 00084: val\_loss did not improve
Epoch 85/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5869 - acc: 0.6735 - val\_loss: 0.5895 - val\_acc: 0.6550

Epoch 00085: val\_loss did not improve
Epoch 86/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5817 - acc: 0.6838 - val\_loss: 0.5896 - val\_acc: 0.6667

Epoch 00086: val\_loss did not improve
Epoch 87/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5818 - acc: 0.6632 - val\_loss: 0.5855 - val\_acc: 0.6725

Epoch 00087: val\_loss did not improve
Epoch 88/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5687 - acc: 0.6956 - val\_loss: 0.5841 - val\_acc: 0.6725

Epoch 00088: val\_loss improved from 0.58478 to 0.58411, saving model to lolkek.hdf5
Epoch 89/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5708 - acc: 0.7088 - val\_loss: 0.5859 - val\_acc: 0.6550

Epoch 00089: val\_loss did not improve
Epoch 90/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5793 - acc: 0.6706 - val\_loss: 0.5897 - val\_acc: 0.6608

Epoch 00090: val\_loss did not improve
Epoch 91/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5626 - acc: 0.6912 - val\_loss: 0.5836 - val\_acc: 0.6608

Epoch 00091: val\_loss improved from 0.58411 to 0.58356, saving model to lolkek.hdf5
Epoch 92/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5745 - acc: 0.6809 - val\_loss: 0.5911 - val\_acc: 0.6784

Epoch 00092: val\_loss did not improve
Epoch 93/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5662 - acc: 0.6676 - val\_loss: 0.5840 - val\_acc: 0.6842

Epoch 00093: val\_loss did not improve
Epoch 94/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5663 - acc: 0.6882 - val\_loss: 0.5901 - val\_acc: 0.6842

Epoch 00094: val\_loss did not improve
Epoch 95/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5642 - acc: 0.7118 - val\_loss: 0.5970 - val\_acc: 0.6901

Epoch 00095: val\_loss did not improve
Epoch 96/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5606 - acc: 0.7000 - val\_loss: 0.5873 - val\_acc: 0.6784

Epoch 00096: val\_loss did not improve
Epoch 97/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5565 - acc: 0.6824 - val\_loss: 0.5824 - val\_acc: 0.6667

Epoch 00097: val\_loss improved from 0.58356 to 0.58244, saving model to lolkek.hdf5
Epoch 98/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5719 - acc: 0.7029 - val\_loss: 0.5778 - val\_acc: 0.6842

Epoch 00098: val\_loss improved from 0.58244 to 0.57780, saving model to lolkek.hdf5
Epoch 99/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5595 - acc: 0.7015 - val\_loss: 0.5740 - val\_acc: 0.6842

Epoch 00099: val\_loss improved from 0.57780 to 0.57398, saving model to lolkek.hdf5
Epoch 100/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5676 - acc: 0.6853 - val\_loss: 0.5758 - val\_acc: 0.7018

Epoch 00100: val\_loss did not improve
Epoch 101/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5382 - acc: 0.7206 - val\_loss: 0.5784 - val\_acc: 0.6842

Epoch 00101: val\_loss did not improve
Epoch 102/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5470 - acc: 0.7074 - val\_loss: 0.5787 - val\_acc: 0.6842

Epoch 00102: val\_loss did not improve
Epoch 103/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5709 - acc: 0.6941 - val\_loss: 0.5865 - val\_acc: 0.6959

Epoch 00103: val\_loss did not improve
Epoch 104/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5673 - acc: 0.6971 - val\_loss: 0.5835 - val\_acc: 0.6784

Epoch 00104: val\_loss did not improve
Epoch 105/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5579 - acc: 0.7000 - val\_loss: 0.5749 - val\_acc: 0.7135

Epoch 00105: val\_loss did not improve
Epoch 106/1500
680/680 [==============================] - 0s 57us/step - loss: 0.5679 - acc: 0.6912 - val\_loss: 0.5782 - val\_acc: 0.6842

Epoch 00106: val\_loss did not improve
Epoch 107/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5362 - acc: 0.6912 - val\_loss: 0.5728 - val\_acc: 0.6959

Epoch 00107: val\_loss improved from 0.57398 to 0.57280, saving model to lolkek.hdf5
Epoch 108/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5668 - acc: 0.7015 - val\_loss: 0.5695 - val\_acc: 0.7018

Epoch 00108: val\_loss improved from 0.57280 to 0.56950, saving model to lolkek.hdf5
Epoch 109/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5554 - acc: 0.6853 - val\_loss: 0.5732 - val\_acc: 0.7135

Epoch 00109: val\_loss did not improve
Epoch 110/1500
680/680 [==============================] - 0s 55us/step - loss: 0.5432 - acc: 0.7000 - val\_loss: 0.5673 - val\_acc: 0.7251

Epoch 00110: val\_loss improved from 0.56950 to 0.56725, saving model to lolkek.hdf5
Epoch 111/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5492 - acc: 0.7103 - val\_loss: 0.5698 - val\_acc: 0.7193

Epoch 00111: val\_loss did not improve
Epoch 112/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5505 - acc: 0.7147 - val\_loss: 0.5699 - val\_acc: 0.7310

Epoch 00112: val\_loss did not improve
Epoch 113/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5669 - acc: 0.6868 - val\_loss: 0.5890 - val\_acc: 0.6842

Epoch 00113: val\_loss did not improve
Epoch 114/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5484 - acc: 0.7088 - val\_loss: 0.5654 - val\_acc: 0.7251

Epoch 00114: val\_loss improved from 0.56725 to 0.56540, saving model to lolkek.hdf5
Epoch 115/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5406 - acc: 0.7088 - val\_loss: 0.5644 - val\_acc: 0.7076

Epoch 00115: val\_loss improved from 0.56540 to 0.56440, saving model to lolkek.hdf5
Epoch 116/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5462 - acc: 0.7059 - val\_loss: 0.5666 - val\_acc: 0.6901

Epoch 00116: val\_loss did not improve
Epoch 117/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5494 - acc: 0.7103 - val\_loss: 0.5638 - val\_acc: 0.7076

Epoch 00117: val\_loss improved from 0.56440 to 0.56384, saving model to lolkek.hdf5
Epoch 118/1500
680/680 [==============================] - 0s 65us/step - loss: 0.5301 - acc: 0.7147 - val\_loss: 0.5533 - val\_acc: 0.6959

Epoch 00118: val\_loss improved from 0.56384 to 0.55333, saving model to lolkek.hdf5
Epoch 119/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5207 - acc: 0.7147 - val\_loss: 0.5499 - val\_acc: 0.7076

Epoch 00119: val\_loss improved from 0.55333 to 0.54992, saving model to lolkek.hdf5
Epoch 120/1500
680/680 [==============================] - 0s 65us/step - loss: 0.5235 - acc: 0.7147 - val\_loss: 0.5476 - val\_acc: 0.7251

Epoch 00120: val\_loss improved from 0.54992 to 0.54759, saving model to lolkek.hdf5
Epoch 121/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5496 - acc: 0.6765 - val\_loss: 0.5488 - val\_acc: 0.7135

Epoch 00121: val\_loss did not improve
Epoch 122/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5372 - acc: 0.7191 - val\_loss: 0.5462 - val\_acc: 0.7251

Epoch 00122: val\_loss improved from 0.54759 to 0.54620, saving model to lolkek.hdf5
Epoch 123/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5432 - acc: 0.7103 - val\_loss: 0.5517 - val\_acc: 0.6901

Epoch 00123: val\_loss did not improve
Epoch 124/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5264 - acc: 0.7103 - val\_loss: 0.5419 - val\_acc: 0.7018

Epoch 00124: val\_loss improved from 0.54620 to 0.54187, saving model to lolkek.hdf5
Epoch 125/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5347 - acc: 0.7103 - val\_loss: 0.5416 - val\_acc: 0.7193

Epoch 00125: val\_loss improved from 0.54187 to 0.54162, saving model to lolkek.hdf5
Epoch 126/1500
680/680 [==============================] - 0s 70us/step - loss: 0.5397 - acc: 0.7191 - val\_loss: 0.5378 - val\_acc: 0.7485

Epoch 00126: val\_loss improved from 0.54162 to 0.53782, saving model to lolkek.hdf5
Epoch 127/1500
680/680 [==============================] - 0s 86us/step - loss: 0.5535 - acc: 0.7074 - val\_loss: 0.5460 - val\_acc: 0.7485

Epoch 00127: val\_loss did not improve
Epoch 128/1500
680/680 [==============================] - 0s 85us/step - loss: 0.5526 - acc: 0.7324 - val\_loss: 0.5520 - val\_acc: 0.7135

Epoch 00128: val\_loss did not improve
Epoch 129/1500
680/680 [==============================] - 0s 81us/step - loss: 0.5029 - acc: 0.7529 - val\_loss: 0.5480 - val\_acc: 0.7251

Epoch 00129: val\_loss did not improve
Epoch 130/1500
680/680 [==============================] - 0s 76us/step - loss: 0.5322 - acc: 0.7176 - val\_loss: 0.5453 - val\_acc: 0.7135

Epoch 00130: val\_loss did not improve
Epoch 131/1500
680/680 [==============================] - 0s 74us/step - loss: 0.5495 - acc: 0.7235 - val\_loss: 0.5491 - val\_acc: 0.7251

Epoch 00131: val\_loss did not improve
Epoch 132/1500
680/680 [==============================] - 0s 76us/step - loss: 0.5248 - acc: 0.7265 - val\_loss: 0.5549 - val\_acc: 0.7251

Epoch 00132: val\_loss did not improve
Epoch 133/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5226 - acc: 0.7441 - val\_loss: 0.5535 - val\_acc: 0.7251

Epoch 00133: val\_loss did not improve
Epoch 134/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5256 - acc: 0.7132 - val\_loss: 0.5500 - val\_acc: 0.7310

Epoch 00134: val\_loss did not improve
Epoch 135/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5336 - acc: 0.7162 - val\_loss: 0.5435 - val\_acc: 0.7193

Epoch 00135: val\_loss did not improve
Epoch 136/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5145 - acc: 0.7353 - val\_loss: 0.5325 - val\_acc: 0.7427

Epoch 00136: val\_loss improved from 0.53782 to 0.53254, saving model to lolkek.hdf5
Epoch 137/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5234 - acc: 0.7544 - val\_loss: 0.5310 - val\_acc: 0.7310

Epoch 00137: val\_loss improved from 0.53254 to 0.53101, saving model to lolkek.hdf5
Epoch 138/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4999 - acc: 0.7412 - val\_loss: 0.5295 - val\_acc: 0.7368

Epoch 00138: val\_loss improved from 0.53101 to 0.52955, saving model to lolkek.hdf5
Epoch 139/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5464 - acc: 0.7176 - val\_loss: 0.5292 - val\_acc: 0.7251

Epoch 00139: val\_loss improved from 0.52955 to 0.52923, saving model to lolkek.hdf5
Epoch 140/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5528 - acc: 0.7147 - val\_loss: 0.5332 - val\_acc: 0.7310

Epoch 00140: val\_loss did not improve
Epoch 141/1500
680/680 [==============================] - 0s 55us/step - loss: 0.5154 - acc: 0.7206 - val\_loss: 0.5390 - val\_acc: 0.7135

Epoch 00141: val\_loss did not improve
Epoch 142/1500
680/680 [==============================] - 0s 55us/step - loss: 0.5165 - acc: 0.7426 - val\_loss: 0.5395 - val\_acc: 0.7135

Epoch 00142: val\_loss did not improve
Epoch 143/1500
680/680 [==============================] - 0s 59us/step - loss: 0.5329 - acc: 0.7103 - val\_loss: 0.5379 - val\_acc: 0.7193

Epoch 00143: val\_loss did not improve
Epoch 144/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5275 - acc: 0.7382 - val\_loss: 0.5295 - val\_acc: 0.7310

Epoch 00144: val\_loss did not improve
Epoch 145/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5177 - acc: 0.7353 - val\_loss: 0.5276 - val\_acc: 0.7368

Epoch 00145: val\_loss improved from 0.52923 to 0.52761, saving model to lolkek.hdf5
Epoch 146/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5207 - acc: 0.7132 - val\_loss: 0.5216 - val\_acc: 0.7485

Epoch 00146: val\_loss improved from 0.52761 to 0.52164, saving model to lolkek.hdf5
Epoch 147/1500
680/680 [==============================] - 0s 64us/step - loss: 0.5282 - acc: 0.7309 - val\_loss: 0.5277 - val\_acc: 0.7310

Epoch 00147: val\_loss did not improve
Epoch 148/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5379 - acc: 0.7191 - val\_loss: 0.5219 - val\_acc: 0.7251

Epoch 00148: val\_loss did not improve
Epoch 149/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5269 - acc: 0.7250 - val\_loss: 0.5231 - val\_acc: 0.7368

Epoch 00149: val\_loss did not improve
Epoch 150/1500
680/680 [==============================] - 0s 56us/step - loss: 0.5054 - acc: 0.7485 - val\_loss: 0.5197 - val\_acc: 0.7427

Epoch 00150: val\_loss improved from 0.52164 to 0.51970, saving model to lolkek.hdf5
Epoch 151/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5175 - acc: 0.7368 - val\_loss: 0.5223 - val\_acc: 0.7368

Epoch 00151: val\_loss did not improve
Epoch 152/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5030 - acc: 0.7559 - val\_loss: 0.5122 - val\_acc: 0.7661

Epoch 00152: val\_loss improved from 0.51970 to 0.51222, saving model to lolkek.hdf5
Epoch 153/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5046 - acc: 0.7441 - val\_loss: 0.5124 - val\_acc: 0.7778

Epoch 00153: val\_loss did not improve
Epoch 154/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5222 - acc: 0.7279 - val\_loss: 0.5116 - val\_acc: 0.7368

Epoch 00154: val\_loss improved from 0.51222 to 0.51164, saving model to lolkek.hdf5
Epoch 155/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5142 - acc: 0.7338 - val\_loss: 0.5073 - val\_acc: 0.7485

Epoch 00155: val\_loss improved from 0.51164 to 0.50729, saving model to lolkek.hdf5
Epoch 156/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4793 - acc: 0.7618 - val\_loss: 0.5113 - val\_acc: 0.7485

Epoch 00156: val\_loss did not improve
Epoch 157/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4955 - acc: 0.7676 - val\_loss: 0.5110 - val\_acc: 0.7602

Epoch 00157: val\_loss did not improve
Epoch 158/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4923 - acc: 0.7500 - val\_loss: 0.5260 - val\_acc: 0.7427

Epoch 00158: val\_loss did not improve
Epoch 159/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4966 - acc: 0.7529 - val\_loss: 0.5194 - val\_acc: 0.7427

Epoch 00159: val\_loss did not improve
Epoch 160/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4915 - acc: 0.7574 - val\_loss: 0.5169 - val\_acc: 0.7544

Epoch 00160: val\_loss did not improve
Epoch 161/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4926 - acc: 0.7368 - val\_loss: 0.5103 - val\_acc: 0.7719

Epoch 00161: val\_loss did not improve
Epoch 162/1500
680/680 [==============================] - 0s 62us/step - loss: 0.5020 - acc: 0.7368 - val\_loss: 0.5060 - val\_acc: 0.7427

Epoch 00162: val\_loss improved from 0.50729 to 0.50597, saving model to lolkek.hdf5
Epoch 163/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5115 - acc: 0.7397 - val\_loss: 0.5014 - val\_acc: 0.7368

Epoch 00163: val\_loss improved from 0.50597 to 0.50140, saving model to lolkek.hdf5
Epoch 164/1500
680/680 [==============================] - 0s 63us/step - loss: 0.5071 - acc: 0.7324 - val\_loss: 0.4950 - val\_acc: 0.7485

Epoch 00164: val\_loss improved from 0.50140 to 0.49495, saving model to lolkek.hdf5
Epoch 165/1500
680/680 [==============================] - 0s 61us/step - loss: 0.5236 - acc: 0.7397 - val\_loss: 0.5101 - val\_acc: 0.7485

Epoch 00165: val\_loss did not improve
Epoch 166/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4756 - acc: 0.7603 - val\_loss: 0.5082 - val\_acc: 0.7485

Epoch 00166: val\_loss did not improve
Epoch 167/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4727 - acc: 0.7603 - val\_loss: 0.5043 - val\_acc: 0.7427

Epoch 00167: val\_loss did not improve
Epoch 168/1500
680/680 [==============================] - 0s 58us/step - loss: 0.5006 - acc: 0.7368 - val\_loss: 0.4937 - val\_acc: 0.7544

Epoch 00168: val\_loss improved from 0.49495 to 0.49370, saving model to lolkek.hdf5
Epoch 169/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4933 - acc: 0.7603 - val\_loss: 0.4995 - val\_acc: 0.7602

Epoch 00169: val\_loss did not improve
Epoch 170/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4820 - acc: 0.7735 - val\_loss: 0.4861 - val\_acc: 0.7602

Epoch 00170: val\_loss improved from 0.49370 to 0.48610, saving model to lolkek.hdf5
Epoch 171/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4944 - acc: 0.7338 - val\_loss: 0.4849 - val\_acc: 0.7953

Epoch 00171: val\_loss improved from 0.48610 to 0.48494, saving model to lolkek.hdf5
Epoch 172/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4626 - acc: 0.7676 - val\_loss: 0.4955 - val\_acc: 0.7661

Epoch 00172: val\_loss did not improve
Epoch 173/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4976 - acc: 0.7441 - val\_loss: 0.4878 - val\_acc: 0.7485

Epoch 00173: val\_loss did not improve
Epoch 174/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4957 - acc: 0.7735 - val\_loss: 0.4851 - val\_acc: 0.7778

Epoch 00174: val\_loss did not improve
Epoch 175/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4773 - acc: 0.7574 - val\_loss: 0.4883 - val\_acc: 0.7778

Epoch 00175: val\_loss did not improve
Epoch 176/1500
680/680 [==============================] - 0s 60us/step - loss: 0.5028 - acc: 0.7397 - val\_loss: 0.4875 - val\_acc: 0.8070

Epoch 00176: val\_loss did not improve
Epoch 177/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4930 - acc: 0.7529 - val\_loss: 0.4847 - val\_acc: 0.8070

Epoch 00177: val\_loss improved from 0.48494 to 0.48467, saving model to lolkek.hdf5
Epoch 178/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4923 - acc: 0.7426 - val\_loss: 0.4782 - val\_acc: 0.8129

Epoch 00178: val\_loss improved from 0.48467 to 0.47822, saving model to lolkek.hdf5
Epoch 179/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4990 - acc: 0.7559 - val\_loss: 0.4866 - val\_acc: 0.7836

Epoch 00179: val\_loss did not improve
Epoch 180/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4774 - acc: 0.7588 - val\_loss: 0.4927 - val\_acc: 0.7836

Epoch 00180: val\_loss did not improve
Epoch 181/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4954 - acc: 0.7279 - val\_loss: 0.4836 - val\_acc: 0.7895

Epoch 00181: val\_loss did not improve
Epoch 182/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4779 - acc: 0.7662 - val\_loss: 0.4783 - val\_acc: 0.8129

Epoch 00182: val\_loss did not improve
Epoch 183/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4472 - acc: 0.7941 - val\_loss: 0.4802 - val\_acc: 0.7895

Epoch 00183: val\_loss did not improve
Epoch 184/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4806 - acc: 0.7471 - val\_loss: 0.4807 - val\_acc: 0.8070

Epoch 00184: val\_loss did not improve
Epoch 185/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4714 - acc: 0.7721 - val\_loss: 0.4881 - val\_acc: 0.7719

Epoch 00185: val\_loss did not improve
Epoch 186/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4958 - acc: 0.7603 - val\_loss: 0.4830 - val\_acc: 0.7953

Epoch 00186: val\_loss did not improve
Epoch 187/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4658 - acc: 0.7897 - val\_loss: 0.4864 - val\_acc: 0.7836

Epoch 00187: val\_loss did not improve
Epoch 188/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4791 - acc: 0.7588 - val\_loss: 0.4860 - val\_acc: 0.7778

Epoch 00188: val\_loss did not improve
Epoch 189/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4950 - acc: 0.7426 - val\_loss: 0.4715 - val\_acc: 0.7836

Epoch 00189: val\_loss improved from 0.47822 to 0.47149, saving model to lolkek.hdf5
Epoch 190/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4945 - acc: 0.7397 - val\_loss: 0.4596 - val\_acc: 0.7953

Epoch 00190: val\_loss improved from 0.47149 to 0.45962, saving model to lolkek.hdf5
Epoch 191/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4954 - acc: 0.7603 - val\_loss: 0.4800 - val\_acc: 0.7778

Epoch 00191: val\_loss did not improve
Epoch 192/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4739 - acc: 0.7485 - val\_loss: 0.4786 - val\_acc: 0.7895

Epoch 00192: val\_loss did not improve
Epoch 193/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4686 - acc: 0.7735 - val\_loss: 0.4821 - val\_acc: 0.7836

Epoch 00193: val\_loss did not improve
Epoch 194/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4866 - acc: 0.7456 - val\_loss: 0.4816 - val\_acc: 0.7895

Epoch 00194: val\_loss did not improve
Epoch 195/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4719 - acc: 0.7559 - val\_loss: 0.4759 - val\_acc: 0.8012

Epoch 00195: val\_loss did not improve
Epoch 196/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4493 - acc: 0.7912 - val\_loss: 0.4837 - val\_acc: 0.7602

Epoch 00196: val\_loss did not improve
Epoch 197/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4603 - acc: 0.7662 - val\_loss: 0.4713 - val\_acc: 0.8012

Epoch 00197: val\_loss did not improve
Epoch 198/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4669 - acc: 0.7588 - val\_loss: 0.4699 - val\_acc: 0.7895

Epoch 00198: val\_loss did not improve
Epoch 199/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4456 - acc: 0.7794 - val\_loss: 0.4851 - val\_acc: 0.7719

Epoch 00199: val\_loss did not improve
Epoch 200/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4736 - acc: 0.7735 - val\_loss: 0.4712 - val\_acc: 0.7778

Epoch 00200: val\_loss did not improve
Epoch 201/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4657 - acc: 0.7721 - val\_loss: 0.4731 - val\_acc: 0.7836

Epoch 00201: val\_loss did not improve
Epoch 202/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4611 - acc: 0.7750 - val\_loss: 0.4752 - val\_acc: 0.7953

Epoch 00202: val\_loss did not improve
Epoch 203/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4775 - acc: 0.7441 - val\_loss: 0.4592 - val\_acc: 0.7953

Epoch 00203: val\_loss improved from 0.45962 to 0.45917, saving model to lolkek.hdf5
Epoch 204/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4533 - acc: 0.7706 - val\_loss: 0.4566 - val\_acc: 0.7719

Epoch 00204: val\_loss improved from 0.45917 to 0.45657, saving model to lolkek.hdf5
Epoch 205/1500
680/680 [==============================] - 0s 64us/step - loss: 0.4703 - acc: 0.7603 - val\_loss: 0.4473 - val\_acc: 0.8129

Epoch 00205: val\_loss improved from 0.45657 to 0.44725, saving model to lolkek.hdf5
Epoch 206/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4647 - acc: 0.7647 - val\_loss: 0.4673 - val\_acc: 0.7895

Epoch 00206: val\_loss did not improve
Epoch 207/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4832 - acc: 0.7676 - val\_loss: 0.4624 - val\_acc: 0.7661

Epoch 00207: val\_loss did not improve
Epoch 208/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4566 - acc: 0.7735 - val\_loss: 0.4536 - val\_acc: 0.7895

Epoch 00208: val\_loss did not improve
Epoch 209/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4489 - acc: 0.7750 - val\_loss: 0.4537 - val\_acc: 0.7895

Epoch 00209: ReduceLROnPlateau reducing learning rate to 0.0018000000854954123.

Epoch 00209: val\_loss did not improve
Epoch 210/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4380 - acc: 0.7794 - val\_loss: 0.4629 - val\_acc: 0.7661

Epoch 00210: val\_loss did not improve
Epoch 211/1500
680/680 [==============================] - 0s 63us/step - loss: 0.4918 - acc: 0.7574 - val\_loss: 0.4571 - val\_acc: 0.7895

Epoch 00211: val\_loss did not improve
Epoch 212/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4524 - acc: 0.7809 - val\_loss: 0.4517 - val\_acc: 0.8012

Epoch 00212: val\_loss did not improve
Epoch 213/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4662 - acc: 0.7603 - val\_loss: 0.4544 - val\_acc: 0.7953

Epoch 00213: val\_loss did not improve
Epoch 214/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4515 - acc: 0.7809 - val\_loss: 0.4605 - val\_acc: 0.7778

Epoch 00214: val\_loss did not improve
Epoch 215/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4706 - acc: 0.7544 - val\_loss: 0.4552 - val\_acc: 0.7836

Epoch 00215: val\_loss did not improve
Epoch 216/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4516 - acc: 0.7868 - val\_loss: 0.4535 - val\_acc: 0.7953

Epoch 00216: val\_loss did not improve
Epoch 217/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4560 - acc: 0.7794 - val\_loss: 0.4633 - val\_acc: 0.7661

Epoch 00217: val\_loss did not improve
Epoch 218/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4541 - acc: 0.7779 - val\_loss: 0.4617 - val\_acc: 0.7953

Epoch 00218: val\_loss did not improve
Epoch 219/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4571 - acc: 0.7882 - val\_loss: 0.4621 - val\_acc: 0.8246

Epoch 00219: val\_loss did not improve
Epoch 220/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4499 - acc: 0.7779 - val\_loss: 0.4593 - val\_acc: 0.7895

Epoch 00220: val\_loss did not improve
Epoch 221/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4223 - acc: 0.7897 - val\_loss: 0.4648 - val\_acc: 0.7836

Epoch 00221: val\_loss did not improve
Epoch 222/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4619 - acc: 0.7809 - val\_loss: 0.4567 - val\_acc: 0.7953

Epoch 00222: val\_loss did not improve
Epoch 223/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4652 - acc: 0.7676 - val\_loss: 0.4621 - val\_acc: 0.7895

Epoch 00223: val\_loss did not improve
Epoch 224/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4590 - acc: 0.7809 - val\_loss: 0.4619 - val\_acc: 0.7895

Epoch 00224: val\_loss did not improve
Epoch 225/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4407 - acc: 0.7897 - val\_loss: 0.4599 - val\_acc: 0.7602

Epoch 00225: val\_loss did not improve
Epoch 226/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4536 - acc: 0.7853 - val\_loss: 0.4622 - val\_acc: 0.7778

Epoch 00226: val\_loss did not improve
Epoch 227/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4621 - acc: 0.7603 - val\_loss: 0.4611 - val\_acc: 0.7953

Epoch 00227: val\_loss did not improve
Epoch 228/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4220 - acc: 0.7941 - val\_loss: 0.4486 - val\_acc: 0.8187

Epoch 00228: val\_loss did not improve
Epoch 229/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4610 - acc: 0.7721 - val\_loss: 0.4425 - val\_acc: 0.8012

Epoch 00229: val\_loss improved from 0.44725 to 0.44253, saving model to lolkek.hdf5
Epoch 230/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4267 - acc: 0.7985 - val\_loss: 0.4390 - val\_acc: 0.8187

Epoch 00230: val\_loss improved from 0.44253 to 0.43904, saving model to lolkek.hdf5
Epoch 231/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4401 - acc: 0.7897 - val\_loss: 0.4444 - val\_acc: 0.8070

Epoch 00231: val\_loss did not improve
Epoch 232/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4464 - acc: 0.7956 - val\_loss: 0.4427 - val\_acc: 0.8187

Epoch 00232: val\_loss did not improve
Epoch 233/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7853 - val\_loss: 0.4398 - val\_acc: 0.8070

Epoch 00233: val\_loss did not improve
Epoch 234/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4601 - acc: 0.7735 - val\_loss: 0.4330 - val\_acc: 0.8012

Epoch 00234: val\_loss improved from 0.43904 to 0.43301, saving model to lolkek.hdf5
Epoch 235/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4447 - acc: 0.7882 - val\_loss: 0.4400 - val\_acc: 0.7953

Epoch 00235: val\_loss did not improve
Epoch 236/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4454 - acc: 0.7735 - val\_loss: 0.4508 - val\_acc: 0.8187

Epoch 00236: val\_loss did not improve
Epoch 237/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4383 - acc: 0.7765 - val\_loss: 0.4482 - val\_acc: 0.8070

Epoch 00237: val\_loss did not improve
Epoch 238/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4495 - acc: 0.7721 - val\_loss: 0.4496 - val\_acc: 0.7953

Epoch 00238: val\_loss did not improve
Epoch 239/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4104 - acc: 0.8132 - val\_loss: 0.4470 - val\_acc: 0.8246

Epoch 00239: val\_loss did not improve
Epoch 240/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4258 - acc: 0.8044 - val\_loss: 0.4490 - val\_acc: 0.7895

Epoch 00240: val\_loss did not improve
Epoch 241/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4474 - acc: 0.7882 - val\_loss: 0.4430 - val\_acc: 0.8012

Epoch 00241: val\_loss did not improve
Epoch 242/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4399 - acc: 0.7882 - val\_loss: 0.4337 - val\_acc: 0.8129

Epoch 00242: val\_loss did not improve
Epoch 243/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4706 - acc: 0.7809 - val\_loss: 0.4341 - val\_acc: 0.8187

Epoch 00243: val\_loss did not improve
Epoch 244/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4347 - acc: 0.7912 - val\_loss: 0.4454 - val\_acc: 0.7895

Epoch 00244: val\_loss did not improve
Epoch 245/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4319 - acc: 0.8088 - val\_loss: 0.4378 - val\_acc: 0.8187

Epoch 00245: val\_loss did not improve
Epoch 246/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4372 - acc: 0.7897 - val\_loss: 0.4326 - val\_acc: 0.8129

Epoch 00246: val\_loss improved from 0.43301 to 0.43264, saving model to lolkek.hdf5
Epoch 247/1500
680/680 [==============================] - 0s 63us/step - loss: 0.4300 - acc: 0.7985 - val\_loss: 0.4170 - val\_acc: 0.8304

Epoch 00247: val\_loss improved from 0.43264 to 0.41705, saving model to lolkek.hdf5
Epoch 248/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4274 - acc: 0.8059 - val\_loss: 0.4155 - val\_acc: 0.8421

Epoch 00248: val\_loss improved from 0.41705 to 0.41547, saving model to lolkek.hdf5
Epoch 249/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4237 - acc: 0.7838 - val\_loss: 0.4280 - val\_acc: 0.8187

Epoch 00249: val\_loss did not improve
Epoch 250/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4324 - acc: 0.7882 - val\_loss: 0.4448 - val\_acc: 0.8129

Epoch 00250: val\_loss did not improve
Epoch 251/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4178 - acc: 0.8015 - val\_loss: 0.4359 - val\_acc: 0.8012

Epoch 00251: val\_loss did not improve
Epoch 252/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4288 - acc: 0.7882 - val\_loss: 0.4303 - val\_acc: 0.7895

Epoch 00252: val\_loss did not improve
Epoch 253/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4240 - acc: 0.7985 - val\_loss: 0.4377 - val\_acc: 0.8187

Epoch 00253: val\_loss did not improve
Epoch 254/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4368 - acc: 0.7882 - val\_loss: 0.4264 - val\_acc: 0.8012

Epoch 00254: val\_loss did not improve
Epoch 255/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4391 - acc: 0.7765 - val\_loss: 0.4189 - val\_acc: 0.8363

Epoch 00255: val\_loss did not improve
Epoch 256/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4054 - acc: 0.8029 - val\_loss: 0.4098 - val\_acc: 0.8363

Epoch 00256: val\_loss improved from 0.41547 to 0.40976, saving model to lolkek.hdf5
Epoch 257/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7779 - val\_loss: 0.4045 - val\_acc: 0.8421

Epoch 00257: val\_loss improved from 0.40976 to 0.40446, saving model to lolkek.hdf5
Epoch 258/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3963 - acc: 0.8191 - val\_loss: 0.4117 - val\_acc: 0.8713

Epoch 00258: val\_loss did not improve
Epoch 259/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4309 - acc: 0.7779 - val\_loss: 0.4045 - val\_acc: 0.8538

Epoch 00259: val\_loss did not improve
Epoch 260/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4209 - acc: 0.8103 - val\_loss: 0.3959 - val\_acc: 0.8596

Epoch 00260: val\_loss improved from 0.40446 to 0.39591, saving model to lolkek.hdf5
Epoch 261/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4207 - acc: 0.8000 - val\_loss: 0.4044 - val\_acc: 0.8480

Epoch 00261: val\_loss did not improve
Epoch 262/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4008 - acc: 0.8176 - val\_loss: 0.4108 - val\_acc: 0.8421

Epoch 00262: val\_loss did not improve
Epoch 263/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4393 - acc: 0.7824 - val\_loss: 0.4100 - val\_acc: 0.8596

Epoch 00263: val\_loss did not improve
Epoch 264/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3842 - acc: 0.8324 - val\_loss: 0.4127 - val\_acc: 0.8480

Epoch 00264: val\_loss did not improve
Epoch 265/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4023 - acc: 0.8103 - val\_loss: 0.4082 - val\_acc: 0.8363

Epoch 00265: val\_loss did not improve
Epoch 266/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3982 - acc: 0.8265 - val\_loss: 0.4022 - val\_acc: 0.8596

Epoch 00266: val\_loss did not improve
Epoch 267/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4006 - acc: 0.7956 - val\_loss: 0.4170 - val\_acc: 0.8304

Epoch 00267: val\_loss did not improve
Epoch 268/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4249 - acc: 0.7956 - val\_loss: 0.4164 - val\_acc: 0.8421

Epoch 00268: val\_loss did not improve
Epoch 269/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4080 - acc: 0.8044 - val\_loss: 0.4097 - val\_acc: 0.8421

Epoch 00269: val\_loss did not improve
Epoch 270/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4388 - acc: 0.7985 - val\_loss: 0.4096 - val\_acc: 0.8304

Epoch 00270: val\_loss did not improve
Epoch 271/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4378 - acc: 0.7912 - val\_loss: 0.4120 - val\_acc: 0.8713

Epoch 00271: val\_loss did not improve
Epoch 272/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4400 - acc: 0.7897 - val\_loss: 0.4153 - val\_acc: 0.8655

Epoch 00272: val\_loss did not improve
Epoch 273/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3991 - acc: 0.8279 - val\_loss: 0.4123 - val\_acc: 0.8596

Epoch 00273: val\_loss did not improve
Epoch 274/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3864 - acc: 0.8221 - val\_loss: 0.4086 - val\_acc: 0.8655

Epoch 00274: val\_loss did not improve
Epoch 275/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3963 - acc: 0.8088 - val\_loss: 0.4086 - val\_acc: 0.8538

Epoch 00275: val\_loss did not improve
Epoch 276/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4077 - acc: 0.8118 - val\_loss: 0.4109 - val\_acc: 0.8655

Epoch 00276: val\_loss did not improve
Epoch 277/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4082 - acc: 0.8147 - val\_loss: 0.4179 - val\_acc: 0.8480

Epoch 00277: val\_loss did not improve
Epoch 278/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4052 - acc: 0.8103 - val\_loss: 0.4135 - val\_acc: 0.8363

Epoch 00278: val\_loss did not improve
Epoch 279/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4093 - acc: 0.8059 - val\_loss: 0.3965 - val\_acc: 0.8421

Epoch 00279: val\_loss did not improve
Epoch 280/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3916 - acc: 0.8162 - val\_loss: 0.3878 - val\_acc: 0.8538

Epoch 00280: val\_loss improved from 0.39591 to 0.38784, saving model to lolkek.hdf5
Epoch 281/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4233 - acc: 0.7956 - val\_loss: 0.3913 - val\_acc: 0.8480

Epoch 00281: val\_loss did not improve
Epoch 282/1500
680/680 [==============================] - 0s 63us/step - loss: 0.4086 - acc: 0.8191 - val\_loss: 0.3955 - val\_acc: 0.8655

Epoch 00282: val\_loss did not improve
Epoch 283/1500
680/680 [==============================] - 0s 67us/step - loss: 0.4020 - acc: 0.7971 - val\_loss: 0.3950 - val\_acc: 0.8655

Epoch 00283: val\_loss did not improve
Epoch 284/1500
680/680 [==============================] - 0s 68us/step - loss: 0.3797 - acc: 0.8162 - val\_loss: 0.4033 - val\_acc: 0.8421

Epoch 00284: val\_loss did not improve
Epoch 285/1500
680/680 [==============================] - 0s 70us/step - loss: 0.4003 - acc: 0.7882 - val\_loss: 0.3991 - val\_acc: 0.8713

Epoch 00285: val\_loss did not improve
Epoch 286/1500
680/680 [==============================] - 0s 63us/step - loss: 0.4150 - acc: 0.8074 - val\_loss: 0.3967 - val\_acc: 0.8655

Epoch 00286: val\_loss did not improve
Epoch 287/1500
680/680 [==============================] - 0s 65us/step - loss: 0.4011 - acc: 0.8191 - val\_loss: 0.3961 - val\_acc: 0.8363

Epoch 00287: val\_loss did not improve
Epoch 288/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4205 - acc: 0.7956 - val\_loss: 0.3896 - val\_acc: 0.8363

Epoch 00288: val\_loss did not improve
Epoch 289/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3986 - acc: 0.8265 - val\_loss: 0.3824 - val\_acc: 0.8596

Epoch 00289: ReduceLROnPlateau reducing learning rate to 0.0016200000769458712.

Epoch 00289: val\_loss improved from 0.38784 to 0.38237, saving model to lolkek.hdf5
Epoch 290/1500
680/680 [==============================] - 0s 61us/step - loss: 0.4098 - acc: 0.8059 - val\_loss: 0.3756 - val\_acc: 0.8772

Epoch 00290: val\_loss improved from 0.38237 to 0.37563, saving model to lolkek.hdf5
Epoch 291/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3870 - acc: 0.8132 - val\_loss: 0.3818 - val\_acc: 0.8596

Epoch 00291: val\_loss did not improve
Epoch 292/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3979 - acc: 0.8074 - val\_loss: 0.3867 - val\_acc: 0.8421

Epoch 00292: val\_loss did not improve
Epoch 293/1500
680/680 [==============================] - 0s 56us/step - loss: 0.4308 - acc: 0.7882 - val\_loss: 0.4120 - val\_acc: 0.8304

Epoch 00293: val\_loss did not improve
Epoch 294/1500
680/680 [==============================] - 0s 55us/step - loss: 0.4220 - acc: 0.8015 - val\_loss: 0.4064 - val\_acc: 0.8304

Epoch 00294: val\_loss did not improve
Epoch 295/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4256 - acc: 0.7941 - val\_loss: 0.4101 - val\_acc: 0.8363

Epoch 00295: val\_loss did not improve
Epoch 296/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4090 - acc: 0.8044 - val\_loss: 0.4069 - val\_acc: 0.8421

Epoch 00296: val\_loss did not improve
Epoch 297/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4044 - acc: 0.8191 - val\_loss: 0.3966 - val\_acc: 0.8655

Epoch 00297: val\_loss did not improve
Epoch 298/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4058 - acc: 0.8162 - val\_loss: 0.3902 - val\_acc: 0.8713

Epoch 00298: val\_loss did not improve
Epoch 299/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3889 - acc: 0.8191 - val\_loss: 0.3953 - val\_acc: 0.8713

Epoch 00299: val\_loss did not improve
Epoch 300/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4064 - acc: 0.8059 - val\_loss: 0.3925 - val\_acc: 0.8596

Epoch 00300: val\_loss did not improve
Epoch 301/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3958 - acc: 0.8162 - val\_loss: 0.3980 - val\_acc: 0.8304

Epoch 00301: val\_loss did not improve
Epoch 302/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3966 - acc: 0.8191 - val\_loss: 0.4065 - val\_acc: 0.8596

Epoch 00302: val\_loss did not improve
Epoch 303/1500
680/680 [==============================] - 0s 63us/step - loss: 0.4077 - acc: 0.8000 - val\_loss: 0.3939 - val\_acc: 0.8713

Epoch 00303: val\_loss did not improve
Epoch 304/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4208 - acc: 0.7882 - val\_loss: 0.3942 - val\_acc: 0.8480

Epoch 00304: val\_loss did not improve
Epoch 305/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4038 - acc: 0.8029 - val\_loss: 0.3899 - val\_acc: 0.8713

Epoch 00305: val\_loss did not improve
Epoch 306/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3849 - acc: 0.8250 - val\_loss: 0.3868 - val\_acc: 0.8538

Epoch 00306: val\_loss did not improve
Epoch 307/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4010 - acc: 0.7897 - val\_loss: 0.3821 - val\_acc: 0.8596

Epoch 00307: val\_loss did not improve
Epoch 308/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3948 - acc: 0.8044 - val\_loss: 0.3841 - val\_acc: 0.8772

Epoch 00308: val\_loss did not improve
Epoch 309/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3858 - acc: 0.8147 - val\_loss: 0.3941 - val\_acc: 0.8772

Epoch 00309: val\_loss did not improve
Epoch 310/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3929 - acc: 0.8132 - val\_loss: 0.3858 - val\_acc: 0.8713

Epoch 00310: val\_loss did not improve
Epoch 311/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4062 - acc: 0.8250 - val\_loss: 0.3840 - val\_acc: 0.8830

Epoch 00311: val\_loss did not improve
Epoch 312/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3784 - acc: 0.8265 - val\_loss: 0.3789 - val\_acc: 0.8830

Epoch 00312: val\_loss did not improve
Epoch 313/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3891 - acc: 0.8132 - val\_loss: 0.3808 - val\_acc: 0.8772

Epoch 00313: val\_loss did not improve
Epoch 314/1500
680/680 [==============================] - 0s 67us/step - loss: 0.4036 - acc: 0.8088 - val\_loss: 0.3831 - val\_acc: 0.8538

Epoch 00314: val\_loss did not improve
Epoch 315/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3609 - acc: 0.8250 - val\_loss: 0.3892 - val\_acc: 0.8772

Epoch 00315: val\_loss did not improve
Epoch 316/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3788 - acc: 0.8191 - val\_loss: 0.3865 - val\_acc: 0.8655

Epoch 00316: val\_loss did not improve
Epoch 317/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3778 - acc: 0.8221 - val\_loss: 0.3810 - val\_acc: 0.8713

Epoch 00317: val\_loss did not improve
Epoch 318/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4053 - acc: 0.8132 - val\_loss: 0.3901 - val\_acc: 0.8596

Epoch 00318: val\_loss did not improve
Epoch 319/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3703 - acc: 0.8397 - val\_loss: 0.4112 - val\_acc: 0.8596

Epoch 00319: val\_loss did not improve
Epoch 320/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3945 - acc: 0.8265 - val\_loss: 0.3991 - val\_acc: 0.8538

Epoch 00320: val\_loss did not improve
Epoch 321/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4065 - acc: 0.8103 - val\_loss: 0.4012 - val\_acc: 0.8480

Epoch 00321: val\_loss did not improve
Epoch 322/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3896 - acc: 0.8059 - val\_loss: 0.3988 - val\_acc: 0.8480

Epoch 00322: val\_loss did not improve
Epoch 323/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3895 - acc: 0.8206 - val\_loss: 0.3993 - val\_acc: 0.8363

Epoch 00323: val\_loss did not improve
Epoch 324/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3974 - acc: 0.8088 - val\_loss: 0.3993 - val\_acc: 0.8363

Epoch 00324: val\_loss did not improve
Epoch 325/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3957 - acc: 0.8353 - val\_loss: 0.3901 - val\_acc: 0.8772

Epoch 00325: val\_loss did not improve
Epoch 326/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3942 - acc: 0.8103 - val\_loss: 0.3849 - val\_acc: 0.8655

Epoch 00326: val\_loss did not improve
Epoch 327/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4096 - acc: 0.8074 - val\_loss: 0.3828 - val\_acc: 0.8713

Epoch 00327: val\_loss did not improve
Epoch 328/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3613 - acc: 0.8279 - val\_loss: 0.3923 - val\_acc: 0.8538

Epoch 00328: val\_loss did not improve
Epoch 329/1500
680/680 [==============================] - 0s 58us/step - loss: 0.4044 - acc: 0.8059 - val\_loss: 0.3946 - val\_acc: 0.8538

Epoch 00329: val\_loss did not improve
Epoch 330/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3876 - acc: 0.8235 - val\_loss: 0.3919 - val\_acc: 0.8596

Epoch 00330: val\_loss did not improve
Epoch 331/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3938 - acc: 0.8162 - val\_loss: 0.3944 - val\_acc: 0.8655

Epoch 00331: val\_loss did not improve
Epoch 332/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3893 - acc: 0.8103 - val\_loss: 0.3873 - val\_acc: 0.8655

Epoch 00332: val\_loss did not improve
Epoch 333/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3777 - acc: 0.8279 - val\_loss: 0.3879 - val\_acc: 0.8596

Epoch 00333: val\_loss did not improve
Epoch 334/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3845 - acc: 0.8279 - val\_loss: 0.3814 - val\_acc: 0.8538

Epoch 00334: val\_loss did not improve
Epoch 335/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3792 - acc: 0.8279 - val\_loss: 0.3876 - val\_acc: 0.8596

Epoch 00335: val\_loss did not improve
Epoch 336/1500
680/680 [==============================] - 0s 60us/step - loss: 0.4057 - acc: 0.7985 - val\_loss: 0.3827 - val\_acc: 0.8480

Epoch 00336: val\_loss did not improve
Epoch 337/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3642 - acc: 0.8279 - val\_loss: 0.3867 - val\_acc: 0.8480

Epoch 00337: val\_loss did not improve
Epoch 338/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3735 - acc: 0.8235 - val\_loss: 0.3896 - val\_acc: 0.8480

Epoch 00338: val\_loss did not improve
Epoch 339/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3773 - acc: 0.8309 - val\_loss: 0.3918 - val\_acc: 0.8538

Epoch 00339: val\_loss did not improve
Epoch 340/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3597 - acc: 0.8397 - val\_loss: 0.3889 - val\_acc: 0.8772

Epoch 00340: val\_loss did not improve
Epoch 341/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3852 - acc: 0.8088 - val\_loss: 0.3889 - val\_acc: 0.8713

Epoch 00341: val\_loss did not improve
Epoch 342/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3848 - acc: 0.8426 - val\_loss: 0.3899 - val\_acc: 0.8713

Epoch 00342: ReduceLROnPlateau reducing learning rate to 0.001458000100683421.

Epoch 00342: val\_loss did not improve
Epoch 343/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3719 - acc: 0.8353 - val\_loss: 0.3893 - val\_acc: 0.8889

Epoch 00343: val\_loss did not improve
Epoch 344/1500
680/680 [==============================] - 0s 59us/step - loss: 0.4069 - acc: 0.8044 - val\_loss: 0.3733 - val\_acc: 0.9006

Epoch 00344: val\_loss improved from 0.37563 to 0.37329, saving model to lolkek.hdf5
Epoch 345/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3701 - acc: 0.8279 - val\_loss: 0.3710 - val\_acc: 0.9006

Epoch 00345: val\_loss improved from 0.37329 to 0.37101, saving model to lolkek.hdf5
Epoch 346/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3539 - acc: 0.8397 - val\_loss: 0.3772 - val\_acc: 0.8772

Epoch 00346: val\_loss did not improve
Epoch 347/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3793 - acc: 0.8324 - val\_loss: 0.3770 - val\_acc: 0.9006

Epoch 00347: val\_loss did not improve
Epoch 348/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3652 - acc: 0.8382 - val\_loss: 0.3833 - val\_acc: 0.8830

Epoch 00348: val\_loss did not improve
Epoch 349/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3620 - acc: 0.8338 - val\_loss: 0.3778 - val\_acc: 0.8830

Epoch 00349: val\_loss did not improve
Epoch 350/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3616 - acc: 0.8382 - val\_loss: 0.3770 - val\_acc: 0.8830

Epoch 00350: val\_loss did not improve
Epoch 351/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3693 - acc: 0.8338 - val\_loss: 0.3714 - val\_acc: 0.8947

Epoch 00351: val\_loss did not improve
Epoch 352/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3906 - acc: 0.8103 - val\_loss: 0.3652 - val\_acc: 0.8889

Epoch 00352: val\_loss improved from 0.37101 to 0.36524, saving model to lolkek.hdf5
Epoch 353/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3824 - acc: 0.8221 - val\_loss: 0.3611 - val\_acc: 0.8830

Epoch 00353: val\_loss improved from 0.36524 to 0.36111, saving model to lolkek.hdf5
Epoch 354/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3811 - acc: 0.8265 - val\_loss: 0.3664 - val\_acc: 0.8947

Epoch 00354: val\_loss did not improve
Epoch 355/1500
680/680 [==============================] - 0s 62us/step - loss: 0.4096 - acc: 0.8029 - val\_loss: 0.3845 - val\_acc: 0.8772

Epoch 00355: val\_loss did not improve
Epoch 356/1500
680/680 [==============================] - 0s 57us/step - loss: 0.4104 - acc: 0.8088 - val\_loss: 0.3779 - val\_acc: 0.8830

Epoch 00356: val\_loss did not improve
Epoch 357/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3749 - acc: 0.8338 - val\_loss: 0.3751 - val\_acc: 0.8830

Epoch 00357: val\_loss did not improve
Epoch 358/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3871 - acc: 0.8147 - val\_loss: 0.3790 - val\_acc: 0.8830

Epoch 00358: val\_loss did not improve
Epoch 359/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3718 - acc: 0.8235 - val\_loss: 0.3770 - val\_acc: 0.8772

Epoch 00359: val\_loss did not improve
Epoch 360/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3529 - acc: 0.8382 - val\_loss: 0.3776 - val\_acc: 0.8889

Epoch 00360: val\_loss did not improve
Epoch 361/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3282 - acc: 0.8603 - val\_loss: 0.3716 - val\_acc: 0.8830

Epoch 00361: val\_loss did not improve
Epoch 362/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3592 - acc: 0.8471 - val\_loss: 0.3741 - val\_acc: 0.8772

Epoch 00362: val\_loss did not improve
Epoch 363/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3657 - acc: 0.8397 - val\_loss: 0.3788 - val\_acc: 0.8830

Epoch 00363: val\_loss did not improve
Epoch 364/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3896 - acc: 0.8074 - val\_loss: 0.3663 - val\_acc: 0.8830

Epoch 00364: val\_loss did not improve
Epoch 365/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3496 - acc: 0.8529 - val\_loss: 0.3682 - val\_acc: 0.8947

Epoch 00365: val\_loss did not improve
Epoch 366/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3613 - acc: 0.8397 - val\_loss: 0.3756 - val\_acc: 0.8713

Epoch 00366: val\_loss did not improve
Epoch 367/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3809 - acc: 0.8279 - val\_loss: 0.3698 - val\_acc: 0.8889

Epoch 00367: val\_loss did not improve
Epoch 368/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3700 - acc: 0.8309 - val\_loss: 0.3824 - val\_acc: 0.8655

Epoch 00368: val\_loss did not improve
Epoch 369/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3784 - acc: 0.8206 - val\_loss: 0.3840 - val\_acc: 0.8538

Epoch 00369: val\_loss did not improve
Epoch 370/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3810 - acc: 0.8191 - val\_loss: 0.3809 - val\_acc: 0.8830

Epoch 00370: val\_loss did not improve
Epoch 371/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3800 - acc: 0.8235 - val\_loss: 0.3806 - val\_acc: 0.8713

Epoch 00371: val\_loss did not improve
Epoch 372/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3428 - acc: 0.8441 - val\_loss: 0.3926 - val\_acc: 0.8713

Epoch 00372: val\_loss did not improve
Epoch 373/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3581 - acc: 0.8397 - val\_loss: 0.3769 - val\_acc: 0.8655

Epoch 00373: val\_loss did not improve
Epoch 374/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3565 - acc: 0.8279 - val\_loss: 0.3811 - val\_acc: 0.8830

Epoch 00374: val\_loss did not improve
Epoch 375/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3624 - acc: 0.8426 - val\_loss: 0.3778 - val\_acc: 0.8830

Epoch 00375: ReduceLROnPlateau reducing learning rate to 0.0013122001430019737.

Epoch 00375: val\_loss did not improve
Epoch 376/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3531 - acc: 0.8309 - val\_loss: 0.3734 - val\_acc: 0.8772

Epoch 00376: val\_loss did not improve
Epoch 377/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3662 - acc: 0.8397 - val\_loss: 0.3727 - val\_acc: 0.8772

Epoch 00377: val\_loss did not improve
Epoch 378/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3910 - acc: 0.8309 - val\_loss: 0.3806 - val\_acc: 0.8713

Epoch 00378: val\_loss did not improve
Epoch 379/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3759 - acc: 0.8368 - val\_loss: 0.3861 - val\_acc: 0.8713

Epoch 00379: val\_loss did not improve
Epoch 380/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3804 - acc: 0.8279 - val\_loss: 0.3785 - val\_acc: 0.8830

Epoch 00380: val\_loss did not improve
Epoch 381/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3772 - acc: 0.8221 - val\_loss: 0.3729 - val\_acc: 0.8830

Epoch 00381: val\_loss did not improve
Epoch 382/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3670 - acc: 0.8235 - val\_loss: 0.3691 - val\_acc: 0.8889

Epoch 00382: val\_loss did not improve
Epoch 383/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3783 - acc: 0.8279 - val\_loss: 0.3679 - val\_acc: 0.8889

Epoch 00383: val\_loss did not improve
Epoch 384/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3586 - acc: 0.8412 - val\_loss: 0.3757 - val\_acc: 0.8889

Epoch 00384: val\_loss did not improve
Epoch 385/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3599 - acc: 0.8353 - val\_loss: 0.3792 - val\_acc: 0.8889

Epoch 00385: val\_loss did not improve
Epoch 386/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3400 - acc: 0.8544 - val\_loss: 0.3687 - val\_acc: 0.8889

Epoch 00386: val\_loss did not improve
Epoch 387/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3380 - acc: 0.8456 - val\_loss: 0.3683 - val\_acc: 0.8772

Epoch 00387: val\_loss did not improve
Epoch 388/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3370 - acc: 0.8426 - val\_loss: 0.3671 - val\_acc: 0.8713

Epoch 00388: val\_loss did not improve
Epoch 389/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3693 - acc: 0.8368 - val\_loss: 0.3596 - val\_acc: 0.8830

Epoch 00389: val\_loss improved from 0.36111 to 0.35957, saving model to lolkek.hdf5
Epoch 390/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3505 - acc: 0.8294 - val\_loss: 0.3579 - val\_acc: 0.8947

Epoch 00390: val\_loss improved from 0.35957 to 0.35785, saving model to lolkek.hdf5
Epoch 391/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3517 - acc: 0.8515 - val\_loss: 0.3547 - val\_acc: 0.9006

Epoch 00391: val\_loss improved from 0.35785 to 0.35469, saving model to lolkek.hdf5
Epoch 392/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3464 - acc: 0.8441 - val\_loss: 0.3517 - val\_acc: 0.8947

Epoch 00392: val\_loss improved from 0.35469 to 0.35172, saving model to lolkek.hdf5
Epoch 393/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3768 - acc: 0.8309 - val\_loss: 0.3559 - val\_acc: 0.8889

Epoch 00393: val\_loss did not improve
Epoch 394/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3669 - acc: 0.8265 - val\_loss: 0.3558 - val\_acc: 0.8889

Epoch 00394: val\_loss did not improve
Epoch 395/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3390 - acc: 0.8603 - val\_loss: 0.3629 - val\_acc: 0.9006

Epoch 00395: val\_loss did not improve
Epoch 396/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3390 - acc: 0.8441 - val\_loss: 0.3659 - val\_acc: 0.8889

Epoch 00396: val\_loss did not improve
Epoch 397/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3409 - acc: 0.8529 - val\_loss: 0.3773 - val\_acc: 0.8655

Epoch 00397: val\_loss did not improve
Epoch 398/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3356 - acc: 0.8515 - val\_loss: 0.3729 - val\_acc: 0.8772

Epoch 00398: val\_loss did not improve
Epoch 399/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3703 - acc: 0.8221 - val\_loss: 0.3641 - val\_acc: 0.8889

Epoch 00399: val\_loss did not improve
Epoch 400/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3502 - acc: 0.8544 - val\_loss: 0.3600 - val\_acc: 0.8947

Epoch 00400: val\_loss did not improve
Epoch 401/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3243 - acc: 0.8706 - val\_loss: 0.3648 - val\_acc: 0.9006

Epoch 00401: val\_loss did not improve
Epoch 402/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3872 - acc: 0.8250 - val\_loss: 0.3660 - val\_acc: 0.8830

Epoch 00402: val\_loss did not improve
Epoch 403/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3436 - acc: 0.8426 - val\_loss: 0.3547 - val\_acc: 0.8830

Epoch 00403: val\_loss did not improve
Epoch 404/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3525 - acc: 0.8426 - val\_loss: 0.3501 - val\_acc: 0.8947

Epoch 00404: val\_loss improved from 0.35172 to 0.35013, saving model to lolkek.hdf5
Epoch 405/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3646 - acc: 0.8294 - val\_loss: 0.3544 - val\_acc: 0.8830

Epoch 00405: ReduceLROnPlateau reducing learning rate to 0.0011809800867922605.

Epoch 00405: val\_loss did not improve
Epoch 406/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3374 - acc: 0.8515 - val\_loss: 0.3571 - val\_acc: 0.8947

Epoch 00406: val\_loss did not improve
Epoch 407/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3581 - acc: 0.8338 - val\_loss: 0.3656 - val\_acc: 0.8830

Epoch 00407: val\_loss did not improve
Epoch 408/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3823 - acc: 0.8250 - val\_loss: 0.3645 - val\_acc: 0.8830

Epoch 00408: val\_loss did not improve
Epoch 409/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3569 - acc: 0.8500 - val\_loss: 0.3591 - val\_acc: 0.8947

Epoch 00409: val\_loss did not improve
Epoch 410/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3556 - acc: 0.8397 - val\_loss: 0.3477 - val\_acc: 0.8947

Epoch 00410: val\_loss improved from 0.35013 to 0.34772, saving model to lolkek.hdf5
Epoch 411/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3415 - acc: 0.8500 - val\_loss: 0.3398 - val\_acc: 0.8889

Epoch 00411: val\_loss improved from 0.34772 to 0.33983, saving model to lolkek.hdf5
Epoch 412/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3566 - acc: 0.8309 - val\_loss: 0.3397 - val\_acc: 0.8830

Epoch 00412: val\_loss improved from 0.33983 to 0.33965, saving model to lolkek.hdf5
Epoch 413/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3374 - acc: 0.8588 - val\_loss: 0.3488 - val\_acc: 0.8947

Epoch 00413: val\_loss did not improve
Epoch 414/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3614 - acc: 0.8235 - val\_loss: 0.3549 - val\_acc: 0.8772

Epoch 00414: val\_loss did not improve
Epoch 415/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3534 - acc: 0.8309 - val\_loss: 0.3501 - val\_acc: 0.8830

Epoch 00415: val\_loss did not improve
Epoch 416/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3625 - acc: 0.8324 - val\_loss: 0.3595 - val\_acc: 0.8772

Epoch 00416: val\_loss did not improve
Epoch 417/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3300 - acc: 0.8618 - val\_loss: 0.3612 - val\_acc: 0.8713

Epoch 00417: val\_loss did not improve
Epoch 418/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3584 - acc: 0.8324 - val\_loss: 0.3587 - val\_acc: 0.8655

Epoch 00418: val\_loss did not improve
Epoch 419/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3451 - acc: 0.8500 - val\_loss: 0.3615 - val\_acc: 0.8713

Epoch 00419: val\_loss did not improve
Epoch 420/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3248 - acc: 0.8721 - val\_loss: 0.3677 - val\_acc: 0.8772

Epoch 00420: val\_loss did not improve
Epoch 421/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3604 - acc: 0.8279 - val\_loss: 0.3617 - val\_acc: 0.8889

Epoch 00421: val\_loss did not improve
Epoch 422/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3488 - acc: 0.8485 - val\_loss: 0.3597 - val\_acc: 0.8713

Epoch 00422: val\_loss did not improve
Epoch 423/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3509 - acc: 0.8441 - val\_loss: 0.3566 - val\_acc: 0.8830

Epoch 00423: val\_loss did not improve
Epoch 424/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3626 - acc: 0.8338 - val\_loss: 0.3625 - val\_acc: 0.8772

Epoch 00424: val\_loss did not improve
Epoch 425/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3709 - acc: 0.8206 - val\_loss: 0.3603 - val\_acc: 0.8830

Epoch 00425: val\_loss did not improve
Epoch 426/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3782 - acc: 0.8382 - val\_loss: 0.3500 - val\_acc: 0.8947

Epoch 00426: val\_loss did not improve
Epoch 427/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3635 - acc: 0.8250 - val\_loss: 0.3495 - val\_acc: 0.9006

Epoch 00427: val\_loss did not improve
Epoch 428/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3538 - acc: 0.8324 - val\_loss: 0.3502 - val\_acc: 0.8889

Epoch 00428: val\_loss did not improve
Epoch 429/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3546 - acc: 0.8529 - val\_loss: 0.3573 - val\_acc: 0.8830

Epoch 00429: val\_loss did not improve
Epoch 430/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3491 - acc: 0.8485 - val\_loss: 0.3538 - val\_acc: 0.9006

Epoch 00430: val\_loss did not improve
Epoch 431/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3541 - acc: 0.8647 - val\_loss: 0.3456 - val\_acc: 0.9064

Epoch 00431: val\_loss did not improve
Epoch 432/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3585 - acc: 0.8368 - val\_loss: 0.3483 - val\_acc: 0.9006

Epoch 00432: val\_loss did not improve
Epoch 433/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3297 - acc: 0.8412 - val\_loss: 0.3523 - val\_acc: 0.8889

Epoch 00433: val\_loss did not improve
Epoch 434/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3331 - acc: 0.8441 - val\_loss: 0.3558 - val\_acc: 0.8830

Epoch 00434: val\_loss did not improve
Epoch 435/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3442 - acc: 0.8441 - val\_loss: 0.3597 - val\_acc: 0.8889

Epoch 00435: val\_loss did not improve
Epoch 436/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3514 - acc: 0.8368 - val\_loss: 0.3641 - val\_acc: 0.8772

Epoch 00436: val\_loss did not improve
Epoch 437/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3334 - acc: 0.8574 - val\_loss: 0.3661 - val\_acc: 0.8772

Epoch 00437: val\_loss did not improve
Epoch 438/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3518 - acc: 0.8353 - val\_loss: 0.3640 - val\_acc: 0.8830

Epoch 00438: val\_loss did not improve
Epoch 439/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3278 - acc: 0.8618 - val\_loss: 0.3601 - val\_acc: 0.8889

Epoch 00439: val\_loss did not improve
Epoch 440/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3733 - acc: 0.8265 - val\_loss: 0.3540 - val\_acc: 0.8772

Epoch 00440: val\_loss did not improve
Epoch 441/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3245 - acc: 0.8544 - val\_loss: 0.3511 - val\_acc: 0.8713

Epoch 00441: val\_loss did not improve
Epoch 442/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3174 - acc: 0.8529 - val\_loss: 0.3522 - val\_acc: 0.8830

Epoch 00442: val\_loss did not improve
Epoch 443/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3601 - acc: 0.8309 - val\_loss: 0.3474 - val\_acc: 0.8889

Epoch 00443: val\_loss did not improve
Epoch 444/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3191 - acc: 0.8662 - val\_loss: 0.3493 - val\_acc: 0.8830

Epoch 00444: val\_loss did not improve
Epoch 445/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3350 - acc: 0.8529 - val\_loss: 0.3531 - val\_acc: 0.8713

Epoch 00445: val\_loss did not improve
Epoch 446/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3803 - acc: 0.8265 - val\_loss: 0.3490 - val\_acc: 0.8830

Epoch 00446: val\_loss did not improve
Epoch 447/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3349 - acc: 0.8662 - val\_loss: 0.3541 - val\_acc: 0.8947

Epoch 00447: val\_loss did not improve
Epoch 448/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3411 - acc: 0.8338 - val\_loss: 0.3534 - val\_acc: 0.9006

Epoch 00448: val\_loss did not improve
Epoch 449/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3456 - acc: 0.8559 - val\_loss: 0.3470 - val\_acc: 0.8947

Epoch 00449: val\_loss did not improve
Epoch 450/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3263 - acc: 0.8706 - val\_loss: 0.3429 - val\_acc: 0.8947

Epoch 00450: val\_loss did not improve
Epoch 451/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3354 - acc: 0.8544 - val\_loss: 0.3397 - val\_acc: 0.9064

Epoch 00451: val\_loss did not improve
Epoch 452/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3601 - acc: 0.8353 - val\_loss: 0.3434 - val\_acc: 0.8947

Epoch 00452: val\_loss did not improve
Epoch 453/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3374 - acc: 0.8603 - val\_loss: 0.3536 - val\_acc: 0.8947

Epoch 00453: val\_loss did not improve
Epoch 454/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3549 - acc: 0.8676 - val\_loss: 0.3543 - val\_acc: 0.8830

Epoch 00454: val\_loss did not improve
Epoch 455/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3686 - acc: 0.8191 - val\_loss: 0.3574 - val\_acc: 0.8830

Epoch 00455: val\_loss did not improve
Epoch 456/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3383 - acc: 0.8426 - val\_loss: 0.3726 - val\_acc: 0.8655

Epoch 00456: val\_loss did not improve
Epoch 457/1500
680/680 [==============================] - 0s 54us/step - loss: 0.3627 - acc: 0.8456 - val\_loss: 0.3749 - val\_acc: 0.8538

Epoch 00457: val\_loss did not improve
Epoch 458/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3242 - acc: 0.8544 - val\_loss: 0.3760 - val\_acc: 0.8655

Epoch 00458: val\_loss did not improve
Epoch 459/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3702 - acc: 0.8265 - val\_loss: 0.3679 - val\_acc: 0.8596

Epoch 00459: val\_loss did not improve
Epoch 460/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3592 - acc: 0.8309 - val\_loss: 0.3608 - val\_acc: 0.8889

Epoch 00460: val\_loss did not improve
Epoch 461/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3386 - acc: 0.8485 - val\_loss: 0.3666 - val\_acc: 0.8889

Epoch 00461: val\_loss did not improve
Epoch 462/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3309 - acc: 0.8618 - val\_loss: 0.3656 - val\_acc: 0.8772

Epoch 00462: ReduceLROnPlateau reducing learning rate to 0.0010628821095451713.

Epoch 00462: val\_loss did not improve
Epoch 463/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3501 - acc: 0.8353 - val\_loss: 0.3596 - val\_acc: 0.8830

Epoch 00463: val\_loss did not improve
Epoch 464/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3372 - acc: 0.8426 - val\_loss: 0.3592 - val\_acc: 0.8772

Epoch 00464: val\_loss did not improve
Epoch 465/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3267 - acc: 0.8603 - val\_loss: 0.3636 - val\_acc: 0.8772

Epoch 00465: val\_loss did not improve
Epoch 466/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3439 - acc: 0.8632 - val\_loss: 0.3633 - val\_acc: 0.8772

Epoch 00466: val\_loss did not improve
Epoch 467/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3295 - acc: 0.8603 - val\_loss: 0.3605 - val\_acc: 0.8889

Epoch 00467: val\_loss did not improve
Epoch 468/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3261 - acc: 0.8662 - val\_loss: 0.3625 - val\_acc: 0.8947

Epoch 00468: val\_loss did not improve
Epoch 469/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3043 - acc: 0.8574 - val\_loss: 0.3647 - val\_acc: 0.8889

Epoch 00469: val\_loss did not improve
Epoch 470/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3210 - acc: 0.8544 - val\_loss: 0.3618 - val\_acc: 0.8889

Epoch 00470: val\_loss did not improve
Epoch 471/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3286 - acc: 0.8529 - val\_loss: 0.3657 - val\_acc: 0.8947

Epoch 00471: val\_loss did not improve
Epoch 472/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3019 - acc: 0.8750 - val\_loss: 0.3656 - val\_acc: 0.8772

Epoch 00472: val\_loss did not improve
Epoch 473/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3743 - acc: 0.8294 - val\_loss: 0.3653 - val\_acc: 0.8772

Epoch 00473: val\_loss did not improve
Epoch 474/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3238 - acc: 0.8515 - val\_loss: 0.3663 - val\_acc: 0.8538

Epoch 00474: val\_loss did not improve
Epoch 475/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3311 - acc: 0.8618 - val\_loss: 0.3615 - val\_acc: 0.8655

Epoch 00475: val\_loss did not improve
Epoch 476/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3624 - acc: 0.8309 - val\_loss: 0.3518 - val\_acc: 0.8713

Epoch 00476: val\_loss did not improve
Epoch 477/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3352 - acc: 0.8471 - val\_loss: 0.3469 - val\_acc: 0.8830

Epoch 00477: val\_loss did not improve
Epoch 478/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3229 - acc: 0.8662 - val\_loss: 0.3524 - val\_acc: 0.8830

Epoch 00478: val\_loss did not improve
Epoch 479/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3545 - acc: 0.8338 - val\_loss: 0.3576 - val\_acc: 0.8772

Epoch 00479: val\_loss did not improve
Epoch 480/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3376 - acc: 0.8382 - val\_loss: 0.3624 - val\_acc: 0.8889

Epoch 00480: val\_loss did not improve
Epoch 481/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3445 - acc: 0.8382 - val\_loss: 0.3725 - val\_acc: 0.8772

Epoch 00481: val\_loss did not improve
Epoch 482/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3504 - acc: 0.8309 - val\_loss: 0.3734 - val\_acc: 0.8772

Epoch 00482: val\_loss did not improve
Epoch 483/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3521 - acc: 0.8382 - val\_loss: 0.3696 - val\_acc: 0.8655

Epoch 00483: val\_loss did not improve
Epoch 484/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3422 - acc: 0.8662 - val\_loss: 0.3604 - val\_acc: 0.8830

Epoch 00484: val\_loss did not improve
Epoch 485/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3625 - acc: 0.8324 - val\_loss: 0.3543 - val\_acc: 0.8830

Epoch 00485: val\_loss did not improve
Epoch 486/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3342 - acc: 0.8456 - val\_loss: 0.3538 - val\_acc: 0.8713

Epoch 00486: val\_loss did not improve
Epoch 487/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3388 - acc: 0.8559 - val\_loss: 0.3595 - val\_acc: 0.8772

Epoch 00487: val\_loss did not improve
Epoch 488/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2995 - acc: 0.8853 - val\_loss: 0.3576 - val\_acc: 0.8830

Epoch 00488: val\_loss did not improve
Epoch 489/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3075 - acc: 0.8809 - val\_loss: 0.3541 - val\_acc: 0.8889

Epoch 00489: val\_loss did not improve
Epoch 490/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3333 - acc: 0.8485 - val\_loss: 0.3518 - val\_acc: 0.8830

Epoch 00490: val\_loss did not improve
Epoch 491/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3143 - acc: 0.8618 - val\_loss: 0.3541 - val\_acc: 0.8830

Epoch 00491: val\_loss did not improve
Epoch 492/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3369 - acc: 0.8426 - val\_loss: 0.3615 - val\_acc: 0.8889

Epoch 00492: ReduceLROnPlateau reducing learning rate to 0.0009565939195454121.

Epoch 00492: val\_loss did not improve
Epoch 493/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3364 - acc: 0.8500 - val\_loss: 0.3581 - val\_acc: 0.8947

Epoch 00493: val\_loss did not improve
Epoch 494/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3159 - acc: 0.8515 - val\_loss: 0.3559 - val\_acc: 0.8947

Epoch 00494: val\_loss did not improve
Epoch 495/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3139 - acc: 0.8426 - val\_loss: 0.3568 - val\_acc: 0.8889

Epoch 00495: val\_loss did not improve
Epoch 496/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3291 - acc: 0.8515 - val\_loss: 0.3595 - val\_acc: 0.8772

Epoch 00496: val\_loss did not improve
Epoch 497/1500
680/680 [==============================] - 0s 55us/step - loss: 0.3456 - acc: 0.8353 - val\_loss: 0.3634 - val\_acc: 0.8830

Epoch 00497: val\_loss did not improve
Epoch 498/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3234 - acc: 0.8588 - val\_loss: 0.3583 - val\_acc: 0.8713

Epoch 00498: val\_loss did not improve
Epoch 499/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3272 - acc: 0.8456 - val\_loss: 0.3637 - val\_acc: 0.8596

Epoch 00499: val\_loss did not improve
Epoch 500/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3348 - acc: 0.8500 - val\_loss: 0.3687 - val\_acc: 0.8830

Epoch 00500: val\_loss did not improve
Epoch 501/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3494 - acc: 0.8338 - val\_loss: 0.3671 - val\_acc: 0.8830

Epoch 00501: val\_loss did not improve
Epoch 502/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3094 - acc: 0.8765 - val\_loss: 0.3656 - val\_acc: 0.8830

Epoch 00502: val\_loss did not improve
Epoch 503/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3415 - acc: 0.8412 - val\_loss: 0.3524 - val\_acc: 0.8830

Epoch 00503: val\_loss did not improve
Epoch 504/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3038 - acc: 0.8603 - val\_loss: 0.3506 - val\_acc: 0.8947

Epoch 00504: val\_loss did not improve
Epoch 505/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3177 - acc: 0.8574 - val\_loss: 0.3554 - val\_acc: 0.8889

Epoch 00505: val\_loss did not improve
Epoch 506/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3151 - acc: 0.8691 - val\_loss: 0.3610 - val\_acc: 0.8830

Epoch 00506: val\_loss did not improve
Epoch 507/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3211 - acc: 0.8559 - val\_loss: 0.3574 - val\_acc: 0.8830

Epoch 00507: val\_loss did not improve
Epoch 508/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3258 - acc: 0.8426 - val\_loss: 0.3546 - val\_acc: 0.8889

Epoch 00508: val\_loss did not improve
Epoch 509/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3387 - acc: 0.8309 - val\_loss: 0.3645 - val\_acc: 0.8713

Epoch 00509: val\_loss did not improve
Epoch 510/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3284 - acc: 0.8471 - val\_loss: 0.3646 - val\_acc: 0.8713

Epoch 00510: val\_loss did not improve
Epoch 511/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3109 - acc: 0.8676 - val\_loss: 0.3568 - val\_acc: 0.8830

Epoch 00511: val\_loss did not improve
Epoch 512/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3150 - acc: 0.8515 - val\_loss: 0.3516 - val\_acc: 0.8830

Epoch 00512: val\_loss did not improve
Epoch 513/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3188 - acc: 0.8574 - val\_loss: 0.3540 - val\_acc: 0.8772

Epoch 00513: val\_loss did not improve
Epoch 514/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3311 - acc: 0.8368 - val\_loss: 0.3561 - val\_acc: 0.8772

Epoch 00514: val\_loss did not improve
Epoch 515/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3295 - acc: 0.8676 - val\_loss: 0.3606 - val\_acc: 0.8713

Epoch 00515: val\_loss did not improve
Epoch 516/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3261 - acc: 0.8529 - val\_loss: 0.3622 - val\_acc: 0.8830

Epoch 00516: val\_loss did not improve
Epoch 517/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3186 - acc: 0.8500 - val\_loss: 0.3613 - val\_acc: 0.9006

Epoch 00517: val\_loss did not improve
Epoch 518/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3450 - acc: 0.8485 - val\_loss: 0.3583 - val\_acc: 0.8830

Epoch 00518: val\_loss did not improve
Epoch 519/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3173 - acc: 0.8721 - val\_loss: 0.3629 - val\_acc: 0.8655

Epoch 00519: val\_loss did not improve
Epoch 520/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3155 - acc: 0.8574 - val\_loss: 0.3640 - val\_acc: 0.8830

Epoch 00520: val\_loss did not improve
Epoch 521/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3176 - acc: 0.8721 - val\_loss: 0.3590 - val\_acc: 0.8889

Epoch 00521: val\_loss did not improve
Epoch 522/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3182 - acc: 0.8618 - val\_loss: 0.3564 - val\_acc: 0.8889

Epoch 00522: ReduceLROnPlateau reducing learning rate to 0.000860934506636113.

Epoch 00522: val\_loss did not improve
Epoch 523/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3153 - acc: 0.8603 - val\_loss: 0.3540 - val\_acc: 0.8889

Epoch 00523: val\_loss did not improve
Epoch 524/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3000 - acc: 0.8588 - val\_loss: 0.3530 - val\_acc: 0.8772

Epoch 00524: val\_loss did not improve
Epoch 525/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2950 - acc: 0.8647 - val\_loss: 0.3526 - val\_acc: 0.8772

Epoch 00525: val\_loss did not improve
Epoch 526/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3478 - acc: 0.8456 - val\_loss: 0.3543 - val\_acc: 0.8713

Epoch 00526: val\_loss did not improve
Epoch 527/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3317 - acc: 0.8574 - val\_loss: 0.3566 - val\_acc: 0.8830

Epoch 00527: val\_loss did not improve
Epoch 528/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3012 - acc: 0.8632 - val\_loss: 0.3536 - val\_acc: 0.8830

Epoch 00528: val\_loss did not improve
Epoch 529/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3183 - acc: 0.8559 - val\_loss: 0.3540 - val\_acc: 0.8772

Epoch 00529: val\_loss did not improve
Epoch 530/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3169 - acc: 0.8544 - val\_loss: 0.3576 - val\_acc: 0.8713

Epoch 00530: val\_loss did not improve
Epoch 531/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3227 - acc: 0.8691 - val\_loss: 0.3580 - val\_acc: 0.8713

Epoch 00531: val\_loss did not improve
Epoch 532/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3101 - acc: 0.8676 - val\_loss: 0.3564 - val\_acc: 0.8830

Epoch 00532: val\_loss did not improve
Epoch 533/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3017 - acc: 0.8809 - val\_loss: 0.3540 - val\_acc: 0.8772

Epoch 00533: val\_loss did not improve
Epoch 534/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3112 - acc: 0.8544 - val\_loss: 0.3494 - val\_acc: 0.8830

Epoch 00534: val\_loss did not improve
Epoch 535/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3424 - acc: 0.8353 - val\_loss: 0.3469 - val\_acc: 0.8947

Epoch 00535: val\_loss did not improve
Epoch 536/1500
680/680 [==============================] - 0s 55us/step - loss: 0.2957 - acc: 0.8765 - val\_loss: 0.3485 - val\_acc: 0.8889

Epoch 00536: val\_loss did not improve
Epoch 537/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3107 - acc: 0.8735 - val\_loss: 0.3519 - val\_acc: 0.8830

Epoch 00537: val\_loss did not improve
Epoch 538/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3322 - acc: 0.8368 - val\_loss: 0.3491 - val\_acc: 0.8830

Epoch 00538: val\_loss did not improve
Epoch 539/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3064 - acc: 0.8721 - val\_loss: 0.3508 - val\_acc: 0.8713

Epoch 00539: val\_loss did not improve
Epoch 540/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3003 - acc: 0.8750 - val\_loss: 0.3511 - val\_acc: 0.8655

Epoch 00540: val\_loss did not improve
Epoch 541/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3367 - acc: 0.8691 - val\_loss: 0.3546 - val\_acc: 0.8655

Epoch 00541: val\_loss did not improve
Epoch 542/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3185 - acc: 0.8500 - val\_loss: 0.3569 - val\_acc: 0.8772

Epoch 00542: val\_loss did not improve
Epoch 543/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3238 - acc: 0.8632 - val\_loss: 0.3541 - val\_acc: 0.8772

Epoch 00543: val\_loss did not improve
Epoch 544/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3318 - acc: 0.8706 - val\_loss: 0.3587 - val\_acc: 0.8830

Epoch 00544: val\_loss did not improve
Epoch 545/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3084 - acc: 0.8721 - val\_loss: 0.3600 - val\_acc: 0.8772

Epoch 00545: val\_loss did not improve
Epoch 546/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2973 - acc: 0.8574 - val\_loss: 0.3604 - val\_acc: 0.8713

Epoch 00546: val\_loss did not improve
Epoch 547/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3318 - acc: 0.8500 - val\_loss: 0.3605 - val\_acc: 0.8889

Epoch 00547: val\_loss did not improve
Epoch 548/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2965 - acc: 0.8926 - val\_loss: 0.3655 - val\_acc: 0.8830

Epoch 00548: val\_loss did not improve
Epoch 549/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3433 - acc: 0.8309 - val\_loss: 0.3618 - val\_acc: 0.8830

Epoch 00549: val\_loss did not improve
Epoch 550/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2965 - acc: 0.8882 - val\_loss: 0.3622 - val\_acc: 0.8947

Epoch 00550: val\_loss did not improve
Epoch 551/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2962 - acc: 0.8765 - val\_loss: 0.3595 - val\_acc: 0.8889

Epoch 00551: val\_loss did not improve
Epoch 552/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3128 - acc: 0.8721 - val\_loss: 0.3617 - val\_acc: 0.8772

Epoch 00552: ReduceLROnPlateau reducing learning rate to 0.0007748410454951227.

Epoch 00552: val\_loss did not improve
Epoch 553/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3175 - acc: 0.8559 - val\_loss: 0.3568 - val\_acc: 0.8889

Epoch 00553: val\_loss did not improve
Epoch 554/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3220 - acc: 0.8544 - val\_loss: 0.3546 - val\_acc: 0.8889

Epoch 00554: val\_loss did not improve
Epoch 555/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3270 - acc: 0.8647 - val\_loss: 0.3535 - val\_acc: 0.8889

Epoch 00555: val\_loss did not improve
Epoch 556/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3215 - acc: 0.8603 - val\_loss: 0.3558 - val\_acc: 0.8830

Epoch 00556: val\_loss did not improve
Epoch 557/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3410 - acc: 0.8559 - val\_loss: 0.3547 - val\_acc: 0.8772

Epoch 00557: val\_loss did not improve
Epoch 558/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3143 - acc: 0.8559 - val\_loss: 0.3583 - val\_acc: 0.8772

Epoch 00558: val\_loss did not improve
Epoch 559/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3081 - acc: 0.8574 - val\_loss: 0.3593 - val\_acc: 0.8655

Epoch 00559: val\_loss did not improve
Epoch 560/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3251 - acc: 0.8471 - val\_loss: 0.3484 - val\_acc: 0.8772

Epoch 00560: val\_loss did not improve
Epoch 561/1500
680/680 [==============================] - 0s 98us/step - loss: 0.3053 - acc: 0.8588 - val\_loss: 0.3463 - val\_acc: 0.8772

Epoch 00561: val\_loss did not improve
Epoch 562/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3183 - acc: 0.8706 - val\_loss: 0.3462 - val\_acc: 0.8947

Epoch 00562: val\_loss did not improve
Epoch 563/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3128 - acc: 0.8529 - val\_loss: 0.3527 - val\_acc: 0.8830

Epoch 00563: val\_loss did not improve
Epoch 564/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2999 - acc: 0.8794 - val\_loss: 0.3580 - val\_acc: 0.8713

Epoch 00564: val\_loss did not improve
Epoch 565/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3291 - acc: 0.8471 - val\_loss: 0.3619 - val\_acc: 0.8830

Epoch 00565: val\_loss did not improve
Epoch 566/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3052 - acc: 0.8735 - val\_loss: 0.3585 - val\_acc: 0.8713

Epoch 00566: val\_loss did not improve
Epoch 567/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2919 - acc: 0.8853 - val\_loss: 0.3553 - val\_acc: 0.8772

Epoch 00567: val\_loss did not improve
Epoch 568/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3487 - acc: 0.8397 - val\_loss: 0.3488 - val\_acc: 0.8889

Epoch 00568: val\_loss did not improve
Epoch 569/1500
680/680 [==============================] - 0s 54us/step - loss: 0.3251 - acc: 0.8529 - val\_loss: 0.3481 - val\_acc: 0.8889

Epoch 00569: val\_loss did not improve
Epoch 570/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3196 - acc: 0.8574 - val\_loss: 0.3487 - val\_acc: 0.8713

Epoch 00570: val\_loss did not improve
Epoch 571/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3332 - acc: 0.8471 - val\_loss: 0.3475 - val\_acc: 0.8713

Epoch 00571: val\_loss did not improve
Epoch 572/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3056 - acc: 0.8765 - val\_loss: 0.3468 - val\_acc: 0.8713

Epoch 00572: val\_loss did not improve
Epoch 573/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3065 - acc: 0.8750 - val\_loss: 0.3440 - val\_acc: 0.8830

Epoch 00573: val\_loss did not improve
Epoch 574/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3035 - acc: 0.8588 - val\_loss: 0.3467 - val\_acc: 0.8830

Epoch 00574: val\_loss did not improve
Epoch 575/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3298 - acc: 0.8412 - val\_loss: 0.3572 - val\_acc: 0.8655

Epoch 00575: val\_loss did not improve
Epoch 576/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3347 - acc: 0.8574 - val\_loss: 0.3631 - val\_acc: 0.8713

Epoch 00576: val\_loss did not improve
Epoch 577/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3012 - acc: 0.8647 - val\_loss: 0.3648 - val\_acc: 0.8713

Epoch 00577: val\_loss did not improve
Epoch 578/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3188 - acc: 0.8618 - val\_loss: 0.3606 - val\_acc: 0.8655

Epoch 00578: val\_loss did not improve
Epoch 579/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3109 - acc: 0.8735 - val\_loss: 0.3547 - val\_acc: 0.8772

Epoch 00579: val\_loss did not improve
Epoch 580/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2937 - acc: 0.8838 - val\_loss: 0.3558 - val\_acc: 0.8772

Epoch 00580: val\_loss did not improve
Epoch 581/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3064 - acc: 0.8750 - val\_loss: 0.3490 - val\_acc: 0.8947

Epoch 00581: val\_loss did not improve
Epoch 582/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3156 - acc: 0.8632 - val\_loss: 0.3475 - val\_acc: 0.8830

Epoch 00582: ReduceLROnPlateau reducing learning rate to 0.0006973569514229894.

Epoch 00582: val\_loss did not improve
Epoch 583/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3019 - acc: 0.8691 - val\_loss: 0.3479 - val\_acc: 0.8772

Epoch 00583: val\_loss did not improve
Epoch 584/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3202 - acc: 0.8676 - val\_loss: 0.3454 - val\_acc: 0.8772

Epoch 00584: val\_loss did not improve
Epoch 585/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3106 - acc: 0.8647 - val\_loss: 0.3475 - val\_acc: 0.8830

Epoch 00585: val\_loss did not improve
Epoch 586/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3146 - acc: 0.8603 - val\_loss: 0.3473 - val\_acc: 0.8772

Epoch 00586: val\_loss did not improve
Epoch 587/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2990 - acc: 0.8765 - val\_loss: 0.3469 - val\_acc: 0.8772

Epoch 00587: val\_loss did not improve
Epoch 588/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3033 - acc: 0.8721 - val\_loss: 0.3469 - val\_acc: 0.8772

Epoch 00588: val\_loss did not improve
Epoch 589/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3276 - acc: 0.8618 - val\_loss: 0.3451 - val\_acc: 0.8772

Epoch 00589: val\_loss did not improve
Epoch 590/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3211 - acc: 0.8441 - val\_loss: 0.3428 - val\_acc: 0.8713

Epoch 00590: val\_loss did not improve
Epoch 591/1500
680/680 [==============================] - 0s 56us/step - loss: 0.3395 - acc: 0.8529 - val\_loss: 0.3382 - val\_acc: 0.8772

Epoch 00591: val\_loss improved from 0.33965 to 0.33819, saving model to lolkek.hdf5
Epoch 592/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3041 - acc: 0.8838 - val\_loss: 0.3394 - val\_acc: 0.8713

Epoch 00592: val\_loss did not improve
Epoch 593/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2965 - acc: 0.8794 - val\_loss: 0.3391 - val\_acc: 0.8713

Epoch 00593: val\_loss did not improve
Epoch 594/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3160 - acc: 0.8588 - val\_loss: 0.3380 - val\_acc: 0.8713

Epoch 00594: val\_loss improved from 0.33819 to 0.33797, saving model to lolkek.hdf5
Epoch 595/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3050 - acc: 0.8647 - val\_loss: 0.3392 - val\_acc: 0.8830

Epoch 00595: val\_loss did not improve
Epoch 596/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2976 - acc: 0.8676 - val\_loss: 0.3375 - val\_acc: 0.8889

Epoch 00596: val\_loss improved from 0.33797 to 0.33746, saving model to lolkek.hdf5
Epoch 597/1500
680/680 [==============================] - 0s 99us/step - loss: 0.3242 - acc: 0.8647 - val\_loss: 0.3422 - val\_acc: 0.8713

Epoch 00597: val\_loss did not improve
Epoch 598/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3012 - acc: 0.8838 - val\_loss: 0.3445 - val\_acc: 0.8713

Epoch 00598: val\_loss did not improve
Epoch 599/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3191 - acc: 0.8603 - val\_loss: 0.3450 - val\_acc: 0.8772

Epoch 00599: val\_loss did not improve
Epoch 600/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3106 - acc: 0.8662 - val\_loss: 0.3472 - val\_acc: 0.8772

Epoch 00600: val\_loss did not improve
Epoch 601/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2944 - acc: 0.8691 - val\_loss: 0.3459 - val\_acc: 0.8772

Epoch 00601: val\_loss did not improve
Epoch 602/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2926 - acc: 0.8794 - val\_loss: 0.3456 - val\_acc: 0.8713

Epoch 00602: val\_loss did not improve
Epoch 603/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3071 - acc: 0.8706 - val\_loss: 0.3432 - val\_acc: 0.8655

Epoch 00603: val\_loss did not improve
Epoch 604/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3090 - acc: 0.8735 - val\_loss: 0.3411 - val\_acc: 0.8889

Epoch 00604: val\_loss did not improve
Epoch 605/1500
680/680 [==============================] - 0s 67us/step - loss: 0.3192 - acc: 0.8559 - val\_loss: 0.3466 - val\_acc: 0.8889

Epoch 00605: val\_loss did not improve
Epoch 606/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3431 - acc: 0.8382 - val\_loss: 0.3468 - val\_acc: 0.8772

Epoch 00606: val\_loss did not improve
Epoch 607/1500
680/680 [==============================] - 0s 68us/step - loss: 0.3057 - acc: 0.8721 - val\_loss: 0.3471 - val\_acc: 0.8889

Epoch 00607: val\_loss did not improve
Epoch 608/1500
680/680 [==============================] - 0s 71us/step - loss: 0.3058 - acc: 0.8662 - val\_loss: 0.3536 - val\_acc: 0.8830

Epoch 00608: val\_loss did not improve
Epoch 609/1500
680/680 [==============================] - 0s 70us/step - loss: 0.3121 - acc: 0.8529 - val\_loss: 0.3555 - val\_acc: 0.8830

Epoch 00609: val\_loss did not improve
Epoch 610/1500
680/680 [==============================] - 0s 67us/step - loss: 0.3009 - acc: 0.8750 - val\_loss: 0.3504 - val\_acc: 0.8830

Epoch 00610: val\_loss did not improve
Epoch 611/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2992 - acc: 0.8765 - val\_loss: 0.3486 - val\_acc: 0.8830

Epoch 00611: val\_loss did not improve
Epoch 612/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2890 - acc: 0.8765 - val\_loss: 0.3506 - val\_acc: 0.8713

Epoch 00612: ReduceLROnPlateau reducing learning rate to 0.0006276212458033115.

Epoch 00612: val\_loss did not improve
Epoch 613/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2839 - acc: 0.8794 - val\_loss: 0.3554 - val\_acc: 0.8655

Epoch 00613: val\_loss did not improve
Epoch 614/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3004 - acc: 0.8721 - val\_loss: 0.3608 - val\_acc: 0.8713

Epoch 00614: val\_loss did not improve
Epoch 615/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2595 - acc: 0.8941 - val\_loss: 0.3583 - val\_acc: 0.8772

Epoch 00615: val\_loss did not improve
Epoch 616/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3043 - acc: 0.8632 - val\_loss: 0.3561 - val\_acc: 0.8713

Epoch 00616: val\_loss did not improve
Epoch 617/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2864 - acc: 0.8794 - val\_loss: 0.3543 - val\_acc: 0.8772

Epoch 00617: val\_loss did not improve
Epoch 618/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3279 - acc: 0.8603 - val\_loss: 0.3522 - val\_acc: 0.8772

Epoch 00618: val\_loss did not improve
Epoch 619/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2854 - acc: 0.8632 - val\_loss: 0.3538 - val\_acc: 0.8713

Epoch 00619: val\_loss did not improve
Epoch 620/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2907 - acc: 0.8765 - val\_loss: 0.3530 - val\_acc: 0.8772

Epoch 00620: val\_loss did not improve
Epoch 621/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3205 - acc: 0.8441 - val\_loss: 0.3523 - val\_acc: 0.8772

Epoch 00621: val\_loss did not improve
Epoch 622/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2800 - acc: 0.8838 - val\_loss: 0.3469 - val\_acc: 0.8830

Epoch 00622: val\_loss did not improve
Epoch 623/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2903 - acc: 0.8647 - val\_loss: 0.3415 - val\_acc: 0.8889

Epoch 00623: val\_loss did not improve
Epoch 624/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2812 - acc: 0.8971 - val\_loss: 0.3397 - val\_acc: 0.8889

Epoch 00624: val\_loss did not improve
Epoch 625/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3283 - acc: 0.8515 - val\_loss: 0.3399 - val\_acc: 0.8830

Epoch 00625: val\_loss did not improve
Epoch 626/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3183 - acc: 0.8588 - val\_loss: 0.3352 - val\_acc: 0.9006

Epoch 00626: val\_loss improved from 0.33746 to 0.33517, saving model to lolkek.hdf5
Epoch 627/1500
680/680 [==============================] - 0s 67us/step - loss: 0.3037 - acc: 0.8706 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 00627: val\_loss did not improve
Epoch 628/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2848 - acc: 0.8794 - val\_loss: 0.3389 - val\_acc: 0.8947

Epoch 00628: val\_loss did not improve
Epoch 629/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3233 - acc: 0.8529 - val\_loss: 0.3389 - val\_acc: 0.8830

Epoch 00629: val\_loss did not improve
Epoch 630/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2766 - acc: 0.8824 - val\_loss: 0.3395 - val\_acc: 0.8889

Epoch 00630: val\_loss did not improve
Epoch 631/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2962 - acc: 0.8588 - val\_loss: 0.3395 - val\_acc: 0.8889

Epoch 00631: val\_loss did not improve
Epoch 632/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3139 - acc: 0.8515 - val\_loss: 0.3429 - val\_acc: 0.8947

Epoch 00632: val\_loss did not improve
Epoch 633/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3001 - acc: 0.8750 - val\_loss: 0.3457 - val\_acc: 0.8772

Epoch 00633: val\_loss did not improve
Epoch 634/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2976 - acc: 0.8779 - val\_loss: 0.3439 - val\_acc: 0.8830

Epoch 00634: val\_loss did not improve
Epoch 635/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2902 - acc: 0.8868 - val\_loss: 0.3412 - val\_acc: 0.8889

Epoch 00635: val\_loss did not improve
Epoch 636/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3181 - acc: 0.8721 - val\_loss: 0.3417 - val\_acc: 0.8830

Epoch 00636: val\_loss did not improve
Epoch 637/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2960 - acc: 0.8706 - val\_loss: 0.3413 - val\_acc: 0.8889

Epoch 00637: val\_loss did not improve
Epoch 638/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2878 - acc: 0.8721 - val\_loss: 0.3427 - val\_acc: 0.8830

Epoch 00638: val\_loss did not improve
Epoch 639/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2911 - acc: 0.8779 - val\_loss: 0.3434 - val\_acc: 0.8830

Epoch 00639: val\_loss did not improve
Epoch 640/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3022 - acc: 0.8662 - val\_loss: 0.3397 - val\_acc: 0.8830

Epoch 00640: val\_loss did not improve
Epoch 641/1500
680/680 [==============================] - 0s 81us/step - loss: 0.3319 - acc: 0.8662 - val\_loss: 0.3456 - val\_acc: 0.8889

Epoch 00641: val\_loss did not improve
Epoch 642/1500
680/680 [==============================] - 0s 71us/step - loss: 0.2732 - acc: 0.8882 - val\_loss: 0.3427 - val\_acc: 0.8889

Epoch 00642: ReduceLROnPlateau reducing learning rate to 0.0005648591264616698.

Epoch 00642: val\_loss did not improve
Epoch 643/1500
680/680 [==============================] - 0s 67us/step - loss: 0.3078 - acc: 0.8662 - val\_loss: 0.3398 - val\_acc: 0.8830

Epoch 00643: val\_loss did not improve
Epoch 644/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3104 - acc: 0.8529 - val\_loss: 0.3433 - val\_acc: 0.8830

Epoch 00644: val\_loss did not improve
Epoch 645/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2966 - acc: 0.8794 - val\_loss: 0.3418 - val\_acc: 0.8889

Epoch 00645: val\_loss did not improve
Epoch 646/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3051 - acc: 0.8618 - val\_loss: 0.3424 - val\_acc: 0.8889

Epoch 00646: val\_loss did not improve
Epoch 647/1500
680/680 [==============================] - 0s 65us/step - loss: 0.3148 - acc: 0.8632 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 00647: val\_loss did not improve
Epoch 648/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3214 - acc: 0.8397 - val\_loss: 0.3407 - val\_acc: 0.8830

Epoch 00648: val\_loss did not improve
Epoch 649/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2937 - acc: 0.8735 - val\_loss: 0.3434 - val\_acc: 0.8772

Epoch 00649: val\_loss did not improve
Epoch 650/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2848 - acc: 0.8824 - val\_loss: 0.3448 - val\_acc: 0.8713

Epoch 00650: val\_loss did not improve
Epoch 651/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3163 - acc: 0.8544 - val\_loss: 0.3456 - val\_acc: 0.8772

Epoch 00651: val\_loss did not improve
Epoch 652/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2913 - acc: 0.8735 - val\_loss: 0.3473 - val\_acc: 0.8772

Epoch 00652: val\_loss did not improve
Epoch 653/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2947 - acc: 0.8662 - val\_loss: 0.3494 - val\_acc: 0.8772

Epoch 00653: val\_loss did not improve
Epoch 654/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3319 - acc: 0.8485 - val\_loss: 0.3482 - val\_acc: 0.8772

Epoch 00654: val\_loss did not improve
Epoch 655/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2901 - acc: 0.8662 - val\_loss: 0.3477 - val\_acc: 0.8830

Epoch 00655: val\_loss did not improve
Epoch 656/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3243 - acc: 0.8529 - val\_loss: 0.3462 - val\_acc: 0.8830

Epoch 00656: val\_loss did not improve
Epoch 657/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3079 - acc: 0.8632 - val\_loss: 0.3460 - val\_acc: 0.8830

Epoch 00657: val\_loss did not improve
Epoch 658/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2899 - acc: 0.8956 - val\_loss: 0.3458 - val\_acc: 0.8830

Epoch 00658: val\_loss did not improve
Epoch 659/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2944 - acc: 0.8809 - val\_loss: 0.3447 - val\_acc: 0.8830

Epoch 00659: val\_loss did not improve
Epoch 660/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3144 - acc: 0.8485 - val\_loss: 0.3458 - val\_acc: 0.8830

Epoch 00660: val\_loss did not improve
Epoch 661/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2960 - acc: 0.8706 - val\_loss: 0.3482 - val\_acc: 0.8830

Epoch 00661: val\_loss did not improve
Epoch 662/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3207 - acc: 0.8721 - val\_loss: 0.3488 - val\_acc: 0.8772

Epoch 00662: val\_loss did not improve
Epoch 663/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3080 - acc: 0.8676 - val\_loss: 0.3519 - val\_acc: 0.8772

Epoch 00663: val\_loss did not improve
Epoch 664/1500
680/680 [==============================] - 0s 67us/step - loss: 0.3042 - acc: 0.8794 - val\_loss: 0.3544 - val\_acc: 0.8655

Epoch 00664: val\_loss did not improve
Epoch 665/1500
680/680 [==============================] - 0s 68us/step - loss: 0.3067 - acc: 0.8676 - val\_loss: 0.3560 - val\_acc: 0.8655

Epoch 00665: val\_loss did not improve
Epoch 666/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2725 - acc: 0.8779 - val\_loss: 0.3560 - val\_acc: 0.8596

Epoch 00666: val\_loss did not improve
Epoch 667/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2897 - acc: 0.8706 - val\_loss: 0.3548 - val\_acc: 0.8655

Epoch 00667: val\_loss did not improve
Epoch 668/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3085 - acc: 0.8603 - val\_loss: 0.3503 - val\_acc: 0.8772

Epoch 00668: val\_loss did not improve
Epoch 669/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2815 - acc: 0.8926 - val\_loss: 0.3491 - val\_acc: 0.8830

Epoch 00669: val\_loss did not improve
Epoch 670/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3139 - acc: 0.8544 - val\_loss: 0.3468 - val\_acc: 0.8772

Epoch 00670: val\_loss did not improve
Epoch 671/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3295 - acc: 0.8544 - val\_loss: 0.3448 - val\_acc: 0.8772

Epoch 00671: val\_loss did not improve
Epoch 672/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2983 - acc: 0.8706 - val\_loss: 0.3438 - val\_acc: 0.8713

Epoch 00672: ReduceLROnPlateau reducing learning rate to 0.0005083732190541923.

Epoch 00672: val\_loss did not improve
Epoch 673/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2720 - acc: 0.9015 - val\_loss: 0.3399 - val\_acc: 0.8713

Epoch 00673: val\_loss did not improve
Epoch 674/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3086 - acc: 0.8662 - val\_loss: 0.3426 - val\_acc: 0.8713

Epoch 00674: val\_loss did not improve
Epoch 675/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2939 - acc: 0.8824 - val\_loss: 0.3430 - val\_acc: 0.8713

Epoch 00675: val\_loss did not improve
Epoch 676/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3007 - acc: 0.8750 - val\_loss: 0.3423 - val\_acc: 0.8772

Epoch 00676: val\_loss did not improve
Epoch 677/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2974 - acc: 0.8647 - val\_loss: 0.3402 - val\_acc: 0.8889

Epoch 00677: val\_loss did not improve
Epoch 678/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2918 - acc: 0.8794 - val\_loss: 0.3426 - val\_acc: 0.8830

Epoch 00678: val\_loss did not improve
Epoch 679/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2963 - acc: 0.8662 - val\_loss: 0.3398 - val\_acc: 0.8889

Epoch 00679: val\_loss did not improve
Epoch 680/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2839 - acc: 0.8794 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 00680: val\_loss did not improve
Epoch 681/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3126 - acc: 0.8676 - val\_loss: 0.3396 - val\_acc: 0.8889

Epoch 00681: val\_loss did not improve
Epoch 682/1500
680/680 [==============================] - 0s 68us/step - loss: 0.3173 - acc: 0.8588 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 00682: val\_loss did not improve
Epoch 683/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2631 - acc: 0.8794 - val\_loss: 0.3351 - val\_acc: 0.9006

Epoch 00683: val\_loss improved from 0.33517 to 0.33508, saving model to lolkek.hdf5
Epoch 684/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2971 - acc: 0.8691 - val\_loss: 0.3371 - val\_acc: 0.9006

Epoch 00684: val\_loss did not improve
Epoch 685/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3244 - acc: 0.8559 - val\_loss: 0.3417 - val\_acc: 0.8947

Epoch 00685: val\_loss did not improve
Epoch 686/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2743 - acc: 0.8809 - val\_loss: 0.3430 - val\_acc: 0.9006

Epoch 00686: val\_loss did not improve
Epoch 687/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3071 - acc: 0.8662 - val\_loss: 0.3437 - val\_acc: 0.9064

Epoch 00687: val\_loss did not improve
Epoch 688/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2884 - acc: 0.8662 - val\_loss: 0.3463 - val\_acc: 0.9006

Epoch 00688: val\_loss did not improve
Epoch 689/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2908 - acc: 0.8779 - val\_loss: 0.3432 - val\_acc: 0.9006

Epoch 00689: val\_loss did not improve
Epoch 690/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2994 - acc: 0.8515 - val\_loss: 0.3444 - val\_acc: 0.8772

Epoch 00690: val\_loss did not improve
Epoch 691/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2730 - acc: 0.8794 - val\_loss: 0.3409 - val\_acc: 0.8889

Epoch 00691: val\_loss did not improve
Epoch 692/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2709 - acc: 0.8838 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 00692: val\_loss did not improve
Epoch 693/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2756 - acc: 0.8838 - val\_loss: 0.3432 - val\_acc: 0.8889

Epoch 00693: val\_loss did not improve
Epoch 694/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3019 - acc: 0.8676 - val\_loss: 0.3509 - val\_acc: 0.8772

Epoch 00694: val\_loss did not improve
Epoch 695/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2837 - acc: 0.8809 - val\_loss: 0.3567 - val\_acc: 0.8830

Epoch 00695: val\_loss did not improve
Epoch 696/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2994 - acc: 0.8647 - val\_loss: 0.3545 - val\_acc: 0.8772

Epoch 00696: val\_loss did not improve
Epoch 697/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2746 - acc: 0.8897 - val\_loss: 0.3509 - val\_acc: 0.8889

Epoch 00697: val\_loss did not improve
Epoch 698/1500
680/680 [==============================] - 0s 76us/step - loss: 0.2836 - acc: 0.8824 - val\_loss: 0.3466 - val\_acc: 0.8830

Epoch 00698: val\_loss did not improve
Epoch 699/1500
680/680 [==============================] - 0s 76us/step - loss: 0.2830 - acc: 0.8735 - val\_loss: 0.3524 - val\_acc: 0.8830

Epoch 00699: val\_loss did not improve
Epoch 700/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2675 - acc: 0.8809 - val\_loss: 0.3522 - val\_acc: 0.8889

Epoch 00700: val\_loss did not improve
Epoch 701/1500
680/680 [==============================] - 0s 70us/step - loss: 0.2950 - acc: 0.8662 - val\_loss: 0.3518 - val\_acc: 0.8830

Epoch 00701: val\_loss did not improve
Epoch 702/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2670 - acc: 0.8868 - val\_loss: 0.3514 - val\_acc: 0.8830

Epoch 00702: ReduceLROnPlateau reducing learning rate to 0.00045753587619401515.

Epoch 00702: val\_loss did not improve
Epoch 703/1500
680/680 [==============================] - 0s 69us/step - loss: 0.2950 - acc: 0.8691 - val\_loss: 0.3501 - val\_acc: 0.8772

Epoch 00703: val\_loss did not improve
Epoch 704/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2934 - acc: 0.8676 - val\_loss: 0.3496 - val\_acc: 0.8772

Epoch 00704: val\_loss did not improve
Epoch 705/1500
680/680 [==============================] - 0s 69us/step - loss: 0.2862 - acc: 0.8750 - val\_loss: 0.3497 - val\_acc: 0.8889

Epoch 00705: val\_loss did not improve
Epoch 706/1500
680/680 [==============================] - 0s 103us/step - loss: 0.2783 - acc: 0.8838 - val\_loss: 0.3498 - val\_acc: 0.8947

Epoch 00706: val\_loss did not improve
Epoch 707/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2745 - acc: 0.8735 - val\_loss: 0.3515 - val\_acc: 0.8889

Epoch 00707: val\_loss did not improve
Epoch 708/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2920 - acc: 0.8529 - val\_loss: 0.3542 - val\_acc: 0.8830

Epoch 00708: val\_loss did not improve
Epoch 709/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3013 - acc: 0.8750 - val\_loss: 0.3537 - val\_acc: 0.8830

Epoch 00709: val\_loss did not improve
Epoch 710/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2935 - acc: 0.8824 - val\_loss: 0.3504 - val\_acc: 0.8830

Epoch 00710: val\_loss did not improve
Epoch 711/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3090 - acc: 0.8706 - val\_loss: 0.3508 - val\_acc: 0.8772

Epoch 00711: val\_loss did not improve
Epoch 712/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2971 - acc: 0.8735 - val\_loss: 0.3515 - val\_acc: 0.8772

Epoch 00712: val\_loss did not improve
Epoch 713/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2799 - acc: 0.8838 - val\_loss: 0.3528 - val\_acc: 0.8830

Epoch 00713: val\_loss did not improve
Epoch 714/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2903 - acc: 0.8750 - val\_loss: 0.3511 - val\_acc: 0.8772

Epoch 00714: val\_loss did not improve
Epoch 715/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2919 - acc: 0.8706 - val\_loss: 0.3476 - val\_acc: 0.8830

Epoch 00715: val\_loss did not improve
Epoch 716/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3175 - acc: 0.8588 - val\_loss: 0.3448 - val\_acc: 0.8830

Epoch 00716: val\_loss did not improve
Epoch 717/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3273 - acc: 0.8529 - val\_loss: 0.3448 - val\_acc: 0.8830

Epoch 00717: val\_loss did not improve
Epoch 718/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2806 - acc: 0.8735 - val\_loss: 0.3438 - val\_acc: 0.8830

Epoch 00718: val\_loss did not improve
Epoch 719/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2810 - acc: 0.8794 - val\_loss: 0.3456 - val\_acc: 0.8830

Epoch 00719: val\_loss did not improve
Epoch 720/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2916 - acc: 0.8765 - val\_loss: 0.3462 - val\_acc: 0.8889

Epoch 00720: val\_loss did not improve
Epoch 721/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2937 - acc: 0.8676 - val\_loss: 0.3443 - val\_acc: 0.8772

Epoch 00721: val\_loss did not improve
Epoch 722/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2837 - acc: 0.8779 - val\_loss: 0.3410 - val\_acc: 0.8830

Epoch 00722: val\_loss did not improve
Epoch 723/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3077 - acc: 0.8500 - val\_loss: 0.3418 - val\_acc: 0.8830

Epoch 00723: val\_loss did not improve
Epoch 724/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2865 - acc: 0.8779 - val\_loss: 0.3472 - val\_acc: 0.8772

Epoch 00724: val\_loss did not improve
Epoch 725/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2872 - acc: 0.8765 - val\_loss: 0.3501 - val\_acc: 0.8772

Epoch 00725: val\_loss did not improve
Epoch 726/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2816 - acc: 0.8897 - val\_loss: 0.3499 - val\_acc: 0.8772

Epoch 00726: val\_loss did not improve
Epoch 727/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2776 - acc: 0.8838 - val\_loss: 0.3495 - val\_acc: 0.8830

Epoch 00727: val\_loss did not improve
Epoch 728/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3040 - acc: 0.8603 - val\_loss: 0.3508 - val\_acc: 0.8889

Epoch 00728: val\_loss did not improve
Epoch 729/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2805 - acc: 0.8735 - val\_loss: 0.3494 - val\_acc: 0.8889

Epoch 00729: val\_loss did not improve
Epoch 730/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2801 - acc: 0.8691 - val\_loss: 0.3489 - val\_acc: 0.8889

Epoch 00730: val\_loss did not improve
Epoch 731/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2810 - acc: 0.8868 - val\_loss: 0.3527 - val\_acc: 0.8830

Epoch 00731: val\_loss did not improve
Epoch 732/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2740 - acc: 0.8897 - val\_loss: 0.3554 - val\_acc: 0.8830

Epoch 00732: ReduceLROnPlateau reducing learning rate to 0.00041178228857461366.

Epoch 00732: val\_loss did not improve
Epoch 733/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3015 - acc: 0.8662 - val\_loss: 0.3561 - val\_acc: 0.8830

Epoch 00733: val\_loss did not improve
Epoch 734/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2808 - acc: 0.8632 - val\_loss: 0.3539 - val\_acc: 0.8889

Epoch 00734: val\_loss did not improve
Epoch 735/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2668 - acc: 0.8853 - val\_loss: 0.3505 - val\_acc: 0.8889

Epoch 00735: val\_loss did not improve
Epoch 736/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2739 - acc: 0.8779 - val\_loss: 0.3484 - val\_acc: 0.8830

Epoch 00736: val\_loss did not improve
Epoch 737/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2999 - acc: 0.8809 - val\_loss: 0.3485 - val\_acc: 0.8889

Epoch 00737: val\_loss did not improve
Epoch 738/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2950 - acc: 0.8559 - val\_loss: 0.3484 - val\_acc: 0.8830

Epoch 00738: val\_loss did not improve
Epoch 739/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2928 - acc: 0.8750 - val\_loss: 0.3426 - val\_acc: 0.8830

Epoch 00739: val\_loss did not improve
Epoch 740/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3019 - acc: 0.8735 - val\_loss: 0.3414 - val\_acc: 0.8830

Epoch 00740: val\_loss did not improve
Epoch 741/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3046 - acc: 0.8750 - val\_loss: 0.3421 - val\_acc: 0.8889

Epoch 00741: val\_loss did not improve
Epoch 742/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2936 - acc: 0.8735 - val\_loss: 0.3422 - val\_acc: 0.8889

Epoch 00742: val\_loss did not improve
Epoch 743/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3098 - acc: 0.8485 - val\_loss: 0.3427 - val\_acc: 0.8889

Epoch 00743: val\_loss did not improve
Epoch 744/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3017 - acc: 0.8735 - val\_loss: 0.3436 - val\_acc: 0.8889

Epoch 00744: val\_loss did not improve
Epoch 745/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2754 - acc: 0.8838 - val\_loss: 0.3423 - val\_acc: 0.8889

Epoch 00745: val\_loss did not improve
Epoch 746/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2612 - acc: 0.8941 - val\_loss: 0.3421 - val\_acc: 0.8947

Epoch 00746: val\_loss did not improve
Epoch 747/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2948 - acc: 0.8574 - val\_loss: 0.3405 - val\_acc: 0.8947

Epoch 00747: val\_loss did not improve
Epoch 748/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3136 - acc: 0.8632 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 00748: val\_loss did not improve
Epoch 749/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2903 - acc: 0.8824 - val\_loss: 0.3356 - val\_acc: 0.8947

Epoch 00749: val\_loss did not improve
Epoch 750/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2815 - acc: 0.8824 - val\_loss: 0.3345 - val\_acc: 0.9006

Epoch 00750: val\_loss improved from 0.33508 to 0.33446, saving model to lolkek.hdf5
Epoch 751/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3004 - acc: 0.8632 - val\_loss: 0.3319 - val\_acc: 0.9006

Epoch 00751: val\_loss improved from 0.33446 to 0.33189, saving model to lolkek.hdf5
Epoch 752/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3012 - acc: 0.8500 - val\_loss: 0.3334 - val\_acc: 0.8889

Epoch 00752: val\_loss did not improve
Epoch 753/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2942 - acc: 0.8691 - val\_loss: 0.3341 - val\_acc: 0.8947

Epoch 00753: val\_loss did not improve
Epoch 754/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2997 - acc: 0.8721 - val\_loss: 0.3325 - val\_acc: 0.8830

Epoch 00754: val\_loss did not improve
Epoch 755/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2926 - acc: 0.8691 - val\_loss: 0.3317 - val\_acc: 0.8889

Epoch 00755: val\_loss improved from 0.33189 to 0.33175, saving model to lolkek.hdf5
Epoch 756/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3128 - acc: 0.8544 - val\_loss: 0.3326 - val\_acc: 0.8889

Epoch 00756: val\_loss did not improve
Epoch 757/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2842 - acc: 0.8779 - val\_loss: 0.3373 - val\_acc: 0.8889

Epoch 00757: val\_loss did not improve
Epoch 758/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2947 - acc: 0.8662 - val\_loss: 0.3400 - val\_acc: 0.8889

Epoch 00758: val\_loss did not improve
Epoch 759/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3135 - acc: 0.8603 - val\_loss: 0.3400 - val\_acc: 0.8947

Epoch 00759: val\_loss did not improve
Epoch 760/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2971 - acc: 0.8721 - val\_loss: 0.3400 - val\_acc: 0.8830

Epoch 00760: val\_loss did not improve
Epoch 761/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2938 - acc: 0.8706 - val\_loss: 0.3397 - val\_acc: 0.8889

Epoch 00761: val\_loss did not improve
Epoch 762/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3115 - acc: 0.8574 - val\_loss: 0.3361 - val\_acc: 0.8947

Epoch 00762: ReduceLROnPlateau reducing learning rate to 0.0003706040675751865.

Epoch 00762: val\_loss did not improve
Epoch 763/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2891 - acc: 0.8824 - val\_loss: 0.3368 - val\_acc: 0.8947

Epoch 00763: val\_loss did not improve
Epoch 764/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3074 - acc: 0.8500 - val\_loss: 0.3407 - val\_acc: 0.8947

Epoch 00764: val\_loss did not improve
Epoch 765/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2861 - acc: 0.8794 - val\_loss: 0.3397 - val\_acc: 0.8947

Epoch 00765: val\_loss did not improve
Epoch 766/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2949 - acc: 0.8721 - val\_loss: 0.3381 - val\_acc: 0.8947

Epoch 00766: val\_loss did not improve
Epoch 767/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2913 - acc: 0.8735 - val\_loss: 0.3368 - val\_acc: 0.9006

Epoch 00767: val\_loss did not improve
Epoch 768/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2830 - acc: 0.8765 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 00768: val\_loss did not improve
Epoch 769/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2918 - acc: 0.8809 - val\_loss: 0.3367 - val\_acc: 0.9006

Epoch 00769: val\_loss did not improve
Epoch 770/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2968 - acc: 0.8735 - val\_loss: 0.3392 - val\_acc: 0.9006

Epoch 00770: val\_loss did not improve
Epoch 771/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3059 - acc: 0.8662 - val\_loss: 0.3428 - val\_acc: 0.8947

Epoch 00771: val\_loss did not improve
Epoch 772/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2837 - acc: 0.8824 - val\_loss: 0.3460 - val\_acc: 0.8947

Epoch 00772: val\_loss did not improve
Epoch 773/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3186 - acc: 0.8603 - val\_loss: 0.3448 - val\_acc: 0.8947

Epoch 00773: val\_loss did not improve
Epoch 774/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2875 - acc: 0.8809 - val\_loss: 0.3445 - val\_acc: 0.8889

Epoch 00774: val\_loss did not improve
Epoch 775/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2784 - acc: 0.8779 - val\_loss: 0.3433 - val\_acc: 0.8830

Epoch 00775: val\_loss did not improve
Epoch 776/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2630 - acc: 0.8838 - val\_loss: 0.3411 - val\_acc: 0.8889

Epoch 00776: val\_loss did not improve
Epoch 777/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2805 - acc: 0.8779 - val\_loss: 0.3418 - val\_acc: 0.8947

Epoch 00777: val\_loss did not improve
Epoch 778/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2853 - acc: 0.8765 - val\_loss: 0.3408 - val\_acc: 0.8947

Epoch 00778: val\_loss did not improve
Epoch 779/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2689 - acc: 0.8956 - val\_loss: 0.3424 - val\_acc: 0.8947

Epoch 00779: val\_loss did not improve
Epoch 780/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2750 - acc: 0.8941 - val\_loss: 0.3388 - val\_acc: 0.8947

Epoch 00780: val\_loss did not improve
Epoch 781/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2728 - acc: 0.8735 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 00781: val\_loss did not improve
Epoch 782/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2738 - acc: 0.8868 - val\_loss: 0.3386 - val\_acc: 0.8947

Epoch 00782: val\_loss did not improve
Epoch 783/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3062 - acc: 0.8750 - val\_loss: 0.3397 - val\_acc: 0.8889

Epoch 00783: val\_loss did not improve
Epoch 784/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2891 - acc: 0.8838 - val\_loss: 0.3416 - val\_acc: 0.8830

Epoch 00784: val\_loss did not improve
Epoch 785/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2881 - acc: 0.8971 - val\_loss: 0.3414 - val\_acc: 0.8947

Epoch 00785: val\_loss did not improve
Epoch 786/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2929 - acc: 0.8824 - val\_loss: 0.3421 - val\_acc: 0.8947

Epoch 00786: val\_loss did not improve
Epoch 787/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2985 - acc: 0.8691 - val\_loss: 0.3434 - val\_acc: 0.8947

Epoch 00787: val\_loss did not improve
Epoch 788/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2784 - acc: 0.8809 - val\_loss: 0.3414 - val\_acc: 0.8830

Epoch 00788: val\_loss did not improve
Epoch 789/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3035 - acc: 0.8603 - val\_loss: 0.3405 - val\_acc: 0.8889

Epoch 00789: val\_loss did not improve
Epoch 790/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2804 - acc: 0.8735 - val\_loss: 0.3399 - val\_acc: 0.8772

Epoch 00790: val\_loss did not improve
Epoch 791/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2909 - acc: 0.8721 - val\_loss: 0.3391 - val\_acc: 0.8772

Epoch 00791: val\_loss did not improve
Epoch 792/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2884 - acc: 0.8897 - val\_loss: 0.3381 - val\_acc: 0.8772

Epoch 00792: ReduceLROnPlateau reducing learning rate to 0.0003335436660563573.

Epoch 00792: val\_loss did not improve
Epoch 793/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2634 - acc: 0.8926 - val\_loss: 0.3367 - val\_acc: 0.8889

Epoch 00793: val\_loss did not improve
Epoch 794/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3105 - acc: 0.8618 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 00794: val\_loss did not improve
Epoch 795/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2715 - acc: 0.8956 - val\_loss: 0.3362 - val\_acc: 0.8947

Epoch 00795: val\_loss did not improve
Epoch 796/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2863 - acc: 0.8750 - val\_loss: 0.3354 - val\_acc: 0.9064

Epoch 00796: val\_loss did not improve
Epoch 797/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3008 - acc: 0.8882 - val\_loss: 0.3343 - val\_acc: 0.9006

Epoch 00797: val\_loss did not improve
Epoch 798/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2838 - acc: 0.8809 - val\_loss: 0.3344 - val\_acc: 0.9006

Epoch 00798: val\_loss did not improve
Epoch 799/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3067 - acc: 0.8632 - val\_loss: 0.3331 - val\_acc: 0.9006

Epoch 00799: val\_loss did not improve
Epoch 800/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2708 - acc: 0.8735 - val\_loss: 0.3316 - val\_acc: 0.9006

Epoch 00800: val\_loss improved from 0.33175 to 0.33162, saving model to lolkek.hdf5
Epoch 801/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3036 - acc: 0.8647 - val\_loss: 0.3323 - val\_acc: 0.8889

Epoch 00801: val\_loss did not improve
Epoch 802/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2986 - acc: 0.8632 - val\_loss: 0.3349 - val\_acc: 0.8830

Epoch 00802: val\_loss did not improve
Epoch 803/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2870 - acc: 0.8721 - val\_loss: 0.3361 - val\_acc: 0.8947

Epoch 00803: val\_loss did not improve
Epoch 804/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2547 - acc: 0.9059 - val\_loss: 0.3360 - val\_acc: 0.8889

Epoch 00804: val\_loss did not improve
Epoch 805/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2584 - acc: 0.8853 - val\_loss: 0.3357 - val\_acc: 0.8830

Epoch 00805: val\_loss did not improve
Epoch 806/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2612 - acc: 0.8897 - val\_loss: 0.3362 - val\_acc: 0.8830

Epoch 00806: val\_loss did not improve
Epoch 807/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2935 - acc: 0.8824 - val\_loss: 0.3381 - val\_acc: 0.8830

Epoch 00807: val\_loss did not improve
Epoch 808/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2863 - acc: 0.8809 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 00808: val\_loss did not improve
Epoch 809/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2711 - acc: 0.8912 - val\_loss: 0.3428 - val\_acc: 0.8830

Epoch 00809: val\_loss did not improve
Epoch 810/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2873 - acc: 0.8676 - val\_loss: 0.3415 - val\_acc: 0.8889

Epoch 00810: val\_loss did not improve
Epoch 811/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2791 - acc: 0.8838 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 00811: val\_loss did not improve
Epoch 812/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3073 - acc: 0.8662 - val\_loss: 0.3403 - val\_acc: 0.8889

Epoch 00812: val\_loss did not improve
Epoch 813/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2699 - acc: 0.8897 - val\_loss: 0.3396 - val\_acc: 0.8947

Epoch 00813: val\_loss did not improve
Epoch 814/1500
680/680 [==============================] - 0s 57us/step - loss: 0.3004 - acc: 0.8618 - val\_loss: 0.3381 - val\_acc: 0.8889

Epoch 00814: val\_loss did not improve
Epoch 815/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2797 - acc: 0.8735 - val\_loss: 0.3376 - val\_acc: 0.8947

Epoch 00815: val\_loss did not improve
Epoch 816/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2757 - acc: 0.8868 - val\_loss: 0.3392 - val\_acc: 0.8830

Epoch 00816: val\_loss did not improve
Epoch 817/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2753 - acc: 0.8912 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 00817: val\_loss did not improve
Epoch 818/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2893 - acc: 0.8603 - val\_loss: 0.3368 - val\_acc: 0.8830

Epoch 00818: val\_loss did not improve
Epoch 819/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2536 - acc: 0.8882 - val\_loss: 0.3358 - val\_acc: 0.8772

Epoch 00819: val\_loss did not improve
Epoch 820/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2658 - acc: 0.9000 - val\_loss: 0.3380 - val\_acc: 0.8772

Epoch 00820: val\_loss did not improve
Epoch 821/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3087 - acc: 0.8603 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 00821: val\_loss did not improve
Epoch 822/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2700 - acc: 0.8926 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 00822: ReduceLROnPlateau reducing learning rate to 0.0003001892968313769.

Epoch 00822: val\_loss did not improve
Epoch 823/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2931 - acc: 0.8794 - val\_loss: 0.3403 - val\_acc: 0.8772

Epoch 00823: val\_loss did not improve
Epoch 824/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2866 - acc: 0.8838 - val\_loss: 0.3391 - val\_acc: 0.8772

Epoch 00824: val\_loss did not improve
Epoch 825/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2850 - acc: 0.8735 - val\_loss: 0.3389 - val\_acc: 0.8830

Epoch 00825: val\_loss did not improve
Epoch 826/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2875 - acc: 0.8662 - val\_loss: 0.3397 - val\_acc: 0.8889

Epoch 00826: val\_loss did not improve
Epoch 827/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2989 - acc: 0.8603 - val\_loss: 0.3412 - val\_acc: 0.8830

Epoch 00827: val\_loss did not improve
Epoch 828/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2831 - acc: 0.8676 - val\_loss: 0.3396 - val\_acc: 0.8889

Epoch 00828: val\_loss did not improve
Epoch 829/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2516 - acc: 0.9103 - val\_loss: 0.3368 - val\_acc: 0.8947

Epoch 00829: val\_loss did not improve
Epoch 830/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2618 - acc: 0.8853 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 00830: val\_loss did not improve
Epoch 831/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2925 - acc: 0.8676 - val\_loss: 0.3372 - val\_acc: 0.9006

Epoch 00831: val\_loss did not improve
Epoch 832/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2773 - acc: 0.8809 - val\_loss: 0.3385 - val\_acc: 0.8830

Epoch 00832: val\_loss did not improve
Epoch 833/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3000 - acc: 0.8706 - val\_loss: 0.3399 - val\_acc: 0.8947

Epoch 00833: val\_loss did not improve
Epoch 834/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2670 - acc: 0.8882 - val\_loss: 0.3431 - val\_acc: 0.8947

Epoch 00834: val\_loss did not improve
Epoch 835/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2810 - acc: 0.8794 - val\_loss: 0.3452 - val\_acc: 0.8889

Epoch 00835: val\_loss did not improve
Epoch 836/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3127 - acc: 0.8676 - val\_loss: 0.3489 - val\_acc: 0.8889

Epoch 00836: val\_loss did not improve
Epoch 837/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2733 - acc: 0.8882 - val\_loss: 0.3500 - val\_acc: 0.8889

Epoch 00837: val\_loss did not improve
Epoch 838/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3056 - acc: 0.8721 - val\_loss: 0.3519 - val\_acc: 0.8830

Epoch 00838: val\_loss did not improve
Epoch 839/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3060 - acc: 0.8544 - val\_loss: 0.3537 - val\_acc: 0.8830

Epoch 00839: val\_loss did not improve
Epoch 840/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2796 - acc: 0.8853 - val\_loss: 0.3551 - val\_acc: 0.8772

Epoch 00840: val\_loss did not improve
Epoch 841/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3073 - acc: 0.8588 - val\_loss: 0.3552 - val\_acc: 0.8772

Epoch 00841: val\_loss did not improve
Epoch 842/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2704 - acc: 0.8941 - val\_loss: 0.3535 - val\_acc: 0.8772

Epoch 00842: val\_loss did not improve
Epoch 843/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2785 - acc: 0.8735 - val\_loss: 0.3530 - val\_acc: 0.8772

Epoch 00843: val\_loss did not improve
Epoch 844/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2689 - acc: 0.8794 - val\_loss: 0.3525 - val\_acc: 0.8772

Epoch 00844: val\_loss did not improve
Epoch 845/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2556 - acc: 0.8868 - val\_loss: 0.3506 - val\_acc: 0.8830

Epoch 00845: val\_loss did not improve
Epoch 846/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2917 - acc: 0.8750 - val\_loss: 0.3484 - val\_acc: 0.8830

Epoch 00846: val\_loss did not improve
Epoch 847/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2661 - acc: 0.8853 - val\_loss: 0.3455 - val\_acc: 0.8830

Epoch 00847: val\_loss did not improve
Epoch 848/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2782 - acc: 0.8897 - val\_loss: 0.3436 - val\_acc: 0.8889

Epoch 00848: val\_loss did not improve
Epoch 849/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2391 - acc: 0.9147 - val\_loss: 0.3451 - val\_acc: 0.8889

Epoch 00849: val\_loss did not improve
Epoch 850/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2720 - acc: 0.8721 - val\_loss: 0.3452 - val\_acc: 0.8830

Epoch 00850: val\_loss did not improve
Epoch 851/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2741 - acc: 0.8750 - val\_loss: 0.3472 - val\_acc: 0.8830

Epoch 00851: val\_loss did not improve
Epoch 852/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2998 - acc: 0.8544 - val\_loss: 0.3476 - val\_acc: 0.8889

Epoch 00852: ReduceLROnPlateau reducing learning rate to 0.0002701703750062734.

Epoch 00852: val\_loss did not improve
Epoch 853/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2710 - acc: 0.8956 - val\_loss: 0.3467 - val\_acc: 0.8889

Epoch 00853: val\_loss did not improve
Epoch 854/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2532 - acc: 0.9044 - val\_loss: 0.3435 - val\_acc: 0.8830

Epoch 00854: val\_loss did not improve
Epoch 855/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3101 - acc: 0.8559 - val\_loss: 0.3419 - val\_acc: 0.8830

Epoch 00855: val\_loss did not improve
Epoch 856/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2463 - acc: 0.9015 - val\_loss: 0.3423 - val\_acc: 0.8830

Epoch 00856: val\_loss did not improve
Epoch 857/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2844 - acc: 0.8632 - val\_loss: 0.3427 - val\_acc: 0.8830

Epoch 00857: val\_loss did not improve
Epoch 858/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2727 - acc: 0.8662 - val\_loss: 0.3459 - val\_acc: 0.8830

Epoch 00858: val\_loss did not improve
Epoch 859/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2900 - acc: 0.8691 - val\_loss: 0.3471 - val\_acc: 0.8830

Epoch 00859: val\_loss did not improve
Epoch 860/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2976 - acc: 0.8676 - val\_loss: 0.3468 - val\_acc: 0.8830

Epoch 00860: val\_loss did not improve
Epoch 861/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2985 - acc: 0.8632 - val\_loss: 0.3480 - val\_acc: 0.8772

Epoch 00861: val\_loss did not improve
Epoch 862/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2817 - acc: 0.8897 - val\_loss: 0.3491 - val\_acc: 0.8772

Epoch 00862: val\_loss did not improve
Epoch 863/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2841 - acc: 0.8794 - val\_loss: 0.3504 - val\_acc: 0.8772

Epoch 00863: val\_loss did not improve
Epoch 864/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2607 - acc: 0.8956 - val\_loss: 0.3506 - val\_acc: 0.8772

Epoch 00864: val\_loss did not improve
Epoch 865/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2956 - acc: 0.8765 - val\_loss: 0.3492 - val\_acc: 0.8713

Epoch 00865: val\_loss did not improve
Epoch 866/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2585 - acc: 0.8853 - val\_loss: 0.3502 - val\_acc: 0.8713

Epoch 00866: val\_loss did not improve
Epoch 867/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2889 - acc: 0.8779 - val\_loss: 0.3492 - val\_acc: 0.8830

Epoch 00867: val\_loss did not improve
Epoch 868/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2683 - acc: 0.8868 - val\_loss: 0.3471 - val\_acc: 0.8830

Epoch 00868: val\_loss did not improve
Epoch 869/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2855 - acc: 0.8809 - val\_loss: 0.3460 - val\_acc: 0.8830

Epoch 00869: val\_loss did not improve
Epoch 870/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2933 - acc: 0.8735 - val\_loss: 0.3449 - val\_acc: 0.8830

Epoch 00870: val\_loss did not improve
Epoch 871/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2722 - acc: 0.8912 - val\_loss: 0.3452 - val\_acc: 0.8830

Epoch 00871: val\_loss did not improve
Epoch 872/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2665 - acc: 0.8882 - val\_loss: 0.3460 - val\_acc: 0.8889

Epoch 00872: val\_loss did not improve
Epoch 873/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2682 - acc: 0.8853 - val\_loss: 0.3459 - val\_acc: 0.8947

Epoch 00873: val\_loss did not improve
Epoch 874/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2860 - acc: 0.8853 - val\_loss: 0.3470 - val\_acc: 0.8889

Epoch 00874: val\_loss did not improve
Epoch 875/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2702 - acc: 0.8897 - val\_loss: 0.3471 - val\_acc: 0.8889

Epoch 00875: val\_loss did not improve
Epoch 876/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2521 - acc: 0.9044 - val\_loss: 0.3462 - val\_acc: 0.8889

Epoch 00876: val\_loss did not improve
Epoch 877/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3177 - acc: 0.8647 - val\_loss: 0.3460 - val\_acc: 0.8947

Epoch 00877: val\_loss did not improve
Epoch 878/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2652 - acc: 0.8882 - val\_loss: 0.3462 - val\_acc: 0.8947

Epoch 00878: val\_loss did not improve
Epoch 879/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2831 - acc: 0.8882 - val\_loss: 0.3471 - val\_acc: 0.8947

Epoch 00879: val\_loss did not improve
Epoch 880/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2800 - acc: 0.8735 - val\_loss: 0.3492 - val\_acc: 0.8889

Epoch 00880: val\_loss did not improve
Epoch 881/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2683 - acc: 0.9000 - val\_loss: 0.3492 - val\_acc: 0.8830

Epoch 00881: val\_loss did not improve
Epoch 882/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2942 - acc: 0.8779 - val\_loss: 0.3496 - val\_acc: 0.8830

Epoch 00882: ReduceLROnPlateau reducing learning rate to 0.0002431533270282671.

Epoch 00882: val\_loss did not improve
Epoch 883/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2975 - acc: 0.8647 - val\_loss: 0.3505 - val\_acc: 0.8772

Epoch 00883: val\_loss did not improve
Epoch 884/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2795 - acc: 0.8824 - val\_loss: 0.3502 - val\_acc: 0.8830

Epoch 00884: val\_loss did not improve
Epoch 885/1500
680/680 [==============================] - 0s 64us/step - loss: 0.3102 - acc: 0.8647 - val\_loss: 0.3502 - val\_acc: 0.8830

Epoch 00885: val\_loss did not improve
Epoch 886/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2873 - acc: 0.8706 - val\_loss: 0.3493 - val\_acc: 0.8889

Epoch 00886: val\_loss did not improve
Epoch 887/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2759 - acc: 0.8941 - val\_loss: 0.3485 - val\_acc: 0.8889

Epoch 00887: val\_loss did not improve
Epoch 888/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2765 - acc: 0.8779 - val\_loss: 0.3478 - val\_acc: 0.8889

Epoch 00888: val\_loss did not improve
Epoch 889/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2825 - acc: 0.8735 - val\_loss: 0.3465 - val\_acc: 0.8889

Epoch 00889: val\_loss did not improve
Epoch 890/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2931 - acc: 0.8838 - val\_loss: 0.3456 - val\_acc: 0.8947

Epoch 00890: val\_loss did not improve
Epoch 891/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2720 - acc: 0.8838 - val\_loss: 0.3444 - val\_acc: 0.8947

Epoch 00891: val\_loss did not improve
Epoch 892/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2839 - acc: 0.8853 - val\_loss: 0.3465 - val\_acc: 0.8947

Epoch 00892: val\_loss did not improve
Epoch 893/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2728 - acc: 0.8735 - val\_loss: 0.3481 - val\_acc: 0.8947

Epoch 00893: val\_loss did not improve
Epoch 894/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2606 - acc: 0.8926 - val\_loss: 0.3498 - val\_acc: 0.8947

Epoch 00894: val\_loss did not improve
Epoch 895/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2700 - acc: 0.8824 - val\_loss: 0.3500 - val\_acc: 0.8947

Epoch 00895: val\_loss did not improve
Epoch 896/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2644 - acc: 0.8794 - val\_loss: 0.3485 - val\_acc: 0.8947

Epoch 00896: val\_loss did not improve
Epoch 897/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2808 - acc: 0.8779 - val\_loss: 0.3471 - val\_acc: 0.8947

Epoch 00897: val\_loss did not improve
Epoch 898/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2602 - acc: 0.9029 - val\_loss: 0.3470 - val\_acc: 0.8889

Epoch 00898: val\_loss did not improve
Epoch 899/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2775 - acc: 0.8824 - val\_loss: 0.3463 - val\_acc: 0.8889

Epoch 00899: val\_loss did not improve
Epoch 900/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2829 - acc: 0.8809 - val\_loss: 0.3469 - val\_acc: 0.8830

Epoch 00900: val\_loss did not improve
Epoch 901/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3033 - acc: 0.8632 - val\_loss: 0.3477 - val\_acc: 0.8772

Epoch 00901: val\_loss did not improve
Epoch 902/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2717 - acc: 0.8882 - val\_loss: 0.3482 - val\_acc: 0.8830

Epoch 00902: val\_loss did not improve
Epoch 903/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2759 - acc: 0.8912 - val\_loss: 0.3485 - val\_acc: 0.8830

Epoch 00903: val\_loss did not improve
Epoch 904/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2841 - acc: 0.8838 - val\_loss: 0.3480 - val\_acc: 0.8830

Epoch 00904: val\_loss did not improve
Epoch 905/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2881 - acc: 0.8750 - val\_loss: 0.3463 - val\_acc: 0.8772

Epoch 00905: val\_loss did not improve
Epoch 906/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2719 - acc: 0.8912 - val\_loss: 0.3451 - val\_acc: 0.8772

Epoch 00906: val\_loss did not improve
Epoch 907/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2791 - acc: 0.8779 - val\_loss: 0.3453 - val\_acc: 0.8772

Epoch 00907: val\_loss did not improve
Epoch 908/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2606 - acc: 0.9029 - val\_loss: 0.3462 - val\_acc: 0.8772

Epoch 00908: val\_loss did not improve
Epoch 909/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2589 - acc: 0.8897 - val\_loss: 0.3477 - val\_acc: 0.8772

Epoch 00909: val\_loss did not improve
Epoch 910/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2707 - acc: 0.8838 - val\_loss: 0.3493 - val\_acc: 0.8772

Epoch 00910: val\_loss did not improve
Epoch 911/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2745 - acc: 0.8971 - val\_loss: 0.3510 - val\_acc: 0.8772

Epoch 00911: val\_loss did not improve
Epoch 912/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2765 - acc: 0.8912 - val\_loss: 0.3512 - val\_acc: 0.8772

Epoch 00912: ReduceLROnPlateau reducing learning rate to 0.0002188379890867509.

Epoch 00912: val\_loss did not improve
Epoch 913/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2885 - acc: 0.8853 - val\_loss: 0.3514 - val\_acc: 0.8830

Epoch 00913: val\_loss did not improve
Epoch 914/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3325 - acc: 0.8426 - val\_loss: 0.3523 - val\_acc: 0.8830

Epoch 00914: val\_loss did not improve
Epoch 915/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2597 - acc: 0.8956 - val\_loss: 0.3525 - val\_acc: 0.8830

Epoch 00915: val\_loss did not improve
Epoch 916/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2848 - acc: 0.8809 - val\_loss: 0.3511 - val\_acc: 0.8713

Epoch 00916: val\_loss did not improve
Epoch 917/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2712 - acc: 0.8897 - val\_loss: 0.3510 - val\_acc: 0.8772

Epoch 00917: val\_loss did not improve
Epoch 918/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2739 - acc: 0.8794 - val\_loss: 0.3516 - val\_acc: 0.8772

Epoch 00918: val\_loss did not improve
Epoch 919/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2859 - acc: 0.8794 - val\_loss: 0.3511 - val\_acc: 0.8713

Epoch 00919: val\_loss did not improve
Epoch 920/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2641 - acc: 0.8912 - val\_loss: 0.3504 - val\_acc: 0.8772

Epoch 00920: val\_loss did not improve
Epoch 921/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2595 - acc: 0.8985 - val\_loss: 0.3506 - val\_acc: 0.8713

Epoch 00921: val\_loss did not improve
Epoch 922/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3085 - acc: 0.8691 - val\_loss: 0.3494 - val\_acc: 0.8713

Epoch 00922: val\_loss did not improve
Epoch 923/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2698 - acc: 0.8838 - val\_loss: 0.3468 - val\_acc: 0.8830

Epoch 00923: val\_loss did not improve
Epoch 924/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2628 - acc: 0.8926 - val\_loss: 0.3469 - val\_acc: 0.8772

Epoch 00924: val\_loss did not improve
Epoch 925/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2748 - acc: 0.8706 - val\_loss: 0.3483 - val\_acc: 0.8713

Epoch 00925: val\_loss did not improve
Epoch 926/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2863 - acc: 0.8721 - val\_loss: 0.3493 - val\_acc: 0.8772

Epoch 00926: val\_loss did not improve
Epoch 927/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2693 - acc: 0.8971 - val\_loss: 0.3500 - val\_acc: 0.8713

Epoch 00927: val\_loss did not improve
Epoch 928/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2924 - acc: 0.8735 - val\_loss: 0.3488 - val\_acc: 0.8713

Epoch 00928: val\_loss did not improve
Epoch 929/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2893 - acc: 0.8765 - val\_loss: 0.3474 - val\_acc: 0.8713

Epoch 00929: val\_loss did not improve
Epoch 930/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2603 - acc: 0.8926 - val\_loss: 0.3464 - val\_acc: 0.8713

Epoch 00930: val\_loss did not improve
Epoch 931/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2633 - acc: 0.8838 - val\_loss: 0.3469 - val\_acc: 0.8772

Epoch 00931: val\_loss did not improve
Epoch 932/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2560 - acc: 0.8956 - val\_loss: 0.3456 - val\_acc: 0.8772

Epoch 00932: val\_loss did not improve
Epoch 933/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2520 - acc: 0.8985 - val\_loss: 0.3456 - val\_acc: 0.8830

Epoch 00933: val\_loss did not improve
Epoch 934/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2444 - acc: 0.8985 - val\_loss: 0.3460 - val\_acc: 0.8830

Epoch 00934: val\_loss did not improve
Epoch 935/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2714 - acc: 0.8824 - val\_loss: 0.3447 - val\_acc: 0.8830

Epoch 00935: val\_loss did not improve
Epoch 936/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2710 - acc: 0.8941 - val\_loss: 0.3446 - val\_acc: 0.8889

Epoch 00936: val\_loss did not improve
Epoch 937/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2770 - acc: 0.8676 - val\_loss: 0.3425 - val\_acc: 0.8947

Epoch 00937: val\_loss did not improve
Epoch 938/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2876 - acc: 0.8779 - val\_loss: 0.3408 - val\_acc: 0.9006

Epoch 00938: val\_loss did not improve
Epoch 939/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2647 - acc: 0.8853 - val\_loss: 0.3410 - val\_acc: 0.9006

Epoch 00939: val\_loss did not improve
Epoch 940/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2674 - acc: 0.8941 - val\_loss: 0.3425 - val\_acc: 0.8889

Epoch 00940: val\_loss did not improve
Epoch 941/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2501 - acc: 0.8971 - val\_loss: 0.3423 - val\_acc: 0.8889

Epoch 00941: val\_loss did not improve
Epoch 942/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2813 - acc: 0.8809 - val\_loss: 0.3417 - val\_acc: 0.8947

Epoch 00942: ReduceLROnPlateau reducing learning rate to 0.00019695418886840345.

Epoch 00942: val\_loss did not improve
Epoch 943/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2699 - acc: 0.8853 - val\_loss: 0.3427 - val\_acc: 0.8947

Epoch 00943: val\_loss did not improve
Epoch 944/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2736 - acc: 0.8750 - val\_loss: 0.3429 - val\_acc: 0.8947

Epoch 00944: val\_loss did not improve
Epoch 945/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2703 - acc: 0.8853 - val\_loss: 0.3429 - val\_acc: 0.8889

Epoch 00945: val\_loss did not improve
Epoch 946/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2811 - acc: 0.8794 - val\_loss: 0.3427 - val\_acc: 0.8889

Epoch 00946: val\_loss did not improve
Epoch 947/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2649 - acc: 0.8868 - val\_loss: 0.3425 - val\_acc: 0.8889

Epoch 00947: val\_loss did not improve
Epoch 948/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2686 - acc: 0.8794 - val\_loss: 0.3419 - val\_acc: 0.8889

Epoch 00948: val\_loss did not improve
Epoch 949/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2684 - acc: 0.8868 - val\_loss: 0.3431 - val\_acc: 0.8889

Epoch 00949: val\_loss did not improve
Epoch 950/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2679 - acc: 0.8897 - val\_loss: 0.3434 - val\_acc: 0.8889

Epoch 00950: val\_loss did not improve
Epoch 951/1500
680/680 [==============================] - 0s 103us/step - loss: 0.2696 - acc: 0.8956 - val\_loss: 0.3437 - val\_acc: 0.8947

Epoch 00951: val\_loss did not improve
Epoch 952/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2785 - acc: 0.8882 - val\_loss: 0.3439 - val\_acc: 0.8830

Epoch 00952: val\_loss did not improve
Epoch 953/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2835 - acc: 0.8897 - val\_loss: 0.3451 - val\_acc: 0.8830

Epoch 00953: val\_loss did not improve
Epoch 954/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2943 - acc: 0.8676 - val\_loss: 0.3456 - val\_acc: 0.8889

Epoch 00954: val\_loss did not improve
Epoch 955/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2767 - acc: 0.8706 - val\_loss: 0.3451 - val\_acc: 0.8889

Epoch 00955: val\_loss did not improve
Epoch 956/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2544 - acc: 0.8956 - val\_loss: 0.3464 - val\_acc: 0.8889

Epoch 00956: val\_loss did not improve
Epoch 957/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2831 - acc: 0.8765 - val\_loss: 0.3459 - val\_acc: 0.8889

Epoch 00957: val\_loss did not improve
Epoch 958/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2748 - acc: 0.8926 - val\_loss: 0.3459 - val\_acc: 0.8889

Epoch 00958: val\_loss did not improve
Epoch 959/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2777 - acc: 0.8868 - val\_loss: 0.3461 - val\_acc: 0.8772

Epoch 00959: val\_loss did not improve
Epoch 960/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2656 - acc: 0.8882 - val\_loss: 0.3478 - val\_acc: 0.8772

Epoch 00960: val\_loss did not improve
Epoch 961/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2967 - acc: 0.8721 - val\_loss: 0.3491 - val\_acc: 0.8772

Epoch 00961: val\_loss did not improve
Epoch 962/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2793 - acc: 0.8882 - val\_loss: 0.3510 - val\_acc: 0.8713

Epoch 00962: val\_loss did not improve
Epoch 963/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2886 - acc: 0.8750 - val\_loss: 0.3545 - val\_acc: 0.8713

Epoch 00963: val\_loss did not improve
Epoch 964/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2763 - acc: 0.8853 - val\_loss: 0.3542 - val\_acc: 0.8713

Epoch 00964: val\_loss did not improve
Epoch 965/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2952 - acc: 0.8691 - val\_loss: 0.3526 - val\_acc: 0.8772

Epoch 00965: val\_loss did not improve
Epoch 966/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2822 - acc: 0.8824 - val\_loss: 0.3515 - val\_acc: 0.8830

Epoch 00966: val\_loss did not improve
Epoch 967/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2961 - acc: 0.8765 - val\_loss: 0.3511 - val\_acc: 0.8772

Epoch 00967: val\_loss did not improve
Epoch 968/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2774 - acc: 0.8868 - val\_loss: 0.3496 - val\_acc: 0.8830

Epoch 00968: val\_loss did not improve
Epoch 969/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3041 - acc: 0.8721 - val\_loss: 0.3487 - val\_acc: 0.8772

Epoch 00969: val\_loss did not improve
Epoch 970/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3162 - acc: 0.8588 - val\_loss: 0.3473 - val\_acc: 0.8830

Epoch 00970: val\_loss did not improve
Epoch 971/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2536 - acc: 0.8941 - val\_loss: 0.3470 - val\_acc: 0.8772

Epoch 00971: val\_loss did not improve
Epoch 972/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2607 - acc: 0.9029 - val\_loss: 0.3461 - val\_acc: 0.8772

Epoch 00972: ReduceLROnPlateau reducing learning rate to 0.00017725877260090783.

Epoch 00972: val\_loss did not improve
Epoch 973/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2905 - acc: 0.8809 - val\_loss: 0.3451 - val\_acc: 0.8772

Epoch 00973: val\_loss did not improve
Epoch 974/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2802 - acc: 0.8824 - val\_loss: 0.3448 - val\_acc: 0.8830

Epoch 00974: val\_loss did not improve
Epoch 975/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2812 - acc: 0.8691 - val\_loss: 0.3449 - val\_acc: 0.8830

Epoch 00975: val\_loss did not improve
Epoch 976/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2623 - acc: 0.8941 - val\_loss: 0.3441 - val\_acc: 0.8889

Epoch 00976: val\_loss did not improve
Epoch 977/1500
680/680 [==============================] - 0s 75us/step - loss: 0.2768 - acc: 0.8853 - val\_loss: 0.3427 - val\_acc: 0.8889

Epoch 00977: val\_loss did not improve
Epoch 978/1500
680/680 [==============================] - 0s 74us/step - loss: 0.2565 - acc: 0.8882 - val\_loss: 0.3416 - val\_acc: 0.8830

Epoch 00978: val\_loss did not improve
Epoch 979/1500
680/680 [==============================] - 0s 75us/step - loss: 0.2525 - acc: 0.8912 - val\_loss: 0.3405 - val\_acc: 0.8889

Epoch 00979: val\_loss did not improve
Epoch 980/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2597 - acc: 0.8985 - val\_loss: 0.3403 - val\_acc: 0.8889

Epoch 00980: val\_loss did not improve
Epoch 981/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2815 - acc: 0.8882 - val\_loss: 0.3407 - val\_acc: 0.8947

Epoch 00981: val\_loss did not improve
Epoch 982/1500
680/680 [==============================] - 0s 79us/step - loss: 0.2559 - acc: 0.8897 - val\_loss: 0.3398 - val\_acc: 0.8889

Epoch 00982: val\_loss did not improve
Epoch 983/1500
680/680 [==============================] - 0s 79us/step - loss: 0.2802 - acc: 0.8721 - val\_loss: 0.3391 - val\_acc: 0.8889

Epoch 00983: val\_loss did not improve
Epoch 984/1500
680/680 [==============================] - 0s 87us/step - loss: 0.2906 - acc: 0.8824 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 00984: val\_loss did not improve
Epoch 985/1500
680/680 [==============================] - 0s 77us/step - loss: 0.2906 - acc: 0.8809 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 00985: val\_loss did not improve
Epoch 986/1500
680/680 [==============================] - 0s 69us/step - loss: 0.2646 - acc: 0.8809 - val\_loss: 0.3375 - val\_acc: 0.8889

Epoch 00986: val\_loss did not improve
Epoch 987/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2865 - acc: 0.8838 - val\_loss: 0.3370 - val\_acc: 0.8889

Epoch 00987: val\_loss did not improve
Epoch 988/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2888 - acc: 0.8691 - val\_loss: 0.3383 - val\_acc: 0.8830

Epoch 00988: val\_loss did not improve
Epoch 989/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2839 - acc: 0.8853 - val\_loss: 0.3381 - val\_acc: 0.8889

Epoch 00989: val\_loss did not improve
Epoch 990/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2643 - acc: 0.8794 - val\_loss: 0.3375 - val\_acc: 0.8830

Epoch 00990: val\_loss did not improve
Epoch 991/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2512 - acc: 0.9015 - val\_loss: 0.3384 - val\_acc: 0.8830

Epoch 00991: val\_loss did not improve
Epoch 992/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2674 - acc: 0.8912 - val\_loss: 0.3385 - val\_acc: 0.8889

Epoch 00992: val\_loss did not improve
Epoch 993/1500
680/680 [==============================] - 0s 59us/step - loss: 0.3023 - acc: 0.8618 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 00993: val\_loss did not improve
Epoch 994/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2360 - acc: 0.9074 - val\_loss: 0.3399 - val\_acc: 0.8889

Epoch 00994: val\_loss did not improve
Epoch 995/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2658 - acc: 0.8912 - val\_loss: 0.3400 - val\_acc: 0.8830

Epoch 00995: val\_loss did not improve
Epoch 996/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2692 - acc: 0.8853 - val\_loss: 0.3394 - val\_acc: 0.8830

Epoch 00996: val\_loss did not improve
Epoch 997/1500
680/680 [==============================] - 0s 71us/step - loss: 0.2770 - acc: 0.8912 - val\_loss: 0.3390 - val\_acc: 0.8889

Epoch 00997: val\_loss did not improve
Epoch 998/1500
680/680 [==============================] - 0s 76us/step - loss: 0.2521 - acc: 0.8985 - val\_loss: 0.3384 - val\_acc: 0.8889

Epoch 00998: val\_loss did not improve
Epoch 999/1500
680/680 [==============================] - 0s 79us/step - loss: 0.2362 - acc: 0.9000 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 00999: val\_loss did not improve
Epoch 1000/1500
680/680 [==============================] - 0s 83us/step - loss: 0.2941 - acc: 0.8691 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01000: val\_loss did not improve
Epoch 1001/1500
680/680 [==============================] - 0s 87us/step - loss: 0.2601 - acc: 0.8956 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 01001: val\_loss did not improve
Epoch 1002/1500
680/680 [==============================] - 0s 77us/step - loss: 0.2820 - acc: 0.8853 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01002: ReduceLROnPlateau reducing learning rate to 0.00015953289403114468.

Epoch 01002: val\_loss did not improve
Epoch 1003/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2827 - acc: 0.8721 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 01003: val\_loss did not improve
Epoch 1004/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2505 - acc: 0.8941 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 01004: val\_loss did not improve
Epoch 1005/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2671 - acc: 0.8750 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01005: val\_loss did not improve
Epoch 1006/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2667 - acc: 0.8794 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01006: val\_loss did not improve
Epoch 1007/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2670 - acc: 0.8809 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01007: val\_loss did not improve
Epoch 1008/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2488 - acc: 0.8897 - val\_loss: 0.3388 - val\_acc: 0.8889

Epoch 01008: val\_loss did not improve
Epoch 1009/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2555 - acc: 0.9059 - val\_loss: 0.3389 - val\_acc: 0.8889

Epoch 01009: val\_loss did not improve
Epoch 1010/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2840 - acc: 0.8926 - val\_loss: 0.3390 - val\_acc: 0.8889

Epoch 01010: val\_loss did not improve
Epoch 1011/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2776 - acc: 0.8838 - val\_loss: 0.3389 - val\_acc: 0.8889

Epoch 01011: val\_loss did not improve
Epoch 1012/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2618 - acc: 0.8926 - val\_loss: 0.3395 - val\_acc: 0.8889

Epoch 01012: val\_loss did not improve
Epoch 1013/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2621 - acc: 0.8809 - val\_loss: 0.3403 - val\_acc: 0.8889

Epoch 01013: val\_loss did not improve
Epoch 1014/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2852 - acc: 0.8750 - val\_loss: 0.3403 - val\_acc: 0.8889

Epoch 01014: val\_loss did not improve
Epoch 1015/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2762 - acc: 0.8647 - val\_loss: 0.3395 - val\_acc: 0.8947

Epoch 01015: val\_loss did not improve
Epoch 1016/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2705 - acc: 0.8735 - val\_loss: 0.3386 - val\_acc: 0.9006

Epoch 01016: val\_loss did not improve
Epoch 1017/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2766 - acc: 0.8838 - val\_loss: 0.3383 - val\_acc: 0.8947

Epoch 01017: val\_loss did not improve
Epoch 1018/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2692 - acc: 0.8779 - val\_loss: 0.3385 - val\_acc: 0.8947

Epoch 01018: val\_loss did not improve
Epoch 1019/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2693 - acc: 0.8765 - val\_loss: 0.3386 - val\_acc: 0.8947

Epoch 01019: val\_loss did not improve
Epoch 1020/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2380 - acc: 0.9103 - val\_loss: 0.3402 - val\_acc: 0.8947

Epoch 01020: val\_loss did not improve
Epoch 1021/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2581 - acc: 0.8794 - val\_loss: 0.3418 - val\_acc: 0.8889

Epoch 01021: val\_loss did not improve
Epoch 1022/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2611 - acc: 0.8897 - val\_loss: 0.3422 - val\_acc: 0.8889

Epoch 01022: val\_loss did not improve
Epoch 1023/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2621 - acc: 0.8809 - val\_loss: 0.3426 - val\_acc: 0.8947

Epoch 01023: val\_loss did not improve
Epoch 1024/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2796 - acc: 0.8765 - val\_loss: 0.3428 - val\_acc: 0.9006

Epoch 01024: val\_loss did not improve
Epoch 1025/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2948 - acc: 0.8676 - val\_loss: 0.3432 - val\_acc: 0.8947

Epoch 01025: val\_loss did not improve
Epoch 1026/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2856 - acc: 0.8603 - val\_loss: 0.3437 - val\_acc: 0.8889

Epoch 01026: val\_loss did not improve
Epoch 1027/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2894 - acc: 0.8618 - val\_loss: 0.3456 - val\_acc: 0.8889

Epoch 01027: val\_loss did not improve
Epoch 1028/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2546 - acc: 0.8956 - val\_loss: 0.3475 - val\_acc: 0.8889

Epoch 01028: val\_loss did not improve
Epoch 1029/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2452 - acc: 0.9029 - val\_loss: 0.3479 - val\_acc: 0.8889

Epoch 01029: val\_loss did not improve
Epoch 1030/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2729 - acc: 0.8882 - val\_loss: 0.3478 - val\_acc: 0.8889

Epoch 01030: val\_loss did not improve
Epoch 1031/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2555 - acc: 0.8956 - val\_loss: 0.3465 - val\_acc: 0.8889

Epoch 01031: val\_loss did not improve
Epoch 1032/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2651 - acc: 0.8971 - val\_loss: 0.3452 - val\_acc: 0.8889

Epoch 01032: ReduceLROnPlateau reducing learning rate to 0.00014357960462803022.

Epoch 01032: val\_loss did not improve
Epoch 1033/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2769 - acc: 0.8691 - val\_loss: 0.3441 - val\_acc: 0.8889

Epoch 01033: val\_loss did not improve
Epoch 1034/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2866 - acc: 0.8721 - val\_loss: 0.3436 - val\_acc: 0.8889

Epoch 01034: val\_loss did not improve
Epoch 1035/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2669 - acc: 0.8721 - val\_loss: 0.3426 - val\_acc: 0.8889

Epoch 01035: val\_loss did not improve
Epoch 1036/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2743 - acc: 0.8824 - val\_loss: 0.3416 - val\_acc: 0.8830

Epoch 01036: val\_loss did not improve
Epoch 1037/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2588 - acc: 0.8971 - val\_loss: 0.3414 - val\_acc: 0.8830

Epoch 01037: val\_loss did not improve
Epoch 1038/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2665 - acc: 0.8824 - val\_loss: 0.3415 - val\_acc: 0.8830

Epoch 01038: val\_loss did not improve
Epoch 1039/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2677 - acc: 0.8926 - val\_loss: 0.3409 - val\_acc: 0.8889

Epoch 01039: val\_loss did not improve
Epoch 1040/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2579 - acc: 0.8926 - val\_loss: 0.3414 - val\_acc: 0.8889

Epoch 01040: val\_loss did not improve
Epoch 1041/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2768 - acc: 0.8765 - val\_loss: 0.3420 - val\_acc: 0.8889

Epoch 01041: val\_loss did not improve
Epoch 1042/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2801 - acc: 0.8735 - val\_loss: 0.3430 - val\_acc: 0.8889

Epoch 01042: val\_loss did not improve
Epoch 1043/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2808 - acc: 0.8691 - val\_loss: 0.3432 - val\_acc: 0.8889

Epoch 01043: val\_loss did not improve
Epoch 1044/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2784 - acc: 0.8809 - val\_loss: 0.3432 - val\_acc: 0.8889

Epoch 01044: val\_loss did not improve
Epoch 1045/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2786 - acc: 0.8765 - val\_loss: 0.3432 - val\_acc: 0.8889

Epoch 01045: val\_loss did not improve
Epoch 1046/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2707 - acc: 0.8824 - val\_loss: 0.3426 - val\_acc: 0.8889

Epoch 01046: val\_loss did not improve
Epoch 1047/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2608 - acc: 0.8868 - val\_loss: 0.3429 - val\_acc: 0.8889

Epoch 01047: val\_loss did not improve
Epoch 1048/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2702 - acc: 0.8735 - val\_loss: 0.3439 - val\_acc: 0.8889

Epoch 01048: val\_loss did not improve
Epoch 1049/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2711 - acc: 0.8912 - val\_loss: 0.3441 - val\_acc: 0.8889

Epoch 01049: val\_loss did not improve
Epoch 1050/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2874 - acc: 0.8662 - val\_loss: 0.3450 - val\_acc: 0.8889

Epoch 01050: val\_loss did not improve
Epoch 1051/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2467 - acc: 0.8897 - val\_loss: 0.3442 - val\_acc: 0.8889

Epoch 01051: val\_loss did not improve
Epoch 1052/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2768 - acc: 0.8853 - val\_loss: 0.3437 - val\_acc: 0.8889

Epoch 01052: val\_loss did not improve
Epoch 1053/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2627 - acc: 0.8941 - val\_loss: 0.3442 - val\_acc: 0.8889

Epoch 01053: val\_loss did not improve
Epoch 1054/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2592 - acc: 0.8897 - val\_loss: 0.3449 - val\_acc: 0.8889

Epoch 01054: val\_loss did not improve
Epoch 1055/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2630 - acc: 0.8691 - val\_loss: 0.3449 - val\_acc: 0.8889

Epoch 01055: val\_loss did not improve
Epoch 1056/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2920 - acc: 0.8779 - val\_loss: 0.3454 - val\_acc: 0.8889

Epoch 01056: val\_loss did not improve
Epoch 1057/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2731 - acc: 0.8897 - val\_loss: 0.3463 - val\_acc: 0.8830

Epoch 01057: val\_loss did not improve
Epoch 1058/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2603 - acc: 0.8897 - val\_loss: 0.3468 - val\_acc: 0.8830

Epoch 01058: val\_loss did not improve
Epoch 1059/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2878 - acc: 0.8706 - val\_loss: 0.3457 - val\_acc: 0.8830

Epoch 01059: val\_loss did not improve
Epoch 1060/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2704 - acc: 0.8750 - val\_loss: 0.3446 - val\_acc: 0.8830

Epoch 01060: val\_loss did not improve
Epoch 1061/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2572 - acc: 0.8882 - val\_loss: 0.3436 - val\_acc: 0.8830

Epoch 01061: val\_loss did not improve
Epoch 1062/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2648 - acc: 0.8824 - val\_loss: 0.3432 - val\_acc: 0.8830

Epoch 01062: ReduceLROnPlateau reducing learning rate to 0.00012922164023621008.

Epoch 01062: val\_loss did not improve
Epoch 1063/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2447 - acc: 0.9118 - val\_loss: 0.3441 - val\_acc: 0.8830

Epoch 01063: val\_loss did not improve
Epoch 1064/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2747 - acc: 0.8853 - val\_loss: 0.3452 - val\_acc: 0.8830

Epoch 01064: val\_loss did not improve
Epoch 1065/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2573 - acc: 0.8956 - val\_loss: 0.3460 - val\_acc: 0.8830

Epoch 01065: val\_loss did not improve
Epoch 1066/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2535 - acc: 0.8882 - val\_loss: 0.3461 - val\_acc: 0.8830

Epoch 01066: val\_loss did not improve
Epoch 1067/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2570 - acc: 0.8838 - val\_loss: 0.3453 - val\_acc: 0.8830

Epoch 01067: val\_loss did not improve
Epoch 1068/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2705 - acc: 0.8971 - val\_loss: 0.3460 - val\_acc: 0.8889

Epoch 01068: val\_loss did not improve
Epoch 1069/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2730 - acc: 0.8824 - val\_loss: 0.3470 - val\_acc: 0.8889

Epoch 01069: val\_loss did not improve
Epoch 1070/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2827 - acc: 0.8926 - val\_loss: 0.3477 - val\_acc: 0.8889

Epoch 01070: val\_loss did not improve
Epoch 1071/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2569 - acc: 0.8926 - val\_loss: 0.3482 - val\_acc: 0.8889

Epoch 01071: val\_loss did not improve
Epoch 1072/1500
680/680 [==============================] - 0s 63us/step - loss: 0.3008 - acc: 0.8618 - val\_loss: 0.3484 - val\_acc: 0.8889

Epoch 01072: val\_loss did not improve
Epoch 1073/1500
680/680 [==============================] - 0s 150us/step - loss: 0.3032 - acc: 0.8574 - val\_loss: 0.3478 - val\_acc: 0.8830

Epoch 01073: val\_loss did not improve
Epoch 1074/1500
680/680 [==============================] - 0s 79us/step - loss: 0.2708 - acc: 0.8765 - val\_loss: 0.3468 - val\_acc: 0.8830

Epoch 01074: val\_loss did not improve
Epoch 1075/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2802 - acc: 0.8765 - val\_loss: 0.3463 - val\_acc: 0.8830

Epoch 01075: val\_loss did not improve
Epoch 1076/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2717 - acc: 0.8750 - val\_loss: 0.3458 - val\_acc: 0.8830

Epoch 01076: val\_loss did not improve
Epoch 1077/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2895 - acc: 0.8735 - val\_loss: 0.3453 - val\_acc: 0.8830

Epoch 01077: val\_loss did not improve
Epoch 1078/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2738 - acc: 0.8838 - val\_loss: 0.3448 - val\_acc: 0.8889

Epoch 01078: val\_loss did not improve
Epoch 1079/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2837 - acc: 0.8809 - val\_loss: 0.3443 - val\_acc: 0.8889

Epoch 01079: val\_loss did not improve
Epoch 1080/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2507 - acc: 0.8971 - val\_loss: 0.3442 - val\_acc: 0.8889

Epoch 01080: val\_loss did not improve
Epoch 1081/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2537 - acc: 0.8838 - val\_loss: 0.3449 - val\_acc: 0.8889

Epoch 01081: val\_loss did not improve
Epoch 1082/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2700 - acc: 0.8779 - val\_loss: 0.3446 - val\_acc: 0.8889

Epoch 01082: val\_loss did not improve
Epoch 1083/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2800 - acc: 0.8853 - val\_loss: 0.3451 - val\_acc: 0.8889

Epoch 01083: val\_loss did not improve
Epoch 1084/1500
680/680 [==============================] - 0s 55us/step - loss: 0.2856 - acc: 0.8735 - val\_loss: 0.3455 - val\_acc: 0.8889

Epoch 01084: val\_loss did not improve
Epoch 1085/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2585 - acc: 0.8971 - val\_loss: 0.3450 - val\_acc: 0.8889

Epoch 01085: val\_loss did not improve
Epoch 1086/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2897 - acc: 0.8691 - val\_loss: 0.3452 - val\_acc: 0.8889

Epoch 01086: val\_loss did not improve
Epoch 1087/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.8691 - val\_loss: 0.3462 - val\_acc: 0.8889

Epoch 01087: val\_loss did not improve
Epoch 1088/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3013 - acc: 0.8662 - val\_loss: 0.3462 - val\_acc: 0.8889

Epoch 01088: val\_loss did not improve
Epoch 1089/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2588 - acc: 0.8971 - val\_loss: 0.3462 - val\_acc: 0.8889

Epoch 01089: val\_loss did not improve
Epoch 1090/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2972 - acc: 0.8765 - val\_loss: 0.3461 - val\_acc: 0.8889

Epoch 01090: val\_loss did not improve
Epoch 1091/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2482 - acc: 0.8926 - val\_loss: 0.3454 - val\_acc: 0.8889

Epoch 01091: val\_loss did not improve
Epoch 1092/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2850 - acc: 0.8941 - val\_loss: 0.3446 - val\_acc: 0.8889

Epoch 01092: ReduceLROnPlateau reducing learning rate to 0.00011629948276095093.

Epoch 01092: val\_loss did not improve
Epoch 1093/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2491 - acc: 0.8985 - val\_loss: 0.3440 - val\_acc: 0.8889

Epoch 01093: val\_loss did not improve
Epoch 1094/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2507 - acc: 0.8868 - val\_loss: 0.3441 - val\_acc: 0.8947

Epoch 01094: val\_loss did not improve
Epoch 1095/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2613 - acc: 0.8956 - val\_loss: 0.3445 - val\_acc: 0.8947

Epoch 01095: val\_loss did not improve
Epoch 1096/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2680 - acc: 0.8750 - val\_loss: 0.3443 - val\_acc: 0.8889

Epoch 01096: val\_loss did not improve
Epoch 1097/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2537 - acc: 0.8926 - val\_loss: 0.3436 - val\_acc: 0.8947

Epoch 01097: val\_loss did not improve
Epoch 1098/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2545 - acc: 0.8735 - val\_loss: 0.3433 - val\_acc: 0.8947

Epoch 01098: val\_loss did not improve
Epoch 1099/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2670 - acc: 0.8897 - val\_loss: 0.3423 - val\_acc: 0.8889

Epoch 01099: val\_loss did not improve
Epoch 1100/1500
680/680 [==============================] - 0s 62us/step - loss: 0.3028 - acc: 0.8632 - val\_loss: 0.3420 - val\_acc: 0.8889

Epoch 01100: val\_loss did not improve
Epoch 1101/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2492 - acc: 0.8912 - val\_loss: 0.3420 - val\_acc: 0.8889

Epoch 01101: val\_loss did not improve
Epoch 1102/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2895 - acc: 0.8838 - val\_loss: 0.3415 - val\_acc: 0.8889

Epoch 01102: val\_loss did not improve
Epoch 1103/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2647 - acc: 0.8941 - val\_loss: 0.3408 - val\_acc: 0.8889

Epoch 01103: val\_loss did not improve
Epoch 1104/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2722 - acc: 0.8779 - val\_loss: 0.3406 - val\_acc: 0.8889

Epoch 01104: val\_loss did not improve
Epoch 1105/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2781 - acc: 0.8824 - val\_loss: 0.3396 - val\_acc: 0.8889

Epoch 01105: val\_loss did not improve
Epoch 1106/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2779 - acc: 0.8676 - val\_loss: 0.3393 - val\_acc: 0.8889

Epoch 01106: val\_loss did not improve
Epoch 1107/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2885 - acc: 0.8676 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 01107: val\_loss did not improve
Epoch 1108/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3115 - acc: 0.8765 - val\_loss: 0.3372 - val\_acc: 0.8889

Epoch 01108: val\_loss did not improve
Epoch 1109/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2758 - acc: 0.8868 - val\_loss: 0.3367 - val\_acc: 0.8889

Epoch 01109: val\_loss did not improve
Epoch 1110/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2619 - acc: 0.8882 - val\_loss: 0.3361 - val\_acc: 0.8889

Epoch 01110: val\_loss did not improve
Epoch 1111/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2879 - acc: 0.8809 - val\_loss: 0.3358 - val\_acc: 0.8889

Epoch 01111: val\_loss did not improve
Epoch 1112/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2601 - acc: 0.8971 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01112: val\_loss did not improve
Epoch 1113/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2656 - acc: 0.8838 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01113: val\_loss did not improve
Epoch 1114/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2611 - acc: 0.8912 - val\_loss: 0.3361 - val\_acc: 0.8889

Epoch 01114: val\_loss did not improve
Epoch 1115/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2779 - acc: 0.8809 - val\_loss: 0.3360 - val\_acc: 0.8889

Epoch 01115: val\_loss did not improve
Epoch 1116/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2871 - acc: 0.8706 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01116: val\_loss did not improve
Epoch 1117/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2578 - acc: 0.8868 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 01117: val\_loss did not improve
Epoch 1118/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2590 - acc: 0.8971 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01118: val\_loss did not improve
Epoch 1119/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2825 - acc: 0.8882 - val\_loss: 0.3357 - val\_acc: 0.8889

Epoch 01119: val\_loss did not improve
Epoch 1120/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2458 - acc: 0.9118 - val\_loss: 0.3354 - val\_acc: 0.8889

Epoch 01120: val\_loss did not improve
Epoch 1121/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2629 - acc: 0.8882 - val\_loss: 0.3354 - val\_acc: 0.8889

Epoch 01121: val\_loss did not improve
Epoch 1122/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2482 - acc: 0.8897 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01122: ReduceLROnPlateau reducing learning rate to 0.00010466953317518346.

Epoch 01122: val\_loss did not improve
Epoch 1123/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2606 - acc: 0.8809 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01123: val\_loss did not improve
Epoch 1124/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2979 - acc: 0.8632 - val\_loss: 0.3369 - val\_acc: 0.8889

Epoch 01124: val\_loss did not improve
Epoch 1125/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2829 - acc: 0.8838 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 01125: val\_loss did not improve
Epoch 1126/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2575 - acc: 0.8779 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 01126: val\_loss did not improve
Epoch 1127/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2769 - acc: 0.8750 - val\_loss: 0.3387 - val\_acc: 0.8889

Epoch 01127: val\_loss did not improve
Epoch 1128/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2747 - acc: 0.8897 - val\_loss: 0.3395 - val\_acc: 0.8889

Epoch 01128: val\_loss did not improve
Epoch 1129/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2837 - acc: 0.8765 - val\_loss: 0.3394 - val\_acc: 0.8947

Epoch 01129: val\_loss did not improve
Epoch 1130/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2809 - acc: 0.8779 - val\_loss: 0.3399 - val\_acc: 0.8947

Epoch 01130: val\_loss did not improve
Epoch 1131/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2607 - acc: 0.8868 - val\_loss: 0.3400 - val\_acc: 0.8947

Epoch 01131: val\_loss did not improve
Epoch 1132/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2901 - acc: 0.8735 - val\_loss: 0.3391 - val\_acc: 0.8947

Epoch 01132: val\_loss did not improve
Epoch 1133/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2431 - acc: 0.8941 - val\_loss: 0.3384 - val\_acc: 0.8889

Epoch 01133: val\_loss did not improve
Epoch 1134/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2850 - acc: 0.8765 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 01134: val\_loss did not improve
Epoch 1135/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2599 - acc: 0.8853 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01135: val\_loss did not improve
Epoch 1136/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2963 - acc: 0.8691 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01136: val\_loss did not improve
Epoch 1137/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2559 - acc: 0.8941 - val\_loss: 0.3387 - val\_acc: 0.8889

Epoch 01137: val\_loss did not improve
Epoch 1138/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2557 - acc: 0.8912 - val\_loss: 0.3392 - val\_acc: 0.8889

Epoch 01138: val\_loss did not improve
Epoch 1139/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2664 - acc: 0.8853 - val\_loss: 0.3395 - val\_acc: 0.8889

Epoch 01139: val\_loss did not improve
Epoch 1140/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2671 - acc: 0.8809 - val\_loss: 0.3394 - val\_acc: 0.8889

Epoch 01140: val\_loss did not improve
Epoch 1141/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2398 - acc: 0.9029 - val\_loss: 0.3393 - val\_acc: 0.8889

Epoch 01141: val\_loss did not improve
Epoch 1142/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2654 - acc: 0.8912 - val\_loss: 0.3388 - val\_acc: 0.8889

Epoch 01142: val\_loss did not improve
Epoch 1143/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2554 - acc: 0.8971 - val\_loss: 0.3389 - val\_acc: 0.8889

Epoch 01143: val\_loss did not improve
Epoch 1144/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2636 - acc: 0.8882 - val\_loss: 0.3391 - val\_acc: 0.8889

Epoch 01144: val\_loss did not improve
Epoch 1145/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2734 - acc: 0.8897 - val\_loss: 0.3394 - val\_acc: 0.8889

Epoch 01145: val\_loss did not improve
Epoch 1146/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2949 - acc: 0.8750 - val\_loss: 0.3403 - val\_acc: 0.8947

Epoch 01146: val\_loss did not improve
Epoch 1147/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2534 - acc: 0.9147 - val\_loss: 0.3409 - val\_acc: 0.8947

Epoch 01147: val\_loss did not improve
Epoch 1148/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2767 - acc: 0.8897 - val\_loss: 0.3412 - val\_acc: 0.8947

Epoch 01148: val\_loss did not improve
Epoch 1149/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2850 - acc: 0.8765 - val\_loss: 0.3414 - val\_acc: 0.8889

Epoch 01149: val\_loss did not improve
Epoch 1150/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2626 - acc: 0.8912 - val\_loss: 0.3413 - val\_acc: 0.8830

Epoch 01150: val\_loss did not improve
Epoch 1151/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2771 - acc: 0.8882 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 01151: val\_loss did not improve
Epoch 1152/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2836 - acc: 0.8779 - val\_loss: 0.3411 - val\_acc: 0.8889

Epoch 01152: ReduceLROnPlateau reducing learning rate to 9.420257920282893e-05.

Epoch 01152: val\_loss did not improve
Epoch 1153/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2633 - acc: 0.8750 - val\_loss: 0.3414 - val\_acc: 0.8889

Epoch 01153: val\_loss did not improve
Epoch 1154/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2525 - acc: 0.8956 - val\_loss: 0.3414 - val\_acc: 0.8889

Epoch 01154: val\_loss did not improve
Epoch 1155/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2377 - acc: 0.9015 - val\_loss: 0.3420 - val\_acc: 0.8889

Epoch 01155: val\_loss did not improve
Epoch 1156/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3015 - acc: 0.8662 - val\_loss: 0.3426 - val\_acc: 0.8889

Epoch 01156: val\_loss did not improve
Epoch 1157/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2688 - acc: 0.8926 - val\_loss: 0.3428 - val\_acc: 0.8889

Epoch 01157: val\_loss did not improve
Epoch 1158/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2558 - acc: 0.8956 - val\_loss: 0.3432 - val\_acc: 0.8947

Epoch 01158: val\_loss did not improve
Epoch 1159/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2797 - acc: 0.8868 - val\_loss: 0.3427 - val\_acc: 0.8947

Epoch 01159: val\_loss did not improve
Epoch 1160/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2829 - acc: 0.8735 - val\_loss: 0.3424 - val\_acc: 0.8947

Epoch 01160: val\_loss did not improve
Epoch 1161/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2733 - acc: 0.8676 - val\_loss: 0.3425 - val\_acc: 0.8947

Epoch 01161: val\_loss did not improve
Epoch 1162/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2418 - acc: 0.8912 - val\_loss: 0.3428 - val\_acc: 0.8947

Epoch 01162: val\_loss did not improve
Epoch 1163/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2905 - acc: 0.8809 - val\_loss: 0.3422 - val\_acc: 0.8947

Epoch 01163: val\_loss did not improve
Epoch 1164/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2651 - acc: 0.8882 - val\_loss: 0.3412 - val\_acc: 0.8889

Epoch 01164: val\_loss did not improve
Epoch 1165/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2605 - acc: 0.8897 - val\_loss: 0.3412 - val\_acc: 0.8889

Epoch 01165: val\_loss did not improve
Epoch 1166/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2389 - acc: 0.9132 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 01166: val\_loss did not improve
Epoch 1167/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2782 - acc: 0.8912 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 01167: val\_loss did not improve
Epoch 1168/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2551 - acc: 0.9059 - val\_loss: 0.3406 - val\_acc: 0.8889

Epoch 01168: val\_loss did not improve
Epoch 1169/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2335 - acc: 0.9074 - val\_loss: 0.3406 - val\_acc: 0.8889

Epoch 01169: val\_loss did not improve
Epoch 1170/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2741 - acc: 0.8824 - val\_loss: 0.3412 - val\_acc: 0.8889

Epoch 01170: val\_loss did not improve
Epoch 1171/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2797 - acc: 0.8838 - val\_loss: 0.3422 - val\_acc: 0.8889

Epoch 01171: val\_loss did not improve
Epoch 1172/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2833 - acc: 0.8735 - val\_loss: 0.3428 - val\_acc: 0.8889

Epoch 01172: val\_loss did not improve
Epoch 1173/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2583 - acc: 0.8971 - val\_loss: 0.3430 - val\_acc: 0.8889

Epoch 01173: val\_loss did not improve
Epoch 1174/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2833 - acc: 0.8706 - val\_loss: 0.3433 - val\_acc: 0.8889

Epoch 01174: val\_loss did not improve
Epoch 1175/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2566 - acc: 0.9000 - val\_loss: 0.3425 - val\_acc: 0.8889

Epoch 01175: val\_loss did not improve
Epoch 1176/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2141 - acc: 0.9265 - val\_loss: 0.3422 - val\_acc: 0.8889

Epoch 01176: val\_loss did not improve
Epoch 1177/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2706 - acc: 0.8735 - val\_loss: 0.3421 - val\_acc: 0.8889

Epoch 01177: val\_loss did not improve
Epoch 1178/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2657 - acc: 0.8824 - val\_loss: 0.3421 - val\_acc: 0.8889

Epoch 01178: val\_loss did not improve
Epoch 1179/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2855 - acc: 0.8853 - val\_loss: 0.3426 - val\_acc: 0.8889

Epoch 01179: val\_loss did not improve
Epoch 1180/1500
680/680 [==============================] - 0s 60us/step - loss: 0.3050 - acc: 0.8706 - val\_loss: 0.3429 - val\_acc: 0.8830

Epoch 01180: val\_loss did not improve
Epoch 1181/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2560 - acc: 0.8956 - val\_loss: 0.3434 - val\_acc: 0.8830

Epoch 01181: val\_loss did not improve
Epoch 1182/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2914 - acc: 0.8706 - val\_loss: 0.3434 - val\_acc: 0.8830

Epoch 01182: ReduceLROnPlateau reducing learning rate to 8.478232193738222e-05.

Epoch 01182: val\_loss did not improve
Epoch 1183/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2867 - acc: 0.8750 - val\_loss: 0.3429 - val\_acc: 0.8830

Epoch 01183: val\_loss did not improve
Epoch 1184/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2459 - acc: 0.9000 - val\_loss: 0.3423 - val\_acc: 0.8830

Epoch 01184: val\_loss did not improve
Epoch 1185/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2728 - acc: 0.8912 - val\_loss: 0.3419 - val\_acc: 0.8830

Epoch 01185: val\_loss did not improve
Epoch 1186/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2724 - acc: 0.8647 - val\_loss: 0.3413 - val\_acc: 0.8830

Epoch 01186: val\_loss did not improve
Epoch 1187/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2644 - acc: 0.8882 - val\_loss: 0.3407 - val\_acc: 0.8830

Epoch 01187: val\_loss did not improve
Epoch 1188/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2559 - acc: 0.8926 - val\_loss: 0.3407 - val\_acc: 0.8830

Epoch 01188: val\_loss did not improve
Epoch 1189/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2519 - acc: 0.8971 - val\_loss: 0.3409 - val\_acc: 0.8830

Epoch 01189: val\_loss did not improve
Epoch 1190/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2758 - acc: 0.8868 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 01190: val\_loss did not improve
Epoch 1191/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2688 - acc: 0.8853 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 01191: val\_loss did not improve
Epoch 1192/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2694 - acc: 0.8912 - val\_loss: 0.3417 - val\_acc: 0.8830

Epoch 01192: val\_loss did not improve
Epoch 1193/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2372 - acc: 0.8971 - val\_loss: 0.3421 - val\_acc: 0.8830

Epoch 01193: val\_loss did not improve
Epoch 1194/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2382 - acc: 0.8985 - val\_loss: 0.3420 - val\_acc: 0.8830

Epoch 01194: val\_loss did not improve
Epoch 1195/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2606 - acc: 0.8897 - val\_loss: 0.3422 - val\_acc: 0.8830

Epoch 01195: val\_loss did not improve
Epoch 1196/1500
680/680 [==============================] - 0s 66us/step - loss: 0.3122 - acc: 0.8515 - val\_loss: 0.3425 - val\_acc: 0.8830

Epoch 01196: val\_loss did not improve
Epoch 1197/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2573 - acc: 0.9074 - val\_loss: 0.3423 - val\_acc: 0.8889

Epoch 01197: val\_loss did not improve
Epoch 1198/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2599 - acc: 0.8971 - val\_loss: 0.3422 - val\_acc: 0.8889

Epoch 01198: val\_loss did not improve
Epoch 1199/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2442 - acc: 0.9044 - val\_loss: 0.3419 - val\_acc: 0.8889

Epoch 01199: val\_loss did not improve
Epoch 1200/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2702 - acc: 0.8809 - val\_loss: 0.3417 - val\_acc: 0.8889

Epoch 01200: val\_loss did not improve
Epoch 1201/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2668 - acc: 0.8912 - val\_loss: 0.3413 - val\_acc: 0.8889

Epoch 01201: val\_loss did not improve
Epoch 1202/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2535 - acc: 0.8956 - val\_loss: 0.3411 - val\_acc: 0.8889

Epoch 01202: val\_loss did not improve
Epoch 1203/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2519 - acc: 0.8971 - val\_loss: 0.3409 - val\_acc: 0.8889

Epoch 01203: val\_loss did not improve
Epoch 1204/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2585 - acc: 0.8971 - val\_loss: 0.3409 - val\_acc: 0.8889

Epoch 01204: val\_loss did not improve
Epoch 1205/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2669 - acc: 0.8926 - val\_loss: 0.3415 - val\_acc: 0.8830

Epoch 01205: val\_loss did not improve
Epoch 1206/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2589 - acc: 0.8956 - val\_loss: 0.3418 - val\_acc: 0.8830

Epoch 01206: val\_loss did not improve
Epoch 1207/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2667 - acc: 0.9015 - val\_loss: 0.3414 - val\_acc: 0.8830

Epoch 01207: val\_loss did not improve
Epoch 1208/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2816 - acc: 0.8809 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 01208: val\_loss did not improve
Epoch 1209/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2598 - acc: 0.8941 - val\_loss: 0.3410 - val\_acc: 0.8889

Epoch 01209: val\_loss did not improve
Epoch 1210/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2512 - acc: 0.8897 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 01210: val\_loss did not improve
Epoch 1211/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2679 - acc: 0.8868 - val\_loss: 0.3401 - val\_acc: 0.8830

Epoch 01211: val\_loss did not improve
Epoch 1212/1500
680/680 [==============================] - 0s 58us/step - loss: 0.3041 - acc: 0.8603 - val\_loss: 0.3400 - val\_acc: 0.8830

Epoch 01212: ReduceLROnPlateau reducing learning rate to 7.630409236298874e-05.

Epoch 01212: val\_loss did not improve
Epoch 1213/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2626 - acc: 0.8838 - val\_loss: 0.3398 - val\_acc: 0.8830

Epoch 01213: val\_loss did not improve
Epoch 1214/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2570 - acc: 0.8912 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 01214: val\_loss did not improve
Epoch 1215/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2483 - acc: 0.9044 - val\_loss: 0.3392 - val\_acc: 0.8830

Epoch 01215: val\_loss did not improve
Epoch 1216/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2660 - acc: 0.8765 - val\_loss: 0.3391 - val\_acc: 0.8830

Epoch 01216: val\_loss did not improve
Epoch 1217/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2607 - acc: 0.8809 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 01217: val\_loss did not improve
Epoch 1218/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2599 - acc: 0.9059 - val\_loss: 0.3394 - val\_acc: 0.8830

Epoch 01218: val\_loss did not improve
Epoch 1219/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2856 - acc: 0.8676 - val\_loss: 0.3395 - val\_acc: 0.8830

Epoch 01219: val\_loss did not improve
Epoch 1220/1500
680/680 [==============================] - 0s 69us/step - loss: 0.2278 - acc: 0.9029 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 01220: val\_loss did not improve
Epoch 1221/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2833 - acc: 0.8779 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 01221: val\_loss did not improve
Epoch 1222/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2634 - acc: 0.8794 - val\_loss: 0.3409 - val\_acc: 0.8830

Epoch 01222: val\_loss did not improve
Epoch 1223/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2570 - acc: 0.9029 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 01223: val\_loss did not improve
Epoch 1224/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2799 - acc: 0.8882 - val\_loss: 0.3414 - val\_acc: 0.8830

Epoch 01224: val\_loss did not improve
Epoch 1225/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2414 - acc: 0.9000 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 01225: val\_loss did not improve
Epoch 1226/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2695 - acc: 0.9000 - val\_loss: 0.3411 - val\_acc: 0.8830

Epoch 01226: val\_loss did not improve
Epoch 1227/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2753 - acc: 0.8853 - val\_loss: 0.3406 - val\_acc: 0.8830

Epoch 01227: val\_loss did not improve
Epoch 1228/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2773 - acc: 0.8721 - val\_loss: 0.3406 - val\_acc: 0.8830

Epoch 01228: val\_loss did not improve
Epoch 1229/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2871 - acc: 0.8853 - val\_loss: 0.3404 - val\_acc: 0.8830

Epoch 01229: val\_loss did not improve
Epoch 1230/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2612 - acc: 0.8897 - val\_loss: 0.3409 - val\_acc: 0.8830

Epoch 01230: val\_loss did not improve
Epoch 1231/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2621 - acc: 0.8750 - val\_loss: 0.3413 - val\_acc: 0.8830

Epoch 01231: val\_loss did not improve
Epoch 1232/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2506 - acc: 0.8912 - val\_loss: 0.3413 - val\_acc: 0.8889

Epoch 01232: val\_loss did not improve
Epoch 1233/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2596 - acc: 0.8897 - val\_loss: 0.3415 - val\_acc: 0.8889

Epoch 01233: val\_loss did not improve
Epoch 1234/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2959 - acc: 0.8574 - val\_loss: 0.3420 - val\_acc: 0.8889

Epoch 01234: val\_loss did not improve
Epoch 1235/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2771 - acc: 0.8838 - val\_loss: 0.3415 - val\_acc: 0.8889

Epoch 01235: val\_loss did not improve
Epoch 1236/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2626 - acc: 0.8868 - val\_loss: 0.3413 - val\_acc: 0.8889

Epoch 01236: val\_loss did not improve
Epoch 1237/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2617 - acc: 0.8794 - val\_loss: 0.3412 - val\_acc: 0.8889

Epoch 01237: val\_loss did not improve
Epoch 1238/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2778 - acc: 0.8809 - val\_loss: 0.3414 - val\_acc: 0.8889

Epoch 01238: val\_loss did not improve
Epoch 1239/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2528 - acc: 0.8985 - val\_loss: 0.3417 - val\_acc: 0.8889

Epoch 01239: val\_loss did not improve
Epoch 1240/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2576 - acc: 0.8809 - val\_loss: 0.3419 - val\_acc: 0.8889

Epoch 01240: val\_loss did not improve
Epoch 1241/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2536 - acc: 0.8956 - val\_loss: 0.3421 - val\_acc: 0.8889

Epoch 01241: val\_loss did not improve
Epoch 1242/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2522 - acc: 0.8941 - val\_loss: 0.3417 - val\_acc: 0.8889

Epoch 01242: ReduceLROnPlateau reducing learning rate to 6.867368574603461e-05.

Epoch 01242: val\_loss did not improve
Epoch 1243/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2674 - acc: 0.8824 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 01243: val\_loss did not improve
Epoch 1244/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2623 - acc: 0.8824 - val\_loss: 0.3407 - val\_acc: 0.8889

Epoch 01244: val\_loss did not improve
Epoch 1245/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2790 - acc: 0.8618 - val\_loss: 0.3404 - val\_acc: 0.8889

Epoch 01245: val\_loss did not improve
Epoch 1246/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2409 - acc: 0.9015 - val\_loss: 0.3402 - val\_acc: 0.8889

Epoch 01246: val\_loss did not improve
Epoch 1247/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2457 - acc: 0.8985 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 01247: val\_loss did not improve
Epoch 1248/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2857 - acc: 0.8647 - val\_loss: 0.3406 - val\_acc: 0.8830

Epoch 01248: val\_loss did not improve
Epoch 1249/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2565 - acc: 0.8926 - val\_loss: 0.3403 - val\_acc: 0.8830

Epoch 01249: val\_loss did not improve
Epoch 1250/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2964 - acc: 0.8618 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 01250: val\_loss did not improve
Epoch 1251/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2645 - acc: 0.8853 - val\_loss: 0.3394 - val\_acc: 0.8830

Epoch 01251: val\_loss did not improve
Epoch 1252/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2715 - acc: 0.8779 - val\_loss: 0.3390 - val\_acc: 0.8830

Epoch 01252: val\_loss did not improve
Epoch 1253/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2612 - acc: 0.8853 - val\_loss: 0.3389 - val\_acc: 0.8830

Epoch 01253: val\_loss did not improve
Epoch 1254/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2600 - acc: 0.8926 - val\_loss: 0.3386 - val\_acc: 0.8830

Epoch 01254: val\_loss did not improve
Epoch 1255/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2711 - acc: 0.8750 - val\_loss: 0.3388 - val\_acc: 0.8830

Epoch 01255: val\_loss did not improve
Epoch 1256/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2708 - acc: 0.8838 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 01256: val\_loss did not improve
Epoch 1257/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2871 - acc: 0.8868 - val\_loss: 0.3396 - val\_acc: 0.8830

Epoch 01257: val\_loss did not improve
Epoch 1258/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2676 - acc: 0.8735 - val\_loss: 0.3402 - val\_acc: 0.8830

Epoch 01258: val\_loss did not improve
Epoch 1259/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2832 - acc: 0.8824 - val\_loss: 0.3401 - val\_acc: 0.8889

Epoch 01259: val\_loss did not improve
Epoch 1260/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2551 - acc: 0.8897 - val\_loss: 0.3400 - val\_acc: 0.8889

Epoch 01260: val\_loss did not improve
Epoch 1261/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2673 - acc: 0.8956 - val\_loss: 0.3403 - val\_acc: 0.8889

Epoch 01261: val\_loss did not improve
Epoch 1262/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2698 - acc: 0.8897 - val\_loss: 0.3402 - val\_acc: 0.8889

Epoch 01262: val\_loss did not improve
Epoch 1263/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2419 - acc: 0.9029 - val\_loss: 0.3400 - val\_acc: 0.8889

Epoch 01263: val\_loss did not improve
Epoch 1264/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2618 - acc: 0.8985 - val\_loss: 0.3394 - val\_acc: 0.8889

Epoch 01264: val\_loss did not improve
Epoch 1265/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2789 - acc: 0.8897 - val\_loss: 0.3397 - val\_acc: 0.8889

Epoch 01265: val\_loss did not improve
Epoch 1266/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2729 - acc: 0.8721 - val\_loss: 0.3396 - val\_acc: 0.8889

Epoch 01266: val\_loss did not improve
Epoch 1267/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2445 - acc: 0.9029 - val\_loss: 0.3393 - val\_acc: 0.8830

Epoch 01267: val\_loss did not improve
Epoch 1268/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2593 - acc: 0.8838 - val\_loss: 0.3396 - val\_acc: 0.8830

Epoch 01268: val\_loss did not improve
Epoch 1269/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2594 - acc: 0.8956 - val\_loss: 0.3398 - val\_acc: 0.8830

Epoch 01269: val\_loss did not improve
Epoch 1270/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2579 - acc: 0.8868 - val\_loss: 0.3397 - val\_acc: 0.8830

Epoch 01270: val\_loss did not improve
Epoch 1271/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2532 - acc: 0.8912 - val\_loss: 0.3395 - val\_acc: 0.8830

Epoch 01271: val\_loss did not improve
Epoch 1272/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2571 - acc: 0.8794 - val\_loss: 0.3395 - val\_acc: 0.8830

Epoch 01272: ReduceLROnPlateau reducing learning rate to 6.180632044561207e-05.

Epoch 01272: val\_loss did not improve
Epoch 1273/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2498 - acc: 0.8912 - val\_loss: 0.3390 - val\_acc: 0.8830

Epoch 01273: val\_loss did not improve
Epoch 1274/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2373 - acc: 0.9147 - val\_loss: 0.3388 - val\_acc: 0.8830

Epoch 01274: val\_loss did not improve
Epoch 1275/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2644 - acc: 0.8882 - val\_loss: 0.3386 - val\_acc: 0.8830

Epoch 01275: val\_loss did not improve
Epoch 1276/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2725 - acc: 0.8735 - val\_loss: 0.3380 - val\_acc: 0.8830

Epoch 01276: val\_loss did not improve
Epoch 1277/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2887 - acc: 0.8603 - val\_loss: 0.3380 - val\_acc: 0.8830

Epoch 01277: val\_loss did not improve
Epoch 1278/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2669 - acc: 0.8897 - val\_loss: 0.3378 - val\_acc: 0.8830

Epoch 01278: val\_loss did not improve
Epoch 1279/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2706 - acc: 0.8941 - val\_loss: 0.3379 - val\_acc: 0.8830

Epoch 01279: val\_loss did not improve
Epoch 1280/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2644 - acc: 0.8985 - val\_loss: 0.3386 - val\_acc: 0.8830

Epoch 01280: val\_loss did not improve
Epoch 1281/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2687 - acc: 0.8838 - val\_loss: 0.3386 - val\_acc: 0.8830

Epoch 01281: val\_loss did not improve
Epoch 1282/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2644 - acc: 0.8897 - val\_loss: 0.3388 - val\_acc: 0.8830

Epoch 01282: val\_loss did not improve
Epoch 1283/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2399 - acc: 0.8971 - val\_loss: 0.3388 - val\_acc: 0.8830

Epoch 01283: val\_loss did not improve
Epoch 1284/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2537 - acc: 0.8926 - val\_loss: 0.3390 - val\_acc: 0.8830

Epoch 01284: val\_loss did not improve
Epoch 1285/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2682 - acc: 0.8838 - val\_loss: 0.3391 - val\_acc: 0.8830

Epoch 01285: val\_loss did not improve
Epoch 1286/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2601 - acc: 0.8897 - val\_loss: 0.3392 - val\_acc: 0.8830

Epoch 01286: val\_loss did not improve
Epoch 1287/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2427 - acc: 0.9000 - val\_loss: 0.3389 - val\_acc: 0.8830

Epoch 01287: val\_loss did not improve
Epoch 1288/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2611 - acc: 0.9000 - val\_loss: 0.3388 - val\_acc: 0.8889

Epoch 01288: val\_loss did not improve
Epoch 1289/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2642 - acc: 0.8765 - val\_loss: 0.3386 - val\_acc: 0.8889

Epoch 01289: val\_loss did not improve
Epoch 1290/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2892 - acc: 0.8750 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01290: val\_loss did not improve
Epoch 1291/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2473 - acc: 0.8971 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01291: val\_loss did not improve
Epoch 1292/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2798 - acc: 0.8809 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01292: val\_loss did not improve
Epoch 1293/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2631 - acc: 0.8868 - val\_loss: 0.3375 - val\_acc: 0.8947

Epoch 01293: val\_loss did not improve
Epoch 1294/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2425 - acc: 0.9015 - val\_loss: 0.3376 - val\_acc: 0.8947

Epoch 01294: val\_loss did not improve
Epoch 1295/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2542 - acc: 0.8912 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01295: val\_loss did not improve
Epoch 1296/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2768 - acc: 0.8882 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01296: val\_loss did not improve
Epoch 1297/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2696 - acc: 0.9029 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01297: val\_loss did not improve
Epoch 1298/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2638 - acc: 0.9000 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01298: val\_loss did not improve
Epoch 1299/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2470 - acc: 0.8868 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01299: val\_loss did not improve
Epoch 1300/1500
680/680 [==============================] - 0s 104us/step - loss: 0.2482 - acc: 0.9132 - val\_loss: 0.3373 - val\_acc: 0.8947

Epoch 01300: val\_loss did not improve
Epoch 1301/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2584 - acc: 0.8868 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01301: val\_loss did not improve
Epoch 1302/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2935 - acc: 0.8735 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01302: ReduceLROnPlateau reducing learning rate to 5.562568840105087e-05.

Epoch 01302: val\_loss did not improve
Epoch 1303/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2667 - acc: 0.8941 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01303: val\_loss did not improve
Epoch 1304/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2507 - acc: 0.8926 - val\_loss: 0.3383 - val\_acc: 0.8947

Epoch 01304: val\_loss did not improve
Epoch 1305/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2787 - acc: 0.8809 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01305: val\_loss did not improve
Epoch 1306/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2751 - acc: 0.8868 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01306: val\_loss did not improve
Epoch 1307/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2738 - acc: 0.8926 - val\_loss: 0.3384 - val\_acc: 0.8947

Epoch 01307: val\_loss did not improve
Epoch 1308/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2638 - acc: 0.9000 - val\_loss: 0.3384 - val\_acc: 0.8947

Epoch 01308: val\_loss did not improve
Epoch 1309/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2681 - acc: 0.8882 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01309: val\_loss did not improve
Epoch 1310/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2693 - acc: 0.8853 - val\_loss: 0.3384 - val\_acc: 0.8947

Epoch 01310: val\_loss did not improve
Epoch 1311/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2352 - acc: 0.9015 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01311: val\_loss did not improve
Epoch 1312/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2513 - acc: 0.8912 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01312: val\_loss did not improve
Epoch 1313/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2524 - acc: 0.9015 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01313: val\_loss did not improve
Epoch 1314/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2576 - acc: 0.8882 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01314: val\_loss did not improve
Epoch 1315/1500
680/680 [==============================] - 0s 69us/step - loss: 0.2816 - acc: 0.8647 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01315: val\_loss did not improve
Epoch 1316/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2637 - acc: 0.9000 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01316: val\_loss did not improve
Epoch 1317/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2648 - acc: 0.8971 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01317: val\_loss did not improve
Epoch 1318/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2898 - acc: 0.8691 - val\_loss: 0.3383 - val\_acc: 0.8947

Epoch 01318: val\_loss did not improve
Epoch 1319/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2494 - acc: 0.8941 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01319: val\_loss did not improve
Epoch 1320/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2910 - acc: 0.8779 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01320: val\_loss did not improve
Epoch 1321/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2587 - acc: 0.8779 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01321: val\_loss did not improve
Epoch 1322/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2595 - acc: 0.9000 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01322: val\_loss did not improve
Epoch 1323/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2575 - acc: 0.8985 - val\_loss: 0.3375 - val\_acc: 0.8889

Epoch 01323: val\_loss did not improve
Epoch 1324/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2456 - acc: 0.9029 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01324: val\_loss did not improve
Epoch 1325/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2597 - acc: 0.8912 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01325: val\_loss did not improve
Epoch 1326/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2795 - acc: 0.8853 - val\_loss: 0.3375 - val\_acc: 0.8889

Epoch 01326: val\_loss did not improve
Epoch 1327/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2471 - acc: 0.8956 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01327: val\_loss did not improve
Epoch 1328/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2399 - acc: 0.8882 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01328: val\_loss did not improve
Epoch 1329/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2669 - acc: 0.8897 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01329: val\_loss did not improve
Epoch 1330/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2524 - acc: 0.8941 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01330: val\_loss did not improve
Epoch 1331/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2635 - acc: 0.8956 - val\_loss: 0.3378 - val\_acc: 0.8889

Epoch 01331: val\_loss did not improve
Epoch 1332/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2608 - acc: 0.8882 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01332: ReduceLROnPlateau reducing learning rate to 5.006312021578197e-05.

Epoch 01332: val\_loss did not improve
Epoch 1333/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2723 - acc: 0.8794 - val\_loss: 0.3378 - val\_acc: 0.8889

Epoch 01333: val\_loss did not improve
Epoch 1334/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2525 - acc: 0.8941 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01334: val\_loss did not improve
Epoch 1335/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2599 - acc: 0.8779 - val\_loss: 0.3378 - val\_acc: 0.8889

Epoch 01335: val\_loss did not improve
Epoch 1336/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2543 - acc: 0.9088 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01336: val\_loss did not improve
Epoch 1337/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2595 - acc: 0.8868 - val\_loss: 0.3381 - val\_acc: 0.8889

Epoch 01337: val\_loss did not improve
Epoch 1338/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2459 - acc: 0.8956 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01338: val\_loss did not improve
Epoch 1339/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2671 - acc: 0.8809 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01339: val\_loss did not improve
Epoch 1340/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2412 - acc: 0.9074 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 01340: val\_loss did not improve
Epoch 1341/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2511 - acc: 0.8926 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 01341: val\_loss did not improve
Epoch 1342/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2650 - acc: 0.8912 - val\_loss: 0.3385 - val\_acc: 0.8889

Epoch 01342: val\_loss did not improve
Epoch 1343/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2545 - acc: 0.8882 - val\_loss: 0.3387 - val\_acc: 0.8889

Epoch 01343: val\_loss did not improve
Epoch 1344/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2701 - acc: 0.8926 - val\_loss: 0.3388 - val\_acc: 0.8889

Epoch 01344: val\_loss did not improve
Epoch 1345/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2743 - acc: 0.8765 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 01345: val\_loss did not improve
Epoch 1346/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2491 - acc: 0.8853 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01346: val\_loss did not improve
Epoch 1347/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2313 - acc: 0.9162 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01347: val\_loss did not improve
Epoch 1348/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2620 - acc: 0.8926 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01348: val\_loss did not improve
Epoch 1349/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2626 - acc: 0.8838 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01349: val\_loss did not improve
Epoch 1350/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2634 - acc: 0.8809 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01350: val\_loss did not improve
Epoch 1351/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2561 - acc: 0.8897 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01351: val\_loss did not improve
Epoch 1352/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2648 - acc: 0.8941 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01352: val\_loss did not improve
Epoch 1353/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2567 - acc: 0.8941 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 01353: val\_loss did not improve
Epoch 1354/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2477 - acc: 0.8941 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 01354: val\_loss did not improve
Epoch 1355/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2695 - acc: 0.8897 - val\_loss: 0.3385 - val\_acc: 0.8889

Epoch 01355: val\_loss did not improve
Epoch 1356/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2591 - acc: 0.9000 - val\_loss: 0.3383 - val\_acc: 0.8947

Epoch 01356: val\_loss did not improve
Epoch 1357/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2788 - acc: 0.8824 - val\_loss: 0.3385 - val\_acc: 0.8889

Epoch 01357: val\_loss did not improve
Epoch 1358/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2774 - acc: 0.8750 - val\_loss: 0.3387 - val\_acc: 0.8889

Epoch 01358: val\_loss did not improve
Epoch 1359/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2679 - acc: 0.8868 - val\_loss: 0.3388 - val\_acc: 0.8947

Epoch 01359: val\_loss did not improve
Epoch 1360/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2935 - acc: 0.8853 - val\_loss: 0.3385 - val\_acc: 0.8947

Epoch 01360: val\_loss did not improve
Epoch 1361/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2798 - acc: 0.8809 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01361: val\_loss did not improve
Epoch 1362/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2687 - acc: 0.8809 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01362: ReduceLROnPlateau reducing learning rate to 4.505680917645805e-05.

Epoch 01362: val\_loss did not improve
Epoch 1363/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2702 - acc: 0.8853 - val\_loss: 0.3373 - val\_acc: 0.8947

Epoch 01363: val\_loss did not improve
Epoch 1364/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2578 - acc: 0.9015 - val\_loss: 0.3372 - val\_acc: 0.8947

Epoch 01364: val\_loss did not improve
Epoch 1365/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2526 - acc: 0.8853 - val\_loss: 0.3370 - val\_acc: 0.8947

Epoch 01365: val\_loss did not improve
Epoch 1366/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2725 - acc: 0.8809 - val\_loss: 0.3376 - val\_acc: 0.8947

Epoch 01366: val\_loss did not improve
Epoch 1367/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2787 - acc: 0.8897 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01367: val\_loss did not improve
Epoch 1368/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2537 - acc: 0.8956 - val\_loss: 0.3369 - val\_acc: 0.8947

Epoch 01368: val\_loss did not improve
Epoch 1369/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2703 - acc: 0.8794 - val\_loss: 0.3368 - val\_acc: 0.8947

Epoch 01369: val\_loss did not improve
Epoch 1370/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2635 - acc: 0.8897 - val\_loss: 0.3367 - val\_acc: 0.8947

Epoch 01370: val\_loss did not improve
Epoch 1371/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2427 - acc: 0.8956 - val\_loss: 0.3363 - val\_acc: 0.8947

Epoch 01371: val\_loss did not improve
Epoch 1372/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2724 - acc: 0.8882 - val\_loss: 0.3361 - val\_acc: 0.8947

Epoch 01372: val\_loss did not improve
Epoch 1373/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2509 - acc: 0.8941 - val\_loss: 0.3357 - val\_acc: 0.8947

Epoch 01373: val\_loss did not improve
Epoch 1374/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2696 - acc: 0.8838 - val\_loss: 0.3358 - val\_acc: 0.8947

Epoch 01374: val\_loss did not improve
Epoch 1375/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2741 - acc: 0.8735 - val\_loss: 0.3362 - val\_acc: 0.8947

Epoch 01375: val\_loss did not improve
Epoch 1376/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2633 - acc: 0.8809 - val\_loss: 0.3364 - val\_acc: 0.8947

Epoch 01376: val\_loss did not improve
Epoch 1377/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2683 - acc: 0.8794 - val\_loss: 0.3364 - val\_acc: 0.8947

Epoch 01377: val\_loss did not improve
Epoch 1378/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2774 - acc: 0.8824 - val\_loss: 0.3368 - val\_acc: 0.8947

Epoch 01378: val\_loss did not improve
Epoch 1379/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2664 - acc: 0.8912 - val\_loss: 0.3366 - val\_acc: 0.8947

Epoch 01379: val\_loss did not improve
Epoch 1380/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2772 - acc: 0.8838 - val\_loss: 0.3367 - val\_acc: 0.8947

Epoch 01380: val\_loss did not improve
Epoch 1381/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2691 - acc: 0.8897 - val\_loss: 0.3370 - val\_acc: 0.8947

Epoch 01381: val\_loss did not improve
Epoch 1382/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2527 - acc: 0.9015 - val\_loss: 0.3372 - val\_acc: 0.8947

Epoch 01382: val\_loss did not improve
Epoch 1383/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2636 - acc: 0.8853 - val\_loss: 0.3372 - val\_acc: 0.8947

Epoch 01383: val\_loss did not improve
Epoch 1384/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2976 - acc: 0.8632 - val\_loss: 0.3372 - val\_acc: 0.8947

Epoch 01384: val\_loss did not improve
Epoch 1385/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2544 - acc: 0.8824 - val\_loss: 0.3373 - val\_acc: 0.8947

Epoch 01385: val\_loss did not improve
Epoch 1386/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2701 - acc: 0.8750 - val\_loss: 0.3380 - val\_acc: 0.8947

Epoch 01386: val\_loss did not improve
Epoch 1387/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2657 - acc: 0.8956 - val\_loss: 0.3384 - val\_acc: 0.8947

Epoch 01387: val\_loss did not improve
Epoch 1388/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2601 - acc: 0.8926 - val\_loss: 0.3385 - val\_acc: 0.8947

Epoch 01388: val\_loss did not improve
Epoch 1389/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2622 - acc: 0.9044 - val\_loss: 0.3389 - val\_acc: 0.8947

Epoch 01389: val\_loss did not improve
Epoch 1390/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2591 - acc: 0.8926 - val\_loss: 0.3388 - val\_acc: 0.8947

Epoch 01390: val\_loss did not improve
Epoch 1391/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2629 - acc: 0.9000 - val\_loss: 0.3390 - val\_acc: 0.8947

Epoch 01391: val\_loss did not improve
Epoch 1392/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2873 - acc: 0.8809 - val\_loss: 0.3388 - val\_acc: 0.8947

Epoch 01392: ReduceLROnPlateau reducing learning rate to 4.055112694913987e-05.

Epoch 01392: val\_loss did not improve
Epoch 1393/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2648 - acc: 0.8824 - val\_loss: 0.3389 - val\_acc: 0.8947

Epoch 01393: val\_loss did not improve
Epoch 1394/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2816 - acc: 0.8706 - val\_loss: 0.3391 - val\_acc: 0.8947

Epoch 01394: val\_loss did not improve
Epoch 1395/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2772 - acc: 0.8897 - val\_loss: 0.3394 - val\_acc: 0.8947

Epoch 01395: val\_loss did not improve
Epoch 1396/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2706 - acc: 0.8882 - val\_loss: 0.3394 - val\_acc: 0.8947

Epoch 01396: val\_loss did not improve
Epoch 1397/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2461 - acc: 0.8941 - val\_loss: 0.3389 - val\_acc: 0.8947

Epoch 01397: val\_loss did not improve
Epoch 1398/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2738 - acc: 0.8956 - val\_loss: 0.3387 - val\_acc: 0.8947

Epoch 01398: val\_loss did not improve
Epoch 1399/1500
680/680 [==============================] - 0s 67us/step - loss: 0.2833 - acc: 0.8794 - val\_loss: 0.3385 - val\_acc: 0.8889

Epoch 01399: val\_loss did not improve
Epoch 1400/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2607 - acc: 0.8956 - val\_loss: 0.3382 - val\_acc: 0.8889

Epoch 01400: val\_loss did not improve
Epoch 1401/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2701 - acc: 0.8882 - val\_loss: 0.3381 - val\_acc: 0.8889

Epoch 01401: val\_loss did not improve
Epoch 1402/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2766 - acc: 0.8691 - val\_loss: 0.3383 - val\_acc: 0.8889

Epoch 01402: val\_loss did not improve
Epoch 1403/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2779 - acc: 0.8794 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01403: val\_loss did not improve
Epoch 1404/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2513 - acc: 0.8971 - val\_loss: 0.3379 - val\_acc: 0.8889

Epoch 01404: val\_loss did not improve
Epoch 1405/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2728 - acc: 0.8912 - val\_loss: 0.3380 - val\_acc: 0.8889

Epoch 01405: val\_loss did not improve
Epoch 1406/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2860 - acc: 0.8838 - val\_loss: 0.3381 - val\_acc: 0.8889

Epoch 01406: val\_loss did not improve
Epoch 1407/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2742 - acc: 0.8765 - val\_loss: 0.3378 - val\_acc: 0.8889

Epoch 01407: val\_loss did not improve
Epoch 1408/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2588 - acc: 0.8956 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01408: val\_loss did not improve
Epoch 1409/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2471 - acc: 0.9044 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01409: val\_loss did not improve
Epoch 1410/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2515 - acc: 0.9088 - val\_loss: 0.3377 - val\_acc: 0.8889

Epoch 01410: val\_loss did not improve
Epoch 1411/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2629 - acc: 0.8941 - val\_loss: 0.3376 - val\_acc: 0.8889

Epoch 01411: val\_loss did not improve
Epoch 1412/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2431 - acc: 0.9074 - val\_loss: 0.3376 - val\_acc: 0.8947

Epoch 01412: val\_loss did not improve
Epoch 1413/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2586 - acc: 0.8838 - val\_loss: 0.3375 - val\_acc: 0.8947

Epoch 01413: val\_loss did not improve
Epoch 1414/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2557 - acc: 0.8868 - val\_loss: 0.3369 - val\_acc: 0.8947

Epoch 01414: val\_loss did not improve
Epoch 1415/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2932 - acc: 0.8706 - val\_loss: 0.3371 - val\_acc: 0.8947

Epoch 01415: val\_loss did not improve
Epoch 1416/1500
680/680 [==============================] - 0s 61us/step - loss: 0.3033 - acc: 0.8588 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01416: val\_loss did not improve
Epoch 1417/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2802 - acc: 0.8882 - val\_loss: 0.3373 - val\_acc: 0.8947

Epoch 01417: val\_loss did not improve
Epoch 1418/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2484 - acc: 0.8971 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01418: val\_loss did not improve
Epoch 1419/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2496 - acc: 0.8912 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01419: val\_loss did not improve
Epoch 1420/1500
680/680 [==============================] - 0s 68us/step - loss: 0.2604 - acc: 0.8868 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01420: val\_loss did not improve
Epoch 1421/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2667 - acc: 0.8853 - val\_loss: 0.3379 - val\_acc: 0.8947

Epoch 01421: val\_loss did not improve
Epoch 1422/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2825 - acc: 0.8750 - val\_loss: 0.3381 - val\_acc: 0.8947

Epoch 01422: ReduceLROnPlateau reducing learning rate to 3.6496014581643975e-05.

Epoch 01422: val\_loss did not improve
Epoch 1423/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2408 - acc: 0.8926 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01423: val\_loss did not improve
Epoch 1424/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2541 - acc: 0.9000 - val\_loss: 0.3382 - val\_acc: 0.8947

Epoch 01424: val\_loss did not improve
Epoch 1425/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2640 - acc: 0.9015 - val\_loss: 0.3378 - val\_acc: 0.8947

Epoch 01425: val\_loss did not improve
Epoch 1426/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2765 - acc: 0.8941 - val\_loss: 0.3377 - val\_acc: 0.8947

Epoch 01426: val\_loss did not improve
Epoch 1427/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2603 - acc: 0.8956 - val\_loss: 0.3374 - val\_acc: 0.8947

Epoch 01427: val\_loss did not improve
Epoch 1428/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2505 - acc: 0.8838 - val\_loss: 0.3371 - val\_acc: 0.8947

Epoch 01428: val\_loss did not improve
Epoch 1429/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2651 - acc: 0.8912 - val\_loss: 0.3371 - val\_acc: 0.8947

Epoch 01429: val\_loss did not improve
Epoch 1430/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2464 - acc: 0.9074 - val\_loss: 0.3373 - val\_acc: 0.8947

Epoch 01430: val\_loss did not improve
Epoch 1431/1500
680/680 [==============================] - 0s 56us/step - loss: 0.2826 - acc: 0.8824 - val\_loss: 0.3370 - val\_acc: 0.8947

Epoch 01431: val\_loss did not improve
Epoch 1432/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2536 - acc: 0.8956 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 01432: val\_loss did not improve
Epoch 1433/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2648 - acc: 0.8838 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01433: val\_loss did not improve
Epoch 1434/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2720 - acc: 0.8897 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01434: val\_loss did not improve
Epoch 1435/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2546 - acc: 0.8912 - val\_loss: 0.3362 - val\_acc: 0.8889

Epoch 01435: val\_loss did not improve
Epoch 1436/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2631 - acc: 0.8941 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01436: val\_loss did not improve
Epoch 1437/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2644 - acc: 0.8868 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01437: val\_loss did not improve
Epoch 1438/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2658 - acc: 0.8838 - val\_loss: 0.3364 - val\_acc: 0.8889

Epoch 01438: val\_loss did not improve
Epoch 1439/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2395 - acc: 0.8956 - val\_loss: 0.3360 - val\_acc: 0.8889

Epoch 01439: val\_loss did not improve
Epoch 1440/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2290 - acc: 0.9029 - val\_loss: 0.3361 - val\_acc: 0.8889

Epoch 01440: val\_loss did not improve
Epoch 1441/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2519 - acc: 0.8853 - val\_loss: 0.3359 - val\_acc: 0.8889

Epoch 01441: val\_loss did not improve
Epoch 1442/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2450 - acc: 0.8956 - val\_loss: 0.3359 - val\_acc: 0.8889

Epoch 01442: val\_loss did not improve
Epoch 1443/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2750 - acc: 0.8838 - val\_loss: 0.3360 - val\_acc: 0.8889

Epoch 01443: val\_loss did not improve
Epoch 1444/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2690 - acc: 0.8809 - val\_loss: 0.3365 - val\_acc: 0.8889

Epoch 01444: val\_loss did not improve
Epoch 1445/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2678 - acc: 0.8794 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01445: val\_loss did not improve
Epoch 1446/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2839 - acc: 0.8853 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 01446: val\_loss did not improve
Epoch 1447/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2624 - acc: 0.8941 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 01447: val\_loss did not improve
Epoch 1448/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2521 - acc: 0.8985 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01448: val\_loss did not improve
Epoch 1449/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2663 - acc: 0.9029 - val\_loss: 0.3365 - val\_acc: 0.8889

Epoch 01449: val\_loss did not improve
Epoch 1450/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2539 - acc: 0.8853 - val\_loss: 0.3364 - val\_acc: 0.8889

Epoch 01450: val\_loss did not improve
Epoch 1451/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2645 - acc: 0.8985 - val\_loss: 0.3365 - val\_acc: 0.8889

Epoch 01451: val\_loss did not improve
Epoch 1452/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2630 - acc: 0.8868 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01452: ReduceLROnPlateau reducing learning rate to 3.284641279606149e-05.

Epoch 01452: val\_loss did not improve
Epoch 1453/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2734 - acc: 0.8735 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01453: val\_loss did not improve
Epoch 1454/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2706 - acc: 0.8926 - val\_loss: 0.3368 - val\_acc: 0.8889

Epoch 01454: val\_loss did not improve
Epoch 1455/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2500 - acc: 0.9029 - val\_loss: 0.3367 - val\_acc: 0.8889

Epoch 01455: val\_loss did not improve
Epoch 1456/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2586 - acc: 0.8882 - val\_loss: 0.3370 - val\_acc: 0.8889

Epoch 01456: val\_loss did not improve
Epoch 1457/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2639 - acc: 0.8897 - val\_loss: 0.3369 - val\_acc: 0.8889

Epoch 01457: val\_loss did not improve
Epoch 1458/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2662 - acc: 0.8941 - val\_loss: 0.3370 - val\_acc: 0.8889

Epoch 01458: val\_loss did not improve
Epoch 1459/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2521 - acc: 0.8926 - val\_loss: 0.3372 - val\_acc: 0.8889

Epoch 01459: val\_loss did not improve
Epoch 1460/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2931 - acc: 0.8691 - val\_loss: 0.3369 - val\_acc: 0.8889

Epoch 01460: val\_loss did not improve
Epoch 1461/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2773 - acc: 0.8750 - val\_loss: 0.3373 - val\_acc: 0.8889

Epoch 01461: val\_loss did not improve
Epoch 1462/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2623 - acc: 0.8824 - val\_loss: 0.3369 - val\_acc: 0.8889

Epoch 01462: val\_loss did not improve
Epoch 1463/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2594 - acc: 0.8971 - val\_loss: 0.3374 - val\_acc: 0.8889

Epoch 01463: val\_loss did not improve
Epoch 1464/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2822 - acc: 0.8838 - val\_loss: 0.3370 - val\_acc: 0.8889

Epoch 01464: val\_loss did not improve
Epoch 1465/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2488 - acc: 0.9029 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01465: val\_loss did not improve
Epoch 1466/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2589 - acc: 0.8956 - val\_loss: 0.3365 - val\_acc: 0.8889

Epoch 01466: val\_loss did not improve
Epoch 1467/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2728 - acc: 0.9015 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01467: val\_loss did not improve
Epoch 1468/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2803 - acc: 0.8750 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01468: val\_loss did not improve
Epoch 1469/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2575 - acc: 0.9088 - val\_loss: 0.3361 - val\_acc: 0.8889

Epoch 01469: val\_loss did not improve
Epoch 1470/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2514 - acc: 0.8897 - val\_loss: 0.3357 - val\_acc: 0.8889

Epoch 01470: val\_loss did not improve
Epoch 1471/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2787 - acc: 0.8985 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01471: val\_loss did not improve
Epoch 1472/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2855 - acc: 0.8750 - val\_loss: 0.3355 - val\_acc: 0.8889

Epoch 01472: val\_loss did not improve
Epoch 1473/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2730 - acc: 0.8647 - val\_loss: 0.3355 - val\_acc: 0.8889

Epoch 01473: val\_loss did not improve
Epoch 1474/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2589 - acc: 0.8838 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01474: val\_loss did not improve
Epoch 1475/1500
680/680 [==============================] - 0s 61us/step - loss: 0.2539 - acc: 0.9015 - val\_loss: 0.3358 - val\_acc: 0.8830

Epoch 01475: val\_loss did not improve
Epoch 1476/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2554 - acc: 0.8912 - val\_loss: 0.3362 - val\_acc: 0.8830

Epoch 01476: val\_loss did not improve
Epoch 1477/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2646 - acc: 0.8897 - val\_loss: 0.3363 - val\_acc: 0.8889

Epoch 01477: val\_loss did not improve
Epoch 1478/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2489 - acc: 0.9074 - val\_loss: 0.3364 - val\_acc: 0.8830

Epoch 01478: val\_loss did not improve
Epoch 1479/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2769 - acc: 0.8868 - val\_loss: 0.3366 - val\_acc: 0.8830

Epoch 01479: val\_loss did not improve
Epoch 1480/1500
680/680 [==============================] - 0s 66us/step - loss: 0.2767 - acc: 0.8794 - val\_loss: 0.3364 - val\_acc: 0.8830

Epoch 01480: val\_loss did not improve
Epoch 1481/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2733 - acc: 0.8838 - val\_loss: 0.3362 - val\_acc: 0.8830

Epoch 01481: val\_loss did not improve
Epoch 1482/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2509 - acc: 0.8838 - val\_loss: 0.3365 - val\_acc: 0.8830

Epoch 01482: ReduceLROnPlateau reducing learning rate to 2.9561770861619153e-05.

Epoch 01482: val\_loss did not improve
Epoch 1483/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2451 - acc: 0.8941 - val\_loss: 0.3366 - val\_acc: 0.8830

Epoch 01483: val\_loss did not improve
Epoch 1484/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2302 - acc: 0.9044 - val\_loss: 0.3363 - val\_acc: 0.8830

Epoch 01484: val\_loss did not improve
Epoch 1485/1500
680/680 [==============================] - 0s 65us/step - loss: 0.2576 - acc: 0.9015 - val\_loss: 0.3364 - val\_acc: 0.8830

Epoch 01485: val\_loss did not improve
Epoch 1486/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2489 - acc: 0.9029 - val\_loss: 0.3366 - val\_acc: 0.8889

Epoch 01486: val\_loss did not improve
Epoch 1487/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2950 - acc: 0.8794 - val\_loss: 0.3364 - val\_acc: 0.8889

Epoch 01487: val\_loss did not improve
Epoch 1488/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2374 - acc: 0.9044 - val\_loss: 0.3364 - val\_acc: 0.8889

Epoch 01488: val\_loss did not improve
Epoch 1489/1500
680/680 [==============================] - 0s 57us/step - loss: 0.2785 - acc: 0.8838 - val\_loss: 0.3365 - val\_acc: 0.8889

Epoch 01489: val\_loss did not improve
Epoch 1490/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2382 - acc: 0.8897 - val\_loss: 0.3360 - val\_acc: 0.8889

Epoch 01490: val\_loss did not improve
Epoch 1491/1500
680/680 [==============================] - 0s 60us/step - loss: 0.2542 - acc: 0.8956 - val\_loss: 0.3359 - val\_acc: 0.8889

Epoch 01491: val\_loss did not improve
Epoch 1492/1500
680/680 [==============================] - 0s 64us/step - loss: 0.2553 - acc: 0.9059 - val\_loss: 0.3358 - val\_acc: 0.8889

Epoch 01492: val\_loss did not improve
Epoch 1493/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2918 - acc: 0.8765 - val\_loss: 0.3359 - val\_acc: 0.8889

Epoch 01493: val\_loss did not improve
Epoch 1494/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2728 - acc: 0.9000 - val\_loss: 0.3358 - val\_acc: 0.8889

Epoch 01494: val\_loss did not improve
Epoch 1495/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2490 - acc: 0.8868 - val\_loss: 0.3357 - val\_acc: 0.8889

Epoch 01495: val\_loss did not improve
Epoch 1496/1500
680/680 [==============================] - 0s 62us/step - loss: 0.2805 - acc: 0.8750 - val\_loss: 0.3356 - val\_acc: 0.8889

Epoch 01496: val\_loss did not improve
Epoch 1497/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2802 - acc: 0.8721 - val\_loss: 0.3352 - val\_acc: 0.8889

Epoch 01497: val\_loss did not improve
Epoch 1498/1500
680/680 [==============================] - 0s 59us/step - loss: 0.2551 - acc: 0.8868 - val\_loss: 0.3350 - val\_acc: 0.8889

Epoch 01498: val\_loss did not improve
Epoch 1499/1500
680/680 [==============================] - 0s 58us/step - loss: 0.2779 - acc: 0.8794 - val\_loss: 0.3350 - val\_acc: 0.8889

Epoch 01499: val\_loss did not improve
Epoch 1500/1500
680/680 [==============================] - 0s 63us/step - loss: 0.2710 - acc: 0.8853 - val\_loss: 0.3350 - val\_acc: 0.8889

Epoch 01500: val\_loss did not improve

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lolkek.hdf5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{score}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} print test accuracy}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{accuracy}\PY{p}{)}
         
         
         \PY{n}{pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 75.7895\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
         
         \PY{c+c1}{\PYZsh{} Print a confusion Matrix}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Confusion Matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{C} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n}{Y\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n}{pred}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(} \PY{n}{C} \PY{o}{/} \PY{n}{C}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix
[[0.72093023 0.23076923]
 [0.25581395 0.78846154]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Print the Test data labels and predictions }
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test data labels and model output}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{pred}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test data labels and model output
[0 1] [0.57055163 0.4294484 ]
[0 1] [0.770627   0.22937301]
[0 1] [0.15699568 0.84300435]
[0 1] [0.06221158 0.93778837]
[0 1] [0.0818068 0.9181932]
[0 1] [0.06892689 0.9310732 ]
[0 1] [0.14084083 0.8591591 ]
[0 1] [0.31993598 0.6800641 ]
[0 1] [0.3093918 0.6906082]
[0 1] [0.39370644 0.60629356]
[0 1] [0.3223714 0.6776286]
[0 1] [0.21300083 0.7869991 ]
[0 1] [0.11902613 0.88097394]
[0 1] [0.3972197  0.60278034]
[0 1] [0.65960944 0.34039062]
[0 1] [0.3013394  0.69866055]
[0 1] [0.37768337 0.62231666]
[0 1] [0.87332916 0.12667078]
[0 1] [0.9241841 0.0758159]
[0 1] [0.6999757  0.30002424]
[0 1] [0.24900971 0.7509903 ]
[0 1] [0.11335941 0.8866406 ]
[0 1] [0.1857652  0.81423485]
[1 0] [0.17374495 0.826255  ]
[0 1] [0.07851047 0.9214896 ]
[0 1] [0.15065569 0.84934425]
[1 0] [0.14985529 0.8501447 ]
[0 1] [0.3318986  0.66810143]
[0 1] [0.11239757 0.88760245]
[0 1] [0.01336066 0.9866393 ]
[0 1] [0.0077877 0.9922123]
[0 1] [0.03646576 0.96353424]
[0 1] [0.05791545 0.94208455]
[0 1] [0.01073247 0.9892675 ]
[1 0] [0.00750515 0.9924948 ]
[1 0] [0.03683981 0.96316016]
[1 0] [0.04105125 0.9589487 ]
[1 0] [0.07384644 0.92615354]
[0 1] [0.09781604 0.9021839 ]
[0 1] [0.27559122 0.72440886]
[0 1] [0.43921322 0.5607868 ]
[1 0] [0.497989   0.50201094]
[0 1] [0.19112016 0.80887985]
[1 0] [0.07336975 0.9266303 ]
[1 0] [0.10120645 0.8987935 ]
[1 0] [0.30536553 0.6946345 ]
[1 0] [0.52996624 0.4700337 ]
[1 0] [0.8569761  0.14302388]
[1 0] [0.9515101  0.04848992]
[0 1] [0.988957   0.01104302]
[1 0] [0.99495935 0.00504063]
[1 0] [0.9705071  0.02949297]
[1 0] [0.9441045  0.05589557]
[1 0] [0.9902192  0.00978087]
[1 0] [0.9989472  0.00105279]
[1 0] [0.9978744  0.00212564]
[1 0] [0.9969111  0.00308889]
[1 0] [0.9898935  0.01010655]
[1 0] [0.9778307 0.0221693]
[1 0] [0.9799153  0.02008471]
[1 0] [0.9884505  0.01154946]
[1 0] [0.99735415 0.00264584]
[1 0] [9.9926537e-01 7.3464075e-04]
[1 0] [0.99822026 0.00177976]
[1 0] [0.9904385  0.00956154]
[1 0] [0.97605973 0.02394027]
[1 0] [0.9978921  0.00210788]
[1 0] [0.9939726  0.00602736]
[1 0] [0.97854084 0.02145916]
[1 0] [0.98513895 0.01486102]
[1 0] [0.9836082  0.01639184]
[1 0] [0.96997595 0.03002402]
[1 0] [0.889107   0.11089301]
[1 0] [0.8830633  0.11693671]
[1 0] [0.91442746 0.08557259]
[1 0] [0.9137332  0.08626678]
[1 0] [0.9418725  0.05812755]
[1 0] [0.82607293 0.17392705]
[0 1] [0.74338776 0.25661227]
[1 0] [0.4748142  0.52518576]
[1 0] [0.35655162 0.64344835]
[0 1] [0.20746101 0.79253894]
[0 1] [0.19320329 0.80679667]
[0 1] [0.22596182 0.7740382 ]
[0 1] [0.11459745 0.8854025 ]
[0 1] [0.08321848 0.91678154]
[0 1] [0.08832104 0.91167897]
[0 1] [0.06411622 0.93588376]
[0 1] [0.13096695 0.86903304]
[0 1] [0.40818128 0.59181875]
[0 1] [0.41129184 0.5887081 ]
[0 1] [0.3037985  0.69620144]
[0 1] [0.52181256 0.47818744]
[0 1] [0.6603642  0.33963582]
[0 1] [0.7296574  0.27034256]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Print charts to indicate performance}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
